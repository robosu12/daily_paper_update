{
  "LiDAR SLAM": {
    "2511.23156": "|2025-12-01|Design loads for wave impacts -- introducing the Probabilistic Adaptive Screening (PAS) method for predicting extreme non-linear loads on maritime structures|Sanne M. van Essen\u7b49|[2511.23156](http://arxiv.org/pdf/2511.23156)|\u65e0|\u25c6 To ensure the safety of marine and coastal structures, extreme (design) values should be known at the design stage.\n\u25c6 But for such complex systems, estimating the magnitude of events which are both non-linear and rare is extremely challenging, and involves considerable computational cost to capture the high-fidelity physics.\n\u25c6 To address this challenge, we offer a new multi-fidelity screening method, Probabilistic Adaptive Screening (PAS), which accurately predicts extreme values of strongly non-linear wave-induced loads while minimising the required high-fidelity simulation duration.|\n",
    "2512.01889": "|2025-12-01|KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM|Zaid Nasser\u7b49|[2512.01889](http://arxiv.org/pdf/2512.01889)|\u65e0|\u25c6 We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments.\n\u25c6 Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training.\n\u25c6 KM-ViPE tightly couples DINO visual features with geometric constraints through a high-level features based adaptive robust kernel that handles both moving objects and movable static objects (e.g., moving furniture in ego-centric views).|\n",
    "2512.01850": "|2025-12-01|Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching|Yue Pan\u7b49|[2512.01850](http://arxiv.org/pdf/2512.01850)|\u65e0|\u25c6 Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization.\n\u25c6 In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered.\n\u25c6 Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud.|\n",
    "2512.01753": "|2025-12-01|AgriLiRa4D: A Multi-Sensor UAV Dataset for Robust SLAM in Challenging Agricultural Fields|Zhihao Zhan\u7b49|[2512.01753](http://arxiv.org/pdf/2512.01753)|\u65e0|\u25c6 Multi-sensor Simultaneous Localization and Mapping (SLAM) is essential for Unmanned Aerial Vehicles (UAVs) performing agricultural tasks such as spraying, surveying, and inspection.\n\u25c6 However, real-world, multi-modal agricultural UAV datasets that enable research on robust operation remain scarce.\n\u25c6 To address this gap, we present AgriLiRa4D, a multi-modal UAV dataset designed for challenging outdoor agricultural environments.|\n",
    "2512.01296": "|2025-12-01|EGG-Fusion: Efficient 3D Reconstruction with Geometry-aware Gaussian Surfel on the Fly|Xiaokun Pan\u7b49|[2512.01296](http://arxiv.org/pdf/2512.01296)|\u65e0|\u25c6 Real-time 3D reconstruction is a fundamental task in computer graphics.\n\u25c6 Recently, differentiable-rendering-based SLAM system has demonstrated significant potential, enabling photorealistic scene rendering through learnable scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS).\n\u25c6 Current differentiable rendering methods face dual challenges in real-time computation and sensor noise sensitivity, leading to degraded geometric fidelity in scene reconstruction and limited practicality.|\n",
    "2512.02293": "|2025-12-02|VIGS-SLAM: Visual Inertial Gaussian Splatting SLAM|Zihan Zhu\u7b49|[2512.02293](http://arxiv.org/pdf/2512.02293)|\u65e0|\u25c6 We present VIGS-SLAM, a visual-inertial 3D Gaussian Splatting SLAM system that achieves robust real-time tracking and high-fidelity reconstruction.\n\u25c6 Although recent 3DGS-based SLAM methods achieve dense and photorealistic mapping, their purely visual design degrades under motion blur, low texture, and exposure variations.\n\u25c6 Our method tightly couples visual and inertial cues within a unified optimization framework, jointly refining camera poses, depths, and IMU states.|\n",
    "2512.03422": "|2025-12-03|What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models|Tianchen Deng\u7b49|[2512.03422](http://arxiv.org/pdf/2512.03422)|\u65e0|\u25c6 In this paper, we provide a comprehensive overview of existing scene representation methods for robotics, covering traditional representations such as point clouds, voxels, signed distance functions (SDF), and scene graphs, as well as more recent neural representations like Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and the emerging Foundation Models.\n\u25c6 While current SLAM and localization systems predominantly rely on sparse representations like point clouds and voxels, dense scene representations are expected to play a critical role in downstream tasks such as navigation and obstacle avoidance.\n\u25c6 Moreover, neural representations such as NeRF, 3DGS, and foundation models are well-suited for integrating high-level semantic features and language-based priors, enabling more comprehensive 3D scene understanding and embodied intelligence.|\n",
    "2512.03397": "|2025-12-03|Surfel-LIO: Fast LiDAR-Inertial Odometry with Pre-computed Surfels and Hierarchical Z-order Voxel Hashing|Seungwon Choi\u7b49|[2512.03397](http://arxiv.org/pdf/2512.03397)|\u65e0|\u25c6 LiDAR-inertial odometry (LIO) is an active research area, as it enables accurate real-time state estimation in GPS-denied environments.\n\u25c6 Recent advances in map data structures and spatial indexing have significantly improved the efficiency of LIO systems.\n\u25c6 Nevertheless, we observe that two aspects may still leave room for improvement: (1) nearest neighbor search often requires examining multiple spatial units to gather sufficient points for plane fitting, and (2) plane parameters are typically recomputed at every iteration despite unchanged map geometry.|\n"
  },
  "Visual SLAM": {
    "2512.01889": "|2025-12-01|KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM|Zaid Nasser\u7b49|[2512.01889](http://arxiv.org/pdf/2512.01889)|\u65e0|\u25c6 We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments.\n\u25c6 Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training.\n\u25c6 KM-ViPE tightly couples DINO visual features with geometric constraints through a high-level features based adaptive robust kernel that handles both moving objects and movable static objects (e.g., moving furniture in ego-centric views).|\n",
    "2512.02293": "|2025-12-02|VIGS-SLAM: Visual Inertial Gaussian Splatting SLAM|Zihan Zhu\u7b49|[2512.02293](http://arxiv.org/pdf/2512.02293)|\u65e0|\u25c6 We present VIGS-SLAM, a visual-inertial 3D Gaussian Splatting SLAM system that achieves robust real-time tracking and high-fidelity reconstruction.\n\u25c6 Although recent 3DGS-based SLAM methods achieve dense and photorealistic mapping, their purely visual design degrades under motion blur, low texture, and exposure variations.\n\u25c6 Our method tightly couples visual and inertial cues within a unified optimization framework, jointly refining camera poses, depths, and IMU states.|\n"
  },
  "Loop Closure": {
    "2512.01850": "|2025-12-01|Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching|Yue Pan\u7b49|[2512.01850](http://arxiv.org/pdf/2512.01850)|\u65e0|\u25c6 Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization.\n\u25c6 In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered.\n\u25c6 Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud.|\n",
    "2512.01768": "|2025-12-01|Exciton-Polariton hybrid skin-topological states|Ruiqi Bao\u7b49|[2512.01768](http://arxiv.org/pdf/2512.01768)|\u65e0|\u25c6 The non Hermitian skin effect, where bulk states accumulate at system boundaries, challenges the conventional bulk boundary correspondence.\n\u25c6 Here we propose a scheme to realize hybrid skin topological states in exciton polariton honeycomb lattices by introducing sublattice dependent gain and loss.\n\u25c6 This non Hermiticity couples with the intrinsic topological edge modes, leading to relocalization of edge states.|\n",
    "2512.01194": "|2025-12-01|RoboLoc: A Benchmark Dataset for Point Place Recognition and Localization in Indoor-Outdoor Integrated Environments|Jaejin Jeon\u7b49|[2512.01194](http://arxiv.org/pdf/2512.01194)|\u65e0|\u25c6 Robust place recognition is essential for reliable localization in robotics, particularly in complex environments with fre- quent indoor-outdoor transitions.\n\u25c6 However, existing LiDAR-based datasets often focus on outdoor scenarios and lack seamless domain shifts.\n\u25c6 In this paper, we propose RoboLoc, a benchmark dataset designed for GPS-free place recognition in indoor-outdoor environments with floor transitions.|\n",
    "2512.03046": "|2025-12-02|MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues|Zichen Liu\u7b49|[2512.03046](http://arxiv.org/pdf/2512.03046)|\u65e0|\u25c6 We propose MagicQuill V2, a novel system that introduces a \\textbf{layered composition} paradigm to generative image editing, bridging the gap between the semantic power of diffusion models and the granular control of traditional graphics software.\n\u25c6 While diffusion transformers excel at holistic generation, their use of singular, monolithic prompts fails to disentangle distinct user intentions for content, position, and appearance.\n\u25c6 To overcome this, our method deconstructs creative intent into a stack of controllable visual cues: a content layer for what to create, a spatial layer for where to place it, a structural layer for how it is shaped, and a color layer for its palette.|\n",
    "2512.02897": "|2025-12-02|Polar Perspectives: Evaluating 2-D LiDAR Projections for Robust Place Recognition with Visual Foundation Models|Pierpaolo Serio\u7b49|[2512.02897](http://arxiv.org/pdf/2512.02897)|\u65e0|\u25c6 This work presents a systematic investigation into how alternative LiDAR-to-image projections affect metric place recognition when coupled with a state-of-the-art vision foundation model.\n\u25c6 We introduce a modular retrieval pipeline that controls for backbone, aggregation, and evaluation protocol, thereby isolating the influence of the 2-D projection itself.\n\u25c6 Using consistent geometric and structural channels across multiple datasets and deployment scenarios, we identify the projection characteristics that most strongly determine discriminative power, robustness to environmental variation, and suitability for real-time autonomy.|\n",
    "2512.02293": "|2025-12-02|VIGS-SLAM: Visual Inertial Gaussian Splatting SLAM|Zihan Zhu\u7b49|[2512.02293](http://arxiv.org/pdf/2512.02293)|\u65e0|\u25c6 We present VIGS-SLAM, a visual-inertial 3D Gaussian Splatting SLAM system that achieves robust real-time tracking and high-fidelity reconstruction.\n\u25c6 Although recent 3DGS-based SLAM methods achieve dense and photorealistic mapping, their purely visual design degrades under motion blur, low texture, and exposure variations.\n\u25c6 Our method tightly couples visual and inertial cues within a unified optimization framework, jointly refining camera poses, depths, and IMU states.|\n",
    "2512.02215": "|2025-12-01|Magnetoelectric effect in the mixed valence polyoxovanadate cage V$_{12}$|Piotr Koz\u0142owski|[2512.02215](http://arxiv.org/pdf/2512.02215)|\u65e0|\u25c6 Development of spintronic and quantum computing devices increases demand for efficient, energy saving method of spin manipulation at molecular scale.\n\u25c6 Polyoxovanadate molecular magnets being susceptible to both electric and magnetic fields may serve here as a good base material.\n\u25c6 In this paper two isostructural anions [V$_{12}$As$_8$O$_{40}$(HCO$_2$)]$^{n-}$ (with $n=3,5$) featuring two different mixed-valence states with itinerant and localized valence electrons are studied.|\n",
    "2512.02108": "|2025-12-01|The Dependence of Earth Milankovitch Cycles on Martian Mass|Stephen R. Kane\u7b49|[2512.02108](http://arxiv.org/pdf/2512.02108)|\u65e0|\u25c6 The Milankovitch cycles of Earth result from gravitational interactions with other bodies in the Solar System.\n\u25c6 These interactions lead to slow changes in the orbit and angular momentum vector of Earth, and correspondingly influence Earth's climate evolution.\n\u25c6 Several studies have shown that Mars may play a significant role in these Milankovitch cycles, such as the 2.4 Myr eccentricity cycle related to perihelion precession dynamics.|\n"
  },
  "Image Matching": {
    "2512.01908": "|2025-12-01|SARL: Spatially-Aware Self-Supervised Representation Learning for Visuo-Tactile Perception|Gurmeher Khurana\u7b49|[2512.01908](http://arxiv.org/pdf/2512.01908)|\u65e0|\u25c6 Contact-rich robotic manipulation requires representations that encode local geometry.\n\u25c6 Vision provides global context but lacks direct measurements of properties such as texture and hardness, whereas touch supplies these cues.\n\u25c6 Modern visuo-tactile sensors capture both modalities in a single fused image, yielding intrinsically aligned inputs that are well suited to manipulation tasks requiring visual and tactile information.|\n",
    "2512.01850": "|2025-12-01|Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching|Yue Pan\u7b49|[2512.01850](http://arxiv.org/pdf/2512.01850)|\u65e0|\u25c6 Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization.\n\u25c6 In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered.\n\u25c6 Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud.|\n",
    "2512.01816": "|2025-12-01|Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights|Juanxi Tian\u7b49|[2512.01816](http://arxiv.org/pdf/2512.01816)|\u65e0|\u25c6 Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency.\n\u25c6 However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time.\n\u25c6 To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation.|\n",
    "2512.01643": "|2025-12-01|ViT$^3$: Unlocking Test-Time Training in Vision|Dongchen Han\u7b49|[2512.01643](http://arxiv.org/pdf/2512.01643)|\u65e0|\u25c6 Test-Time Training (TTT) has recently emerged as a promising direction for efficient sequence modeling.\n\u25c6 TTT reformulates attention operation as an online learning problem, constructing a compact inner model from key-value pairs at test time.\n\u25c6 This reformulation opens a rich and flexible design space while achieving linear computational complexity.|\n",
    "2512.01611": "|2025-12-01|Depth Matching Method Based on ShapeDTW for Oil-Based Mud Imager|Fengfeng Li\u7b49|[2512.01611](http://arxiv.org/pdf/2512.01611)|\u65e0|\u25c6 In well logging operations using the oil-based mud (OBM) microresistivity imager, which employs an interleaved design with upper and lower pad sets, depth misalignment issues persist between the pad images even after velocity correction.\n\u25c6 This paper presents a depth matching method for borehole images based on the Shape Dynamic Time Warping (ShapeDTW) algorithm.\n\u25c6 The method extracts local shape features to construct a morphologically sensitive distance matrix, better preserving structural similarity between sequences during alignment.|\n",
    "2512.01510": "|2025-12-01|Semantic-aware Random Convolution and Source Matching for Domain Generalization in Medical Image Segmentation|Franz Thaler\u7b49|[2512.01510](http://arxiv.org/pdf/2512.01510)|\u65e0|\u25c6 We tackle the challenging problem of single-source domain generalization (DG) for medical image segmentation.\n\u25c6 To this end, we aim for training a network on one domain (e.g., CT) and directly apply it to a different domain (e.g., MR) without adapting the model and without requiring images or annotations from the new domain during training.\n\u25c6 We propose a novel method for promoting DG when training deep segmentation networks, which we call SRCSM.|\n",
    "2512.01479": "|2025-12-01|Non-Markovian dynamics in ice nucleation|Pablo Montero de Hijes\u7b49|[2512.01479](http://arxiv.org/pdf/2512.01479)|\u65e0|\u25c6 In simulation studies of crystallisation, the size of the largest crystalline nucleus is often used as a reaction coordinate to monitor the progress of the nucleation process.\n\u25c6 Here, we investigate, for the case of homogeneous ice nucleation, whether the nucleus size exhibits Markovian dynamics, as assumed in classical nucleation theory.\n\u25c6 Using 300 independent nucleation trajectories generated by molecular dynamics, we evaluate the mean recurrence time required to reach selected values of the largest nucleus size.|\n",
    "2512.02944": "|2025-12-02|The Convex Matching Distance in Multiparameter Persistence|Patrizio Frosini\u7b49|[2512.02944](http://arxiv.org/pdf/2512.02944)|\u65e0|\u25c6 We introduce the convex matching distance, a novel metric for comparing functions with values in the real plane.\n\u25c6 This metric measures the maximal bottleneck distance between the persistence diagrams associated with the convex combinations of the two function components.\n\u25c6 Similarly to the traditional matching distance, the convex matching distance aggregates the information provided by two real-valued components.|\n",
    "2512.02920": "|2025-12-02|Learning Multimodal Embeddings for Traffic Accident Prediction and Causal Estimation|Ziniu Zhang\u7b49|[2512.02920](http://arxiv.org/pdf/2512.02920)|\u65e0|\u25c6 We consider analyzing traffic accident patterns using both road network data and satellite images aligned to road graph nodes.\n\u25c6 Previous work for predicting accident occurrences relies primarily on road network structural features while overlooking physical and environmental information from the road surface and its surroundings.\n\u25c6 In this work, we construct a large multimodal dataset across six U.S.|\n",
    "2512.02889": "|2025-12-02|Terahertz Emission from Spintronic Stack Nanodecorated with Drop-Cast Core-Shell Plasmonic Nanoparticles|Vittorio Cecconi\u7b49|[2512.02889](http://arxiv.org/pdf/2512.02889)|\u65e0|\u25c6 Spintronic emitters promise to revolutionise terahertz (THz) sources by converting ultrafast optical pulses into broadband THz radiation without phase-matching constraints.\n\u25c6 Because the conversion relies on spin-current injection across a nanometre-thin magnetic layer, its efficiency is ordinarily limited by weak optical coupling.\n\u25c6 Here, we present a demonstration of a drop-casting based approach to introduce ultrafast plasmonic-mediated coupling: a sparse-layer of silica-gold core-shell nanoparticles is deposited directly onto a W/Fe/Pt spintronic trilayer.|\n",
    "2512.02833": "|2025-12-02|A Comparative Study on How Data Normalization Affects Zero-Shot Generalization in Time Series Foundation Models|Ihab Ahmed\u7b49|[2512.02833](http://arxiv.org/pdf/2512.02833)|\u65e0|\u25c6 We investigate input normalization methods for Time-Series Foundation Models (TSFMs).\n\u25c6 While normalization is well-studied in dataset-specific time-series models, it remains overlooked in TSFMs where generalization is critical.\n\u25c6 Time-series data, unlike text or images, exhibits significant scale variation across domains and channels, coupled with non-stationarity, can undermine TSFM performance regardless of architectural complexity.|\n",
    "2512.02826": "|2025-12-02|From Navigation to Refinement: Revealing the Two-Stage Nature of Flow-based Diffusion Models through Oracle Velocity|Haoming Liu\u7b49|[2512.02826](http://arxiv.org/pdf/2512.02826)|\u65e0|\u25c6 Flow-based diffusion models have emerged as a leading paradigm for training generative models across images and videos.\n\u25c6 However, their memorization-generalization behavior remains poorly understood.\n\u25c6 In this work, we revisit the flow matching (FM) objective and study its marginal velocity field, which admits a closed-form expression, allowing exact computation of the oracle FM target.|\n",
    "2512.02805": "|2025-12-02|Direct observational evidence that higher-luminosity type 1 active galactic nuclei are most commonly triggered by galaxy mergers|Yongmin Yoon\u7b49|[2512.02805](http://arxiv.org/pdf/2512.02805)|\u65e0|\u25c6 We examine the connection between galaxy mergers and the triggering of active galactic nuclei (AGNs) using a sample of 614 type 1 AGNs at $z<0.07$, along with a control sample of inactive galaxies matched to the AGNs for comparison.\n\u25c6 We used tidal features, detected in deep images from the DESI Legacy Imaging Survey, as direct evidence of recent mergers.\n\u25c6 We find that the fraction of type 1 AGN hosts with tidal features ($f_T$) is higher for AGNs with higher luminosities and (to a lesser extent) more massive black holes.|\n",
    "2512.02783": "|2025-12-02|Exploring Definitions of Quality and Diversity in Sonic Measurement Spaces|Bj\u00f6rn \u00de\u00f3r J\u00f3nsson\u7b49|[2512.02783](http://arxiv.org/pdf/2512.02783)|\u65e0|\u25c6 Digital sound synthesis presents the opportunity to explore vast parameter spaces containing millions of configurations.\n\u25c6 Quality diversity (QD) evolutionary algorithms offer a promising approach to harness this potential, yet their success hinges on appropriate sonic feature representations.\n\u25c6 Existing QD methods predominantly employ handcrafted descriptors or supervised classifiers, potentially introducing unintended exploration biases and constraining discovery to familiar sonic regions.|\n",
    "2512.02768": "|2025-12-02|Diffusion-Prior Split Gibbs Sampling for Synthetic Aperture Radar Imaging under Incomplete Measurements|Hefei Gao\u7b49|[2512.02768](http://arxiv.org/pdf/2512.02768)|\u65e0|\u25c6 Synthetic aperture radar (SAR) imaging plays a critical role in all-weather, day-and-night remote sensing, yet reconstruction is often challenged by noise, undersampling, and complex scattering scenarios.\n\u25c6 Conventional methods, including matched filtering and sparsity-based compressed sensing, are limited in capturing intricate scene structures and frequently suffer from artifacts, elevated sidelobes, and loss of fine details.\n\u25c6 Recent diffusion models have demonstrated superior capability in representing high-order priors; however, existing diffusion-based SAR methods still yield degraded reconstructions due to oversimplified likelihood approximations in guided sampling.|\n",
    "2512.02737": "|2025-12-02|Beyond Paired Data: Self-Supervised UAV Geo-Localization from Reference Imagery Alone|Tristan Amadei\u7b49|[2512.02737](http://arxiv.org/pdf/2512.02737)|\u65e0|\u25c6 Image-based localization in GNSS-denied environments is critical for UAV autonomy.\n\u25c6 Existing state-of-the-art approaches rely on matching UAV images to geo-referenced satellite images; however, they typically require large-scale, paired UAV-satellite datasets for training.\n\u25c6 Such data are costly to acquire and often unavailable, limiting their applicability.|\n",
    "2512.02697": "|2025-12-02|GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization|Zixuan Song\u7b49|[2512.02697](http://arxiv.org/pdf/2512.02697)|\u65e0|\u25c6 Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image.\n\u25c6 However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable.\n\u25c6 It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image).|\n",
    "2512.03880": "|2025-12-03|Leveraging topological data analysis to estimate bone strength from micro-CT as a surrogate for advanced imaging|John Rick Manzanares\u7b49|[2512.03880](http://arxiv.org/pdf/2512.03880)|\u65e0|\u25c6 Accurate bone strength prediction is essential for assessing fracture risk, particularly in aging populations and individuals with osteoporosis.\n\u25c6 Bone imaging has evolved from X-rays and DXA to clinical computed tomography (CT), and now to advanced modalities such as high-resolution peripheral quantitative CT and synchrotron radiation CT, which offer unprecedented resolution of bone microarchitecture.\n\u25c6 However, analytical methods have not kept pace with these imaging advances.|\n",
    "2512.03715": "|2025-12-03|DINO-RotateMatch: A Rotation-Aware Deep Framework for Robust Image Matching in Large-Scale 3D Reconstruction|Kaichen Zhang\u7b49|[2512.03715](http://arxiv.org/pdf/2512.03715)|\u65e0|\u25c6 This paper presents DINO-RotateMatch, a deep-learning framework designed to address the chal lenges of image matching in large-scale 3D reconstruction from unstructured Internet images.\n\u25c6 The   method integrates a dataset-adaptive image pairing strategy with rotation-aware keypoint extraction and   matching.\n\u25c6 DINO is employed to retrieve semantically relevant image pairs in large collections, while   rotation-based augmentation captures orientation-dependent local features using ALIKED and Light Glue.|\n",
    "2512.03684": "|2025-12-03|A Novel Approach to Tomato Harvesting Using a Hybrid Gripper with Semantic Segmentation and Keypoint Detection|Shahid Ansari\u7b49|[2512.03684](http://arxiv.org/pdf/2512.03684)|\u65e0|\u25c6 This paper presents an autonomous tomato-harvesting system built around a hybrid robotic gripper that combines six soft auxetic fingers with a rigid exoskeleton and a latex basket to achieve gentle, cage-like grasping.\n\u25c6 The gripper is driven by a servo-actuated Scotch--yoke mechanism, and includes separator leaves that form a conical frustum for fruit isolation, with an integrated micro-servo cutter for pedicel cutting.\n\u25c6 For perception, an RGB--D camera and a Detectron2-based pipeline perform semantic segmentation of ripe/unripe tomatoes and keypoint localization of the pedicel and fruit center under occlusion and variable illumination.|\n",
    "2512.03660": "|2025-12-03|Linking Aneurysmal Geometry and Hemodynamics Using Computational Fluid Dynamics|Spyridon C. Katsoudas\u7b49|[2512.03660](http://arxiv.org/pdf/2512.03660)|\u65e0|\u25c6 The development and progression of abdominal aortic aneurysms (AAA) are related to complex flow patterns and wall-shear-driven mechanobiological stimuli, yet the quantitative relationship between aneurysmal geometry and hemodynamics remains poorly defined.\n\u25c6 In this study, we conducted a comprehensive hemodynamic analysis of 74 patient-specific abdominal aortas, representing one of the largest Computational Fluid Dynamics (CFD) cohorts reported to date.\n\u25c6 A multiscale framework coupling 0D-1D systemic circulation models with 3D stabilized finite-element simulations is used to generate physiologically consistent boundary conditions and high-fidelity flow fields.|\n",
    "2512.03598": "|2025-12-03|Memory-Guided Point Cloud Completion for Dental Reconstruction|Jianan Sun\u7b49|[2512.03598](http://arxiv.org/pdf/2512.03598)|\u65e0|\u25c6 Partial dental point clouds often suffer from large missing regions caused by occlusion and limited scanning views, which bias encoder-only global features and force decoders to hallucinate structures.\n\u25c6 We propose a retrieval-augmented framework for tooth completion that integrates a prototype memory into standard encoder--decoder pipelines.\n\u25c6 After encoding a partial input into a global descriptor, the model retrieves the nearest manifold prototype from a learnable memory and fuses it with the query feature through confidence-gated weighting before decoding.|\n",
    "2512.03346": "|2025-12-03|Hierarchical Attention for Sparse Volumetric Anomaly Detection in Subclinical Keratoconus|Lynn Kandakji\u7b49|[2512.03346](http://arxiv.org/pdf/2512.03346)|\u65e0|\u25c6 The detection of weak, spatially distributed anomalies in volumetric medical imaging remains a major challenge.\n\u25c6 The subtle, non-adjacent nature of early disease signals is often lost due to suboptimal architectural inductive biases: 2D/3D CNNs impose strong locality, while ViTs diffuse unconstrained global attention.\n\u25c6 This conflict leaves the optimal inductive structure for robust, sparse volumetric pattern recognition unresolved.|\n"
  },
  "3DGS": {
    "2512.01296": "|2025-12-01|EGG-Fusion: Efficient 3D Reconstruction with Geometry-aware Gaussian Surfel on the Fly|Xiaokun Pan\u7b49|[2512.01296](http://arxiv.org/pdf/2512.01296)|\u65e0|\u25c6 Real-time 3D reconstruction is a fundamental task in computer graphics.\n\u25c6 Recently, differentiable-rendering-based SLAM system has demonstrated significant potential, enabling photorealistic scene rendering through learnable scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS).\n\u25c6 Current differentiable rendering methods face dual challenges in real-time computation and sensor noise sensitivity, leading to degraded geometric fidelity in scene reconstruction and limited practicality.|\n",
    "2512.02932": "|2025-12-02|EGGS: Exchangeable 2D/3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis|Yancheng Zhang\u7b49|[2512.02932](http://arxiv.org/pdf/2512.02932)|\u65e0|\u25c6 Novel view synthesis (NVS) is crucial in computer vision and graphics, with wide applications in AR, VR, and autonomous driving.\n\u25c6 While 3D Gaussian Splatting (3DGS) enables real-time rendering with high appearance fidelity, it suffers from multi-view inconsistencies, limiting geometric accuracy.\n\u25c6 In contrast, 2D Gaussian Splatting (2DGS) enforces multi-view consistency but compromises texture details.|\n",
    "2512.02664": "|2025-12-02|PolarGuide-GSDR: 3D Gaussian Splatting Driven by Polarization Priors and Deferred Reflection for Real-World Reflective Scenes|Derui Shan\u7b49|[2512.02664](http://arxiv.org/pdf/2512.02664)|\u65e0|\u25c6 Polarization-aware Neural Radiance Fields (NeRF) enable novel view synthesis of specular-reflection scenes but face challenges in slow training, inefficient rendering, and strong dependencies on material/viewpoint assumptions.\n\u25c6 However, 3D Gaussian Splatting (3DGS) enables real-time rendering yet struggles with accurate reflection reconstruction from reflection-geometry entanglement, adding a deferred reflection module introduces environment map dependence.\n\u25c6 We address these limitations by proposing PolarGuide-GSDR, a polarization-forward-guided paradigm establishing a bidirectional coupling mechanism between polarization and 3DGS: first 3DGS's geometric priors are leveraged to resolve polarization ambiguity, and then the refined polarization information cues are used to guide 3DGS's normal and spherical harmonic representation.|\n",
    "2512.02293": "|2025-12-02|VIGS-SLAM: Visual Inertial Gaussian Splatting SLAM|Zihan Zhu\u7b49|[2512.02293](http://arxiv.org/pdf/2512.02293)|\u65e0|\u25c6 We present VIGS-SLAM, a visual-inertial 3D Gaussian Splatting SLAM system that achieves robust real-time tracking and high-fidelity reconstruction.\n\u25c6 Although recent 3DGS-based SLAM methods achieve dense and photorealistic mapping, their purely visual design degrades under motion blur, low texture, and exposure variations.\n\u25c6 Our method tightly couples visual and inertial cues within a unified optimization framework, jointly refining camera poses, depths, and IMU states.|\n",
    "2512.02172": "|2025-12-01|SplatSuRe: Selective Super-Resolution for Multi-view Consistent 3D Gaussian Splatting|Pranav Asthana\u7b49|[2512.02172](http://arxiv.org/pdf/2512.02172)|\u65e0|\u25c6 3D Gaussian Splatting (3DGS) enables high-quality novel view synthesis, motivating interest in generating higher-resolution renders than those available during training.\n\u25c6 A natural strategy is to apply super-resolution (SR) to low-resolution (LR) input views, but independently enhancing each image introduces multi-view inconsistencies, leading to blurry renders.\n\u25c6 Prior methods attempt to mitigate these inconsistencies through learned neural components, temporally consistent video priors, or joint optimization on LR and SR views, but all uniformly apply SR across every image.|\n",
    "2512.03621": "|2025-12-03|ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation|Yaokun Li\u7b49|[2512.03621](http://arxiv.org/pdf/2512.03621)|\u65e0|\u25c6 We propose ReCamDriving, a purely vision-based, camera-controlled novel-trajectory video generation framework.\n\u25c6 While repair-based methods fail to restore complex artifacts and LiDAR-based approaches rely on sparse and incomplete cues, ReCamDriving leverages dense and scene-complete 3DGS renderings for explicit geometric guidance, achieving precise camera-controllable generation.\n\u25c6 To mitigate overfitting to restoration behaviors when conditioned on 3DGS renderings, ReCamDriving adopts a two-stage training paradigm: the first stage uses camera poses for coarse control, while the second stage incorporates 3DGS renderings for fine-grained viewpoint and geometric guidance.|\n",
    "2512.03422": "|2025-12-03|What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models|Tianchen Deng\u7b49|[2512.03422](http://arxiv.org/pdf/2512.03422)|\u65e0|\u25c6 In this paper, we provide a comprehensive overview of existing scene representation methods for robotics, covering traditional representations such as point clouds, voxels, signed distance functions (SDF), and scene graphs, as well as more recent neural representations like Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and the emerging Foundation Models.\n\u25c6 While current SLAM and localization systems predominantly rely on sparse representations like point clouds and voxels, dense scene representations are expected to play a critical role in downstream tasks such as navigation and obstacle avoidance.\n\u25c6 Moreover, neural representations such as NeRF, 3DGS, and foundation models are well-suited for integrating high-level semantic features and language-based priors, enabling more comprehensive 3D scene understanding and embodied intelligence.|\n",
    "2512.03210": "|2025-12-02|Flux4D: Flow-based Unsupervised 4D Reconstruction|Jingkang Wang\u7b49|[2512.03210](http://arxiv.org/pdf/2512.03210)|\u65e0|\u25c6 Reconstructing large-scale dynamic scenes from visual observations is a fundamental challenge in computer vision, with critical implications for robotics and autonomous systems.\n\u25c6 While recent differentiable rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved impressive photorealistic reconstruction, they suffer from scalability limitations and require annotations to decouple actor motion.\n\u25c6 Existing self-supervised methods attempt to eliminate explicit annotations by leveraging motion cues and geometric priors, yet they remain constrained by per-scene optimization and sensitivity to hyperparameter tuning.|\n"
  },
  "Depth Estimation": {
    "2512.01889": "|2025-12-01|KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM|Zaid Nasser\u7b49|[2512.01889](http://arxiv.org/pdf/2512.01889)|\u65e0|\u25c6 We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments.\n\u25c6 Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training.\n\u25c6 KM-ViPE tightly couples DINO visual features with geometric constraints through a high-level features based adaptive robust kernel that handles both moving objects and movable static objects (e.g., moving furniture in ego-centric views).|\n",
    "2512.01366": "|2025-12-01|BlinkBud: Detecting Hazards from Behind via Sampled Monocular 3D Detection on a Single Earbud|Yunzhe Li\u7b49|[2512.01366](http://arxiv.org/pdf/2512.01366)|\u65e0|\u25c6 Failing to be aware of speeding vehicles approaching from behind poses a huge threat to the road safety of pedestrians and cyclists.\n\u25c6 In this paper, we propose BlinkBud, which utilizes a single earbud and a paired phone to online detect hazardous objects approaching from behind of a user.\n\u25c6 The core idea is to accurately track visually identified objects utilizing a small number of sampled camera images taken from the earbud.|\n",
    "2512.01317": "|2025-12-01|Data-Driven Learnability Transition of Measurement-Induced Entanglement|Dongheng Qian\u7b49|[2512.01317](http://arxiv.org/pdf/2512.01317)|\u65e0|\u25c6 Measurement-induced entanglement (MIE) captures how local measurements generate long-range quantum correlations and drive dynamical phase transitions in many-body systems.\n\u25c6 Yet estimating MIE experimentally remains challenging: direct evaluation requires extensive post-selection over measurement outcomes, raising the question of whether MIE is accessible with only polynomial resources.\n\u25c6 We address this challenge by reframing MIE detection as a data-driven learning problem that assumes no prior knowledge of state preparation.|\n",
    "2512.03000": "|2025-12-03|DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling|Kairun Wen\u7b49|[2512.03000](http://arxiv.org/pdf/2512.03000)|\u65e0|\u25c6 Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities.\n\u25c6 However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet.\n\u25c6 To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video.|\n",
    "2512.02972": "|2025-12-02|BEVDilation: LiDAR-Centric Multi-Modal Fusion for 3D Object Detection|Guowen Zhang\u7b49|[2512.02972](http://arxiv.org/pdf/2512.02972)|\u65e0|\u25c6 Integrating LiDAR and camera information in the bird's eye view (BEV) representation has demonstrated its effectiveness in 3D object detection.\n\u25c6 However, because of the fundamental disparity in geometric accuracy between these sensors, indiscriminate fusion in previous methods often leads to degraded performance.\n\u25c6 In this paper, we propose BEVDilation, a novel LiDAR-centric framework that prioritizes LiDAR information in the fusion.|\n",
    "2512.02727": "|2025-12-02|DF-Mamba: Deformable State Space Modeling for 3D Hand Pose Estimation in Interactions|Yifan Zhou\u7b49|[2512.02727](http://arxiv.org/pdf/2512.02727)|\u65e0|\u25c6 Modeling daily hand interactions often struggles with severe occlusions, such as when two hands overlap, which highlights the need for robust feature learning in 3D hand pose estimation (HPE).\n\u25c6 To handle such occluded hand images, it is vital to effectively learn the relationship between local image features (e.g., for occluded joints) and global context (e.g., cues from inter-joints, inter-hands, or the scene).\n\u25c6 However, most current 3D HPE methods still rely on ResNet for feature extraction, and such CNN's inductive bias may not be optimal for 3D HPE due to its limited capability to model the global context.|\n",
    "2512.02263": "|2025-12-01|DepthScape: Authoring 2.5D Designs via Depth Estimation, Semantic Understanding, and Geometry Extraction|Xia Su\u7b49|[2512.02263](http://arxiv.org/pdf/2512.02263)|\u65e0|\u25c6 2.5D effects, such as occlusion and perspective foreshortening, enhance visual dynamics and realism by incorporating 3D depth cues into 2D designs.\n\u25c6 However, creating such effects remains challenging and labor-intensive due to the complexity of depth perception.\n\u25c6 We introduce DepthScape, a human-AI collaborative system that facilitates 2.5D effect creation by directly placing design elements into 3D reconstructions.|\n",
    "2512.04085": "|2025-12-03|Unique Lives, Shared World: Learning from Single-Life Videos|Tengda Han\u7b49|[2512.04085](http://arxiv.org/pdf/2512.04085)|\u65e0|\u25c6 We introduce the \"single-life\" learning paradigm, where we train a distinct vision model exclusively on egocentric videos captured by one individual.\n\u25c6 We leverage the multiple viewpoints naturally captured within a single life to learn a visual encoder in a self-supervised manner.\n\u25c6 Our experiments demonstrate three key findings.|\n",
    "2512.04069": "|2025-12-03|SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL|Siyi Chen\u7b49|[2512.04069](http://arxiv.org/pdf/2512.04069)|\u65e0|\u25c6 Vision Language Models (VLMs) demonstrate strong qualitative visual understanding, but struggle with metrically precise spatial reasoning required for embodied applications.\n\u25c6 The agentic paradigm promises that VLMs can use a wide variety of tools that could augment these capabilities, such as depth estimators, segmentation models, and pose estimators.\n\u25c6 Yet it remains an open challenge how to realize this vision without solely relying on handcrafted prompting strategies or enforcing fixed, predefined tool pipelines that limit VLMs' ability to discover optimal tool-use patterns.|\n",
    "2512.03971": "|2025-12-03|Approximate Optimal Active Learning of Decision Trees|Zunchen Huang\u7b49|[2512.03971](http://arxiv.org/pdf/2512.03971)|\u65e0|\u25c6 We consider the problem of actively learning an unknown binary decision tree using only membership queries, a setting in which the learner must reason about a large hypothesis space while maintaining formal guarantees.\n\u25c6 Rather than enumerating candidate trees or relying on heuristic impurity or entropy measures, we encode the entire space of bounded-depth decision trees symbolically in SAT formulas.\n\u25c6 We propose a symbolic method for active learning of decision trees, in which approximate model counting is used to estimate the reduction of the hypothesis space caused by each potential query, enabling near-optimal query selection without full model enumeration.|\n",
    "2512.03958": "|2025-12-03|MDE-AgriVLN: Agricultural Vision-and-Language Navigation with Monocular Depth Estimation|Xiaobei Zhao\u7b49|[2512.03958](http://arxiv.org/pdf/2512.03958)|\u65e0|\u25c6 Agricultural robots are serving as powerful assistants across a wide range of agricultural tasks, nevertheless, still heavily relying on manual operations or railway systems for movement.\n\u25c6 The AgriVLN method and the A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural domain, enabling a robot to navigate to a target position following a natural language instruction.\n\u25c6 Unlike human binocular vision, most agricultural robots are only given a single camera for monocular vision, which results in limited spatial perception.|\n",
    "2512.03632": "|2025-12-03|Three-dimensional modelling of drag anchor penetration using the material point method|Robert E. Bird\u7b49|[2512.03632](http://arxiv.org/pdf/2512.03632)|\u65e0|\u25c6 Drag embedment anchors are a key threat to buried subsea linear infrastructure, such as power/data cables and pipelines.\n\u25c6 For cables, selecting a burial depth is a compromise between protecting the cable from anchor strike and the increased cost of deeper installation.\n\u25c6 This presents an efficient large deformation, elasto-plastic Material Point Method-based soil-structure interaction predictive tool for the estimation of anchor penetration based on Cone Penetration Test (CPT) site investigation data.|\n",
    "2512.03559": "|2025-12-03|Postseismicity of slow-slip doublets discerned on the outermost of the Nankai Trough subduction megathrust|Dye SK Sato\u7b49|[2512.03559](http://arxiv.org/pdf/2512.03559)|\u65e0|\u25c6 Despite dissimilar slip rates, slow earthquakes are faulting as ordinary earthquakes are.\n\u25c6 It is therefore physically natural that slow earthquakes also cause postseismic motions similarly to ordinary earthquakes, even though coseismic and postseismic slips remain undifferentiated for slow earthquakes.\n\u25c6 We pursue the slow-earthquake postseismicity based on the analysis of a fault slip beneath the Bungo Channel, the westernmost region of the Nankai Trough subduction zone in southwestern Japan.|\n",
    "2512.03427": "|2025-12-03|Generalization Evaluation of Deep Stereo Matching Methods for UAV-Based Forestry Applications|Yida Lin\u7b49|[2512.03427](http://arxiv.org/pdf/2512.03427)|\u65e0|\u25c6 Autonomous UAV forestry operations require robust depth estimation methods with strong cross-domain generalization.\n\u25c6 However, existing evaluations focus on urban and indoor scenarios, leaving a critical gap for specialized vegetation-dense environments.\n\u25c6 We present the first systematic zero-shot evaluation of eight state-of-the-art stereo methods--RAFT-Stereo, IGEV, IGEV++, BridgeDepth, StereoAnywhere, DEFOM (plus baseline methods ACVNet, PSMNet, TCstereo)--spanning iterative refinement, foundation model, and zero-shot adaptation paradigms.|\n",
    "2512.03332": "|2025-12-03|A three-dimensional model for the reversal in the local large-scale interstellar magnetic field|Rebecca A. Booth\u7b49|[2512.03332](http://arxiv.org/pdf/2512.03332)|\u65e0|\u25c6 We probe the three-dimensional geometry of the large-scale Galactic magnetic field within 1 kpc of the Sun using the Dominion Radio Astrophysical Observatory (DRAO) Global Magneto-Ionic Medium Survey (GMIMS) of the Northern Sky (DRAGONS).\n\u25c6 DRAGONS is a new full polarization survey of the Northern sky from 350 to 1030 MHz covering declinations -20\u00b0 < $\u03b4$ < 90\u00b0 and a component of GMIMS.\n\u25c6 The first moment of the Faraday depth spectra produced from DRAGONS above 500 MHz reveals large-angular-scale Faraday depth structures with signs that alternate only once in the Southern Galactic hemisphere and twice in the Northern hemisphere, patterns shared by other Faraday rotation datasets.|\n"
  }
}