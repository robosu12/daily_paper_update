{
  "LiDAR SLAM": {
    "2511.23156": "|2025-12-01|Design loads for wave impacts -- introducing the Probabilistic Adaptive Screening (PAS) method for predicting extreme non-linear loads on maritime structures|Sanne M. van Essen\u7b49|[2511.23156](http://arxiv.org/pdf/2511.23156)|\u65e0|\u25c6 To ensure the safety of marine and coastal structures, extreme (design) values should be known at the design stage.\n\u25c6 But for such complex systems, estimating the magnitude of events which are both non-linear and rare is extremely challenging, and involves considerable computational cost to capture the high-fidelity physics.\n\u25c6 To address this challenge, we offer a new multi-fidelity screening method, Probabilistic Adaptive Screening (PAS), which accurately predicts extreme values of strongly non-linear wave-induced loads while minimising the required high-fidelity simulation duration.|\n",
    "2512.01889": "|2025-12-01|KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM|Zaid Nasser\u7b49|[2512.01889](http://arxiv.org/pdf/2512.01889)|\u65e0|\u25c6 We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments.\n\u25c6 Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training.\n\u25c6 KM-ViPE tightly couples DINO visual features with geometric constraints through a high-level features based adaptive robust kernel that handles both moving objects and movable static objects (e.g., moving furniture in ego-centric views).|\n",
    "2512.01850": "|2025-12-01|Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching|Yue Pan\u7b49|[2512.01850](http://arxiv.org/pdf/2512.01850)|\u65e0|\u25c6 Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization.\n\u25c6 In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered.\n\u25c6 Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud.|\n",
    "2512.01753": "|2025-12-01|AgriLiRa4D: A Multi-Sensor UAV Dataset for Robust SLAM in Challenging Agricultural Fields|Zhihao Zhan\u7b49|[2512.01753](http://arxiv.org/pdf/2512.01753)|\u65e0|\u25c6 Multi-sensor Simultaneous Localization and Mapping (SLAM) is essential for Unmanned Aerial Vehicles (UAVs) performing agricultural tasks such as spraying, surveying, and inspection.\n\u25c6 However, real-world, multi-modal agricultural UAV datasets that enable research on robust operation remain scarce.\n\u25c6 To address this gap, we present AgriLiRa4D, a multi-modal UAV dataset designed for challenging outdoor agricultural environments.|\n",
    "2512.01296": "|2025-12-01|EGG-Fusion: Efficient 3D Reconstruction with Geometry-aware Gaussian Surfel on the Fly|Xiaokun Pan\u7b49|[2512.01296](http://arxiv.org/pdf/2512.01296)|\u65e0|\u25c6 Real-time 3D reconstruction is a fundamental task in computer graphics.\n\u25c6 Recently, differentiable-rendering-based SLAM system has demonstrated significant potential, enabling photorealistic scene rendering through learnable scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS).\n\u25c6 Current differentiable rendering methods face dual challenges in real-time computation and sensor noise sensitivity, leading to degraded geometric fidelity in scene reconstruction and limited practicality.|\n",
    "2512.02293": "|2025-12-02|VIGS-SLAM: Visual Inertial Gaussian Splatting SLAM|Zihan Zhu\u7b49|[2512.02293](http://arxiv.org/pdf/2512.02293)|\u65e0|\u25c6 We present VIGS-SLAM, a visual-inertial 3D Gaussian Splatting SLAM system that achieves robust real-time tracking and high-fidelity reconstruction.\n\u25c6 Although recent 3DGS-based SLAM methods achieve dense and photorealistic mapping, their purely visual design degrades under motion blur, low texture, and exposure variations.\n\u25c6 Our method tightly couples visual and inertial cues within a unified optimization framework, jointly refining camera poses, depths, and IMU states.|\n",
    "2512.03422": "|2025-12-03|What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models|Tianchen Deng\u7b49|[2512.03422](http://arxiv.org/pdf/2512.03422)|\u65e0|\u25c6 In this paper, we provide a comprehensive overview of existing scene representation methods for robotics, covering traditional representations such as point clouds, voxels, signed distance functions (SDF), and scene graphs, as well as more recent neural representations like Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and the emerging Foundation Models.\n\u25c6 While current SLAM and localization systems predominantly rely on sparse representations like point clouds and voxels, dense scene representations are expected to play a critical role in downstream tasks such as navigation and obstacle avoidance.\n\u25c6 Moreover, neural representations such as NeRF, 3DGS, and foundation models are well-suited for integrating high-level semantic features and language-based priors, enabling more comprehensive 3D scene understanding and embodied intelligence.|\n",
    "2512.03397": "|2025-12-04|Surfel-LIO: Fast LiDAR-Inertial Odometry with Pre-computed Surfels and Hierarchical Z-order Voxel Hashing|Seungwon Choi\u7b49|[2512.03397](http://arxiv.org/pdf/2512.03397)|\u65e0|\u25c6 LiDAR-inertial odometry (LIO) is an active research area, as it enables accurate real-time state estimation in GPS-denied environments.\n\u25c6 Recent advances in map data structures and spatial indexing have significantly improved the efficiency of LIO systems.\n\u25c6 Nevertheless, we observe that two aspects may still leave room for improvement: (1) nearest neighbor search often requires examining multiple spatial units to gather sufficient points for plane fitting, and (2) plane parameters are typically recomputed at every iteration despite unchanged map geometry.|\n",
    "2512.04772": "|2025-12-04|TEMPO-VINE: A Multi-Temporal Sensor Fusion Dataset for Localization and Mapping in Vineyards|Mauro Martini\u7b49|[2512.04772](http://arxiv.org/pdf/2512.04772)|\u65e0|\u25c6 In recent years, precision agriculture has been introducing groundbreaking innovations in the field, with a strong focus on automation.\n\u25c6 However, research studies in robotics and autonomous navigation often rely on controlled simulations or isolated field trials.\n\u25c6 The absence of a realistic common benchmark represents a significant limitation for the diffusion of robust autonomous systems under real complex agricultural conditions.|\n",
    "2512.07775": "|2025-12-08|OptMap: Geometric Map Distillation via Submodular Maximization|David Thorne\u7b49|[2512.07775](http://arxiv.org/pdf/2512.07775)|\u65e0|\u25c6 Autonomous robots rely on geometric maps to inform a diverse set of perception and decision-making algorithms.\n\u25c6 As autonomy requires reasoning and planning on multiple scales of the environment, each algorithm may require a different map for optimal performance.\n\u25c6 Light Detection And Ranging (LiDAR) sensors generate an abundance of geometric data to satisfy these diverse requirements, but selecting informative, size-constrained maps is computationally challenging as it requires solving an NP-hard combinatorial optimization.|\n",
    "2512.07221": "|2025-12-08|Spatiotemporal Calibration and Ground Truth Estimation for High-Precision SLAM Benchmarking in Extended Reality|Zichao Shu\u7b49|[2512.07221](http://arxiv.org/pdf/2512.07221)|\u65e0|\u25c6 Simultaneous localization and mapping (SLAM) plays a fundamental role in extended reality (XR) applications.\n\u25c6 As the standards for immersion in XR continue to increase, the demands for SLAM benchmarking have become more stringent.\n\u25c6 Trajectory accuracy is the key metric, and marker-based optical motion capture (MoCap) systems are widely used to generate ground truth (GT) because of their drift-free and relatively accurate measurements.|\n",
    "2512.06868": "|2025-12-07|Dynamic Visual SLAM using a General 3D Prior|Xingguang Zhong\u7b49|[2512.06868](http://arxiv.org/pdf/2512.06868)|\u65e0|\u25c6 Reliable incremental estimation of camera poses and 3D reconstruction is key to enable various applications including robotics, interactive visualization, and augmented reality.\n\u25c6 However, this task is particularly challenging in dynamic natural environments, where scene dynamics can severely deteriorate camera pose estimation accuracy.\n\u25c6 In this work, we propose a novel monocular visual SLAM system that can robustly estimate camera poses in dynamic scenes.|\n",
    "2512.05299": "|2025-12-04|ARCAS: An Augmented Reality Collision Avoidance System with SLAM-Based Tracking for Enhancing VRU Safety|Ahmad Yehia\u7b49|[2512.05299](http://arxiv.org/pdf/2512.05299)|\u65e0|\u25c6 Vulnerable road users (VRUs) face high collision risks in mixed traffic, yet most existing safety systems prioritize driver or vehicle assistance over direct VRU support.\n\u25c6 This paper presents ARCAS, a real-time augmented reality collision avoidance system that provides personalized spatial alerts to VRUs via wearable AR headsets.\n\u25c6 By fusing roadside 360-degree 3D LiDAR with SLAM-based headset tracking and an automatic 3D calibration procedure, ARCAS accurately overlays world-locked 3D bounding boxes and directional arrows onto approaching hazards in the user's passthrough view.|\n",
    "2512.08653": "|2025-12-09|A Sensor-Aware Phenomenological Framework for Lidar Degradation Simulation and SLAM Robustness Evaluation|Doumegna Mawuto Koudjo Felix\u7b49|[2512.08653](http://arxiv.org/pdf/2512.08653)|\u65e0|\u25c6 Lidar-based SLAM systems are highly sensitive to adverse conditions such as occlusion, noise, and field-of-view (FoV) degradation, yet existing robustness evaluation methods either lack physical grounding or do not capture sensor-specific behavior.\n\u25c6 This paper presents a sensor-aware, phenomenological framework for simulating interpretable lidar degradations directly on real point clouds, enabling controlled and reproducible SLAM stress testing.\n\u25c6 Unlike image-derived corruption benchmarks (e.g., SemanticKITTI-C) or simulation-only approaches (e.g., lidarsim), the proposed system preserves per-point geometry, intensity, and temporal structure while applying structured dropout, FoV reduction, Gaussian noise, occlusion masking, sparsification, and motion distortion.|\n",
    "2512.08625": "|2025-12-09|OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics|Jisang Yoo\u7b49|[2512.08625](http://arxiv.org/pdf/2512.08625)|\u65e0|\u25c6 Simultaneous Localization and Mapping (SLAM) is a foundational component in robotics, AR/VR, and autonomous systems.\n\u25c6 With the rising focus on spatial AI in recent years, combining SLAM with semantic understanding has become increasingly important for enabling intelligent perception and interaction.\n\u25c6 Recent efforts have explored this integration, but they often rely on depth sensors or closed-set semantic models, limiting their scalability and adaptability in open-world environments.|\n",
    "2512.07969": "|2025-12-08|Sparse Variable Projection in Robotic Perception: Exploiting Separable Structure for Efficient Nonlinear Optimization|Alan Papalia\u7b49|[2512.07969](http://arxiv.org/pdf/2512.07969)|\u65e0|\u25c6 Robotic perception often requires solving large nonlinear least-squares (NLS) problems.\n\u25c6 While sparsity has been well-exploited to scale solvers, a complementary and underexploited structure is \\emph{separability} -- where some variables (e.g., visual landmarks) appear linearly in the residuals and, for any estimate of the remaining variables (e.g., poses), have a closed-form solution.\n\u25c6 Variable projection (VarPro) methods are a family of techniques that exploit this structure by analytically eliminating the linear variables and presenting a reduced problem in the remaining variables that has favorable properties.|\n",
    "2512.09608": "|2025-12-10|Super4DR: 4D Radar-centric Self-supervised Odometry and Gaussian-based Map Optimization|Zhiheng Li\u7b49|[2512.09608](http://arxiv.org/pdf/2512.09608)|\u65e0|\u25c6 Conventional SLAM systems using visual or LiDAR data often struggle in poor lighting and severe weather.\n\u25c6 Although 4D radar is suited for such environments, its sparse and noisy point clouds hinder accurate odometry estimation, while the radar maps suffer from obscure and incomplete structures.\n\u25c6 Thus, we propose Super4DR, a 4D radar-centric framework for learning-based odometry estimation and gaussian-based map optimization.|\n",
    "2512.09411": "|2025-12-10|D$^2$GSLAM: 4D Dynamic Gaussian Splatting SLAM|Siting Zhu\u7b49|[2512.09411](http://arxiv.org/pdf/2512.09411)|\u65e0|\u25c6 Recent advances in Dense Simultaneous Localization and Mapping (SLAM) have demonstrated remarkable performance in static environments.\n\u25c6 However, dense SLAM in dynamic environments remains challenging.\n\u25c6 Most methods directly remove dynamic objects and focus solely on static scene reconstruction, which ignores the motion information contained in these dynamic objects.|\n",
    "2512.10481": "|2025-12-11|Contact SLAM: An Active Tactile Exploration Policy Based on Physical Reasoning Utilized in Robotic Fine Blind Manipulation Tasks|Gaozhao Wang\u7b49|[2512.10481](http://arxiv.org/pdf/2512.10481)|\u65e0|\u25c6 Contact-rich manipulation is difficult for robots to execute and requires accurate perception of the environment.\n\u25c6 In some scenarios, vision is occluded.\n\u25c6 The robot can then no longer obtain real-time scene state information through visual feedback.|\n",
    "2512.10360": "|2025-12-11|CLASH: Collaborative Large-Small Hierarchical Framework for Continuous Vision-and-Language Navigation|Liuyi Wang\u7b49|[2512.10360](http://arxiv.org/pdf/2512.10360)|\u65e0|\u25c6 Vision-and-Language Navigation (VLN) requires robots to follow natural language instructions and navigate complex environments without prior maps.\n\u25c6 While recent vision-language large models demonstrate strong reasoning abilities, they often underperform task-specific panoramic small models in VLN tasks.\n\u25c6 To address this, we propose CLASH (Collaborative Large-Small Hierarchy), a VLN-CE framework that integrates a reactive small-model planner (RSMP) with a reflective large-model reasoner (RLMR).|\n",
    "2512.10128": "|2025-12-10|Inertial Magnetic SLAM Systems Using Low-Cost Sensors|Chuan Huang\u7b49|[2512.10128](http://arxiv.org/pdf/2512.10128)|\u65e0|\u25c6 Spatially inhomogeneous magnetic fields offer a valuable, non-visual information source for positioning.\n\u25c6 Among systems leveraging this, magnetic field-based simultaneous localization and mapping (SLAM) systems are particularly attractive because they can provide positioning information and build a magnetic field map on the fly.\n\u25c6 Moreover, they have bounded error within mapped regions.|\n",
    "2512.14428": "|2025-12-16|Odyssey: An Automotive Lidar-Inertial Odometry Dataset for GNSS-denied situations|Aaron Kurda\u7b49|[2512.14428](http://arxiv.org/pdf/2512.14428)|\u65e0|\u25c6 The development and evaluation of Lidar-Inertial Odometry (LIO) and Simultaneous Localization and Mapping (SLAM) systems requires a precise ground truth.\n\u25c6 The Global Navigation Satellite System (GNSS) is often used as a foundation for this, but its signals can be unreliable in obstructed environments due to multi-path effects or loss-of-signal.\n\u25c6 While existing datasets compensate for the sporadic loss of GNSS signals by incorporating Inertial Measurement Unit (IMU) measurements, the commonly used Micro-Electro-Mechanical Systems (MEMS) or Fiber Optic Gyroscope (FOG)-based systems do not permit the prolonged study of GNSS-denied environments.|\n",
    "2512.14340": "|2025-12-16|Field evaluation and optimization of a lightweight lidar-based UAV navigation system for dense boreal forest environments|Aleksi Karhunen\u7b49|[2512.14340](http://arxiv.org/pdf/2512.14340)|\u65e0|\u25c6 The interest in the usage of uncrewed aerial vehicles (UAVs) for forest applications has increased in recent years.\n\u25c6 While above-canopy flight has reached a high level of autonomy, navigating under-canopy remains a significant challenge.\n\u25c6 The use of autonomous UAVs could reduce the burden of data collection, which has motivated the development of numerous solutions for under-canopy autonomous flight.|\n",
    "2512.14189": "|2025-12-16|SUPER -- A Framework for Sensitivity-based Uncertainty-aware Performance and Risk Assessment in Visual Inertial Odometry|Johannes A. Gaus\u7b49|[2512.14189](http://arxiv.org/pdf/2512.14189)|\u65e0|\u25c6 While many visual odometry (VO), visual-inertial odometry (VIO), and SLAM systems achieve high accuracy, the majority of existing methods miss to assess risks at runtime.\n\u25c6 This paper presents SUPER (Sensitivity-based Uncertainty-aware PErformance and Risk assessment) that is a generic and explainable framework that propagates uncertainties via sensitivities for real-time risk assessment in VIO.\n\u25c6 The scientific novelty lies in the derivation of a real-time risk indicator that is backend-agnostic and exploits the Schur complement blocks of the Gauss-Newton normal matrix to propagate uncertainties.|\n",
    "2512.14032": "|2025-12-16|ACE-SLAM: Scene Coordinate Regression for Neural Implicit Real-Time SLAM|Ignacio Alzugaray\u7b49|[2512.14032](http://arxiv.org/pdf/2512.14032)|\u65e0|\u25c6 We present a novel neural RGB-D Simultaneous Localization And Mapping (SLAM) system that learns an implicit map of the scene in real time.\n\u25c6 For the first time, we explore the use of Scene Coordinate Regression (SCR) as the core implicit map representation in a neural SLAM pipeline, a paradigm that trains a lightweight network to directly map 2D image features to 3D global coordinates.\n\u25c6 SCR networks provide efficient, low-memory 3D map representations, enable extremely fast relocalization, and inherently preserve privacy, making them particularly suitable for neural implicit SLAM.|\n",
    "2512.14020": "|2025-12-16|Deep Learning Perspective of Scene Understanding in Autonomous Robots|Afia Maham\u7b49|[2512.14020](http://arxiv.org/pdf/2512.14020)|\u65e0|\u25c6 This paper provides a review of deep learning applications in scene understanding in autonomous robots, including innovations in object detection, semantic and instance segmentation, depth estimation, 3D reconstruction, and visual SLAM.\n\u25c6 It emphasizes how these techniques address limitations of traditional geometric models, improve depth perception in real time despite occlusions and textureless surfaces, and enhance semantic reasoning to understand the environment better.\n\u25c6 When these perception modules are integrated into dynamic and unstructured environments, they become more effective in decisionmaking, navigation and interaction.|\n",
    "2512.13974": "|2025-12-16|Autonomous Construction-Site Safety Inspection Using Mobile Robots: A Multilayer VLM-LLM Pipeline|Hossein Naderi\u7b49|[2512.13974](http://arxiv.org/pdf/2512.13974)|\u65e0|\u25c6 Construction safety inspection remains mostly manual, and automated approaches still rely on task-specific datasets that are hard to maintain in fast-changing construction environments due to frequent retraining.\n\u25c6 Meanwhile, field inspection with robots still depends on human teleoperation and manual reporting, which are labor-intensive.\n\u25c6 This paper aims to connect what a robot sees during autonomous navigation to the safety rules that are common in construction sites, automatically generating a safety inspection report.|\n",
    "2512.12377": "|2025-12-13|INDOOR-LiDAR: Bridging Simulation and Reality for Robot-Centric 360 degree Indoor LiDAR Perception -- A Robot-Centric Hybrid Dataset|Haichuan Li\u7b49|[2512.12377](http://arxiv.org/pdf/2512.12377)|\u65e0|\u25c6 We present INDOOR-LIDAR, a comprehensive hybrid dataset of indoor 3D LiDAR point clouds designed to advance research in robot perception.\n\u25c6 Existing indoor LiDAR datasets often suffer from limited scale, inconsistent annotation formats, and human-induced variability during data collection.\n\u25c6 INDOOR-LIDAR addresses these limitations by integrating simulated environments with real-world scans acquired using autonomous ground robots, providing consistent coverage and realistic sensor behavior under controlled variations.|\n",
    "2512.12228": "|2025-12-13|Semantic Zone based 3D Map Management for Mobile Robot|Huichang Yun\u7b49|[2512.12228](http://arxiv.org/pdf/2512.12228)|\u65e0|\u25c6 Mobile robots in large-scale indoor environments, such as hospitals and logistics centers, require accurate 3D spatial representations.\n\u25c6 However, 3D maps consume substantial memory, making it difficult to maintain complete map data within limited computational resources.\n\u25c6 Existing SLAM frameworks typically rely on geometric distance or temporal metrics for memory management, often resulting in inefficient data retrieval in spatially compartmentalized environments.|\n",
    "2512.12203": "|2025-12-13|Navigation Around Unknown Space Objects Using Visible-Thermal Image Fusion|Eric J. Elias\u7b49|[2512.12203](http://arxiv.org/pdf/2512.12203)|\u65e0|\u25c6 As the popularity of on-orbit operations grows, so does the need for precise navigation around unknown resident space objects (RSOs) such as other spacecraft, orbital debris, and asteroids.\n\u25c6 The use of Simultaneous Localization and Mapping (SLAM) algorithms is often studied as a method to map out the surface of an RSO and find the inspector's relative pose using a lidar or conventional camera.\n\u25c6 However, conventional cameras struggle during eclipse or shadowed periods, and lidar, though robust to lighting conditions, tends to be heavier, bulkier, and more power-intensive.|\n",
    "2512.15716": "|2025-12-17|Spatia: Video Generation with Updatable Spatial Memory|Jinjing Zhao\u7b49|[2512.15716](http://arxiv.org/pdf/2512.15716)|\u65e0|\u25c6 Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals.\n\u25c6 To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory.\n\u25c6 Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM.|\n",
    "2512.15080": "|2025-12-17|NAP3D: NeRF Assisted 3D-3D Pose Alignment for Autonomous Vehicles|Gaurav Bansal|[2512.15080](http://arxiv.org/pdf/2512.15080)|\u65e0|\u25c6 Accurate localization is essential for autonomous vehicles, yet sensor noise and drift over time can lead to significant pose estimation errors, particularly in long-horizon environments.\n\u25c6 A common strategy for correcting accumulated error is visual loop closure in SLAM, which adjusts the pose graph when the agent revisits previously mapped locations.\n\u25c6 These techniques typically rely on identifying visual mappings between the current view and previously observed scenes and often require fusing data from multiple sensors.|\n",
    "2512.14979": "|2025-12-17|A Parameter-Free Stochastic LineseArch Method (SLAM) for Minimizing Expectation Residuals|Qi Wang\u7b49|[2512.14979](http://arxiv.org/pdf/2512.14979)|\u65e0|\u25c6 Most existing rate and complexity guarantees for stochastic gradient methods in $L$-smooth settings mandates that such sequences be non-adaptive, non-increasing, and upper bounded by $\\tfrac{a}{L}$ for $a > 0$.\n\u25c6 This requires knowledge of $L$ and may preclude larger steps.\n\u25c6 Motivated by these shortcomings, we present an Armijo-enabled stochastic linesearch framework with standard stochastic zeroth- and first-order oracles.|\n",
    "2512.16461": "|2025-12-18|SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning|Tin Stribor Sohn\u7b49|[2512.16461](http://arxiv.org/pdf/2512.16461)|\u65e0|\u25c6 Autonomous robotic systems require spatio-temporal understanding of dynamic environments to ensure reliable navigation and interaction.\n\u25c6 While Vision-Language Models (VLMs) provide open-world semantic priors, they lack grounding in 3D geometry and temporal dynamics.\n\u25c6 Conversely, geometric perception captures structure and motion but remains semantically sparse.|\n",
    "2512.19567": "|2026-01-05|LIMOncello: Iterated Error-State Kalman Filter on the SGal(3) Manifold for Fast LiDAR-Inertial Odometry|Carlos P\u00e9rez-Ruiz\u7b49|[2512.19567](http://arxiv.org/pdf/2512.19567)|\u65e0|\u25c6 This work introduces LIMOncello, a tightly coupled LiDAR-Inertial Odometry system that models 6-DoF motion on the $\\mathrm{SGal}(3)$ manifold within an iterated error-state Kalman filter backend.\n\u25c6 Compared to state representations defined on $\\mathrm{SO}(3)\\times\\mathbb{R}^6$, the use of $\\mathrm{SGal}(3)$ provides a coherent and numerically stable discrete-time propagation model that helps limit drift in low-observability conditions.\n\u25c6 LIMOncello also includes a lightweight incremental i-Octree mapping backend that enables faster updates and substantially lower memory usage than incremental kd-tree style map structures, without relying on locality-restricted search heuristics.|\n",
    "2512.20355": "|2025-12-25|FAR-AVIO: Fast and Robust Schur-Complement Based Acoustic-Visual-Inertial Fusion Odometry with Sensor Calibration|Hao Wei\u7b49|[2512.20355](http://arxiv.org/pdf/2512.20355)|\u65e0|\u25c6 Underwater environments impose severe challenges to visual-inertial odometry systems, as strong light attenuation, marine snow and turbidity, together with weakly exciting motions, degrade inertial observability and cause frequent tracking failures over long-term operation.\n\u25c6 While tightly coupled acoustic-visual-inertial fusion, typically implemented through an acoustic Doppler Velocity Log (DVL) integrated with visual-inertial measurements, can provide accurate state estimation, the associated graph-based optimization is often computationally prohibitive for real-time deployment on resource-constrained platforms.\n\u25c6 Here we present FAR-AVIO, a Schur-Complement based, tightly coupled acoustic-visual-inertial odometry framework tailored for underwater robots.|\n",
    "2512.21573": "|2025-12-25|World-Coordinate Human Motion Retargeting via SAM 3D Body|Zhangzheng Tu\u7b49|[2512.21573](http://arxiv.org/pdf/2512.21573)|\u65e0|\u25c6 Recovering world-coordinate human motion from monocular videos with humanoid robot retargeting is significant for embodied intelligence and robotics.\n\u25c6 To avoid complex SLAM pipelines or heavy temporal models, we propose a lightweight, engineering-oriented framework that leverages SAM 3D Body (3DB) as a frozen perception backbone and uses the Momentum HumanRig (MHR) representation as a robot-friendly intermediate.\n\u25c6 Our method (i) locks the identity and skeleton-scale parameters of per tracked subject to enforce temporally consistent bone lengths, (ii) smooths per-frame predictions via efficient sliding-window optimization in the low-dimensional MHR latent space, and (iii) recovers physically plausible global root trajectories with a differentiable soft foot-ground contact model and contact-aware global optimization.|\n",
    "2512.22690": "|2025-12-27|Mesquite MoCap: Democratizing Real-Time Motion Capture with Affordable, Bodyworn IoT Sensors and WebXR SLAM|Poojan Vanani\u7b49|[2512.22690](http://arxiv.org/pdf/2512.22690)|\u65e0|\u25c6 Motion capture remains costly and complex to deploy, limiting use outside specialized laboratories.\n\u25c6 We present Mesquite, an open-source, low-cost inertial motion-capture system that combines a body-worn network of 15 IMU sensor nodes with a hip-worn Android smartphone for position tracking.\n\u25c6 A low-power wireless link streams quaternion orientations to a central USB dongle and a browser-based application for real-time visualization and recording.|\n",
    "2512.22393": "|2025-12-30|Simultaneous Source Separation, Synchronization, Localization and Mapping for 6G Systems|Alexander Venus\u7b49|[2512.22393](http://arxiv.org/pdf/2512.22393)|\u65e0|\u25c6 Multipath-based simultaneous localization and mapping (MP-SLAM) is a promising approach for future 6G networks to jointly estimate the positions of transmitters and receivers together with the propagation environment.\n\u25c6 In cooperative MP-SLAM, information collected by multiple mobile terminals (MTs) is fused to enhance accuracy and robustness.\n\u25c6 Existing methods, however, typically assume perfectly synchronized base stations (BSs) and orthogonal transmission sequences, rendering inter-BS interference at the MTs negligible.|\n",
    "2512.25008": "|2026-01-01|FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM|Yuchen Wu\u7b49|[2512.25008](http://arxiv.org/pdf/2512.25008)|\u65e0|\u25c6 We present FoundationSLAM, a learning-based monocular dense SLAM system that addresses the absence of geometric consistency in previous flow-based approaches for accurate and robust tracking and mapping.\n\u25c6 Our core idea is to bridge flow estimation with geometric reasoning by leveraging the guidance from foundation depth models.\n\u25c6 To this end, we first develop a Hybrid Flow Network that produces geometry-aware correspondences, enabling consistent depth and pose inference across diverse keyframes.|\n",
    "2601.00545": "|2026-01-13|Variable Elimination in Hybrid Factor Graphs for Discrete-Continuous Inference & Estimation|Varun Agrawal\u7b49|[2601.00545](http://arxiv.org/pdf/2601.00545)|\u65e0|\u25c6 Many hybrid problems in robotics involve both continuous and discrete components, and modeling them together for estimation tasks has been a long standing and difficult problem.\n\u25c6 Hybrid Factor Graphs give us a mathematical framework to model these types of problems, however existing approaches for solving them are based on approximations.\n\u25c6 In this work, we propose an efficient Hybrid Factor Graph framework alongwith a variable elimination algorithm to produce a hybrid Bayes network, which can then be used for exact Maximum A Posteriori estimation and marginalization over both sets of variables.|\n",
    "2601.00705": "|2025-12-28|RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization|Wei-Tse Cheng\u7b49|[2601.00705](http://arxiv.org/pdf/2601.00705)|\u65e0|\u25c6 We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization.\n\u25c6 Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization.\n\u25c6 This initialization stabilizes early mapping and accelerates convergence by roughly 20\\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines.|\n",
    "2601.02184": "|2026-01-05|Differential Barometric Altimetry for Submeter Vertical Localization and Floor Recognition Indoors|Yuhang Zhang\u7b49|[2601.02184](http://arxiv.org/pdf/2601.02184)|\u65e0|\u25c6 Accurate altitude estimation and reliable floor recognition are critical for mobile robot localization and navigation within complex multi-storey environments.\n\u25c6 In this paper, we present a robust, low-cost vertical estimation framework leveraging differential barometric sensing integrated within a fully ROS-compliant software package.\n\u25c6 Our system simultaneously publishes real-time altitude data from both a stationary base station and a mobile sensor, enabling precise and drift-free vertical localization.|\n",
    "2601.01144": "|2026-01-03|VISO: Robust Underwater Visual-Inertial-Sonar SLAM with Photometric Rendering for Dense 3D Reconstruction|Shu Pan\u7b49|[2601.01144](http://arxiv.org/pdf/2601.01144)|\u65e0|\u25c6 Visual challenges in underwater environments significantly hinder the accuracy of vision-based localisation and the high-fidelity dense reconstruction.\n\u25c6 In this paper, we propose VISO, a robust underwater SLAM system that fuses a stereo camera, an inertial measurement unit (IMU), and a 3D sonar to achieve accurate 6-DoF localisation and enable efficient dense 3D reconstruction with high photometric fidelity.\n\u25c6 We introduce a coarse-to-fine online calibration approach for extrinsic parameters estimation between the 3D sonar and the camera.|\n",
    "2601.02723": "|2026-01-06|Loop Closure using AnyLoc Visual Place Recognition in DPV-SLAM|Wenzheng Zhang\u7b49|[2601.02723](http://arxiv.org/pdf/2601.02723)|\u65e0|\u25c6 Loop closure is crucial for maintaining the accuracy and consistency of visual SLAM.\n\u25c6 We propose a method to improve loop closure performance in DPV-SLAM.\n\u25c6 Our approach integrates AnyLoc, a learning-based visual place recognition technique, as a replacement for the classical Bag of Visual Words (BoVW) loop detection method.|\n",
    "2601.04551": "|2026-01-08|Discrete Fourier Transform-based Point Cloud Compression for Efficient SLAM in Featureless Terrain|Riku Suzuki\u7b49|[2601.04551](http://arxiv.org/pdf/2601.04551)|\u65e0|\u25c6 Simultaneous Localization and Mapping (SLAM) is an essential technology for the efficiency and reliability of unmanned robotic exploration missions.\n\u25c6 While the onboard computational capability and communication bandwidth are critically limited, the point cloud data handled by SLAM is large in size, attracting attention to data compression methods.\n\u25c6 To address such a problem, in this paper, we propose a new method for compressing point cloud maps by exploiting the Discrete Fourier Transform (DFT).|\n",
    "2601.04493": "|2026-01-08|Fast Continuum Robot Shape and External Load State Estimation on SE(3)|James M. Ferguson\u7b49|[2601.04493](http://arxiv.org/pdf/2601.04493)|\u65e0|\u25c6 Previous on-manifold approaches to continuum robot state estimation have typically adopted simplified Cosserat rod models, which cannot directly account for actuation inputs or external loads.\n\u25c6 We introduce a general framework that incorporates uncertainty models for actuation (e.g., tendon tensions), applied forces and moments, process noise, boundary conditions, and arbitrary backbone measurements.\n\u25c6 By adding temporal priors across time steps, our method additionally performs joint estimation in both the spatial (arclength) and temporal domains, enabling full \\textit{spacetime} state estimation.|\n",
    "2601.05805": "|2026-01-09|InsSo3D: Inertial Navigation System and 3D Sonar SLAM for turbid environment inspection|Simon Archieri\u7b49|[2601.05805](http://arxiv.org/pdf/2601.05805)|\u65e0|\u25c6 This paper presents InsSo3D, an accurate and efficient method for large-scale 3D Simultaneous Localisation and Mapping (SLAM) using a 3D Sonar and an Inertial Navigation System (INS).\n\u25c6 Unlike traditional sonar, which produces 2D images containing range and azimuth information but lacks elevation information, 3D Sonar produces a 3D point cloud, which therefore does not suffer from elevation ambiguity.\n\u25c6 We introduce a robust and modern SLAM framework adapted to the 3D Sonar data using INS as prior, detecting loop closure and performing pose graph optimisation.|\n",
    "2601.05738": "|2026-01-09|FeatureSLAM: Feature-enriched 3D gaussian splatting SLAM in real time|Christopher Thirgood\u7b49|[2601.05738](http://arxiv.org/pdf/2601.05738)|\u65e0|\u25c6 We present a real-time tracking SLAM system that unifies efficient camera tracking with photorealistic feature-enriched mapping using 3D Gaussian Splatting (3DGS).\n\u25c6 Our main contribution is integrating dense feature rasterization into the novel-view synthesis, aligned with a visual foundation model.\n\u25c6 This yields strong semantics, going beyond basic RGB-D input, aiding both tracking and mapping accuracy.|\n",
    "2601.05105": "|2026-01-08|UniLiPs: Unified LiDAR Pseudo-Labeling with Geometry-Grounded Dynamic Scene Decomposition|Filippo Ghilotti\u7b49|[2601.05105](http://arxiv.org/pdf/2601.05105)|\u65e0|\u25c6 Unlabeled LiDAR logs, in autonomous driving applications, are inherently a gold mine of dense 3D geometry hiding in plain sight - yet they are almost useless without human labels, highlighting a dominant cost barrier for autonomous-perception research.\n\u25c6 In this work we tackle this bottleneck by leveraging temporal-geometric consistency across LiDAR sweeps to lift and fuse cues from text and 2D vision foundation models directly into 3D, without any manual input.\n\u25c6 We introduce an unsupervised multi-modal pseudo-labeling method relying on strong geometric priors learned from temporally accumulated LiDAR maps, alongside with a novel iterative update rule that enforces joint geometric-semantic consistency, and vice-versa detecting moving objects from inconsistencies.|\n",
    "2601.08110": "|2026-01-13|Efficient Incremental SLAM via Information-Guided and Selective Optimization|Reza Arablouei|[2601.08110](http://arxiv.org/pdf/2601.08110)|\u65e0|\u25c6 We present an efficient incremental SLAM back-end that achieves the accuracy of full batch optimization while substantially reducing computational cost.\n\u25c6 The proposed approach combines two complementary ideas: information-guided gating (IGG) and selective partial optimization (SPO).\n\u25c6 IGG employs an information-theoretic criterion based on the log-determinant of the information matrix to quantify the contribution of new measurements, triggering global optimization only when a significant information gain is observed.|\n",
    "2601.09665": "|2026-01-14|SCE-SLAM: Scale-Consistent Monocular SLAM via Scene Coordinate Embeddings|Yuchen Wu\u7b49|[2601.09665](http://arxiv.org/pdf/2601.09665)|\u65e0|\u25c6 Monocular visual SLAM enables 3D reconstruction from internet video and autonomous navigation on resource-constrained platforms, yet suffers from scale drift, i.e., the gradual divergence of estimated scale over long sequences.\n\u25c6 Existing frame-to-frame methods achieve real-time performance through local optimization but accumulate scale drift due to the lack of global constraints among independent windows.\n\u25c6 To address this, we propose SCE-SLAM, an end-to-end SLAM system that maintains scale consistency through scene coordinate embeddings, which are learned patch-level representations encoding 3D geometric relationships under a canonical scale reference.|\n",
    "2601.09578": "|2026-01-14|Multimodal Signal Processing For Thermo-Visible-Lidar Fusion In Real-time 3D Semantic Mapping|Jiajun Sun\u7b49|[2601.09578](http://arxiv.org/pdf/2601.09578)|\u65e0|\u25c6 In complex environments, autonomous robot navigation and environmental perception pose higher requirements for SLAM technology.\n\u25c6 This paper presents a novel method for semantically enhancing 3D point cloud maps with thermal information.\n\u25c6 By first performing pixel-level fusion of visible and infrared images, the system projects real-time LiDAR point clouds onto this fused image stream.|\n",
    "2601.09385": "|2026-01-14|SLAM-LLM: A Modular, Open-Source Multimodal Large Language Model Framework and Best Practice for Speech, Language, Audio and Music Processing|Ziyang Ma\u7b49|[2601.09385](http://arxiv.org/pdf/2601.09385)|\u65e0|\u25c6 The recent surge in open-source Multimodal Large Language Models (MLLM) frameworks, such as LLaVA, provides a convenient kickoff for artificial intelligence developers and researchers.\n\u25c6 However, most of the MLLM frameworks take vision as the main input modality, and provide limited in-depth support for the modality of speech, audio, and music.\n\u25c6 This situation hinders the development of audio-language models, and forces researchers to spend a lot of effort on code writing and hyperparameter tuning.|\n",
    "2601.08977": "|2026-01-13|Thermo-LIO: A Novel Multi-Sensor Integrated System for Structural Health Monitoring|Chao Yang\u7b49|[2601.08977](http://arxiv.org/pdf/2601.08977)|\u65e0|\u25c6 Traditional two-dimensional thermography, despite being non-invasive and useful for defect detection in the construction field, is limited in effectively assessing complex geometries, inaccessible areas, and subsurface defects.\n\u25c6 This paper introduces Thermo-LIO, a novel multi-sensor system that can enhance Structural Health Monitoring (SHM) by fusing thermal imaging with high-resolution LiDAR.\n\u25c6 To achieve this, the study first develops a multimodal fusion method combining thermal imaging and LiDAR, enabling precise calibration and synchronization of multimodal data streams to create accurate representations of temperature distributions in buildings.|\n",
    "2601.11514": "|2026-01-16|ShapeR: Robust Conditional 3D Shape Generation from Casual Captures|Yawar Siddiqui\u7b49|[2601.11514](http://arxiv.org/pdf/2601.11514)|\u65e0|\u25c6 Recent advances in 3D shape generation have achieved impressive results, but most existing methods rely on clean, unoccluded, and well-segmented inputs.\n\u25c6 Such conditions are rarely met in real-world scenarios.\n\u25c6 We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences.|\n",
    "2601.10814": "|2026-01-20|SurfSLAM: Sim-to-Real Underwater Stereo Reconstruction For Real-Time SLAM|Onur Bagoren\u7b49|[2601.10814](http://arxiv.org/pdf/2601.10814)|\u65e0|\u25c6 Localization and mapping are core perceptual capabilities for underwater robots.\n\u25c6 Stereo cameras provide a low-cost means of directly estimating metric depth to support these tasks.\n\u25c6 However, despite recent advances in stereo depth estimation on land, computing depth from image pairs in underwater scenes remains challenging.|\n",
    "2601.13252": "|2026-01-19|Autonomous Navigation at the Nano-Scale: Algorithms, Architectures, and Constraints|Mahmud S. Zango\u7b49|[2601.13252](http://arxiv.org/pdf/2601.13252)|\u65e0|\u25c6 Autonomous navigation for nano-scale unmanned aerial vehicles (nano-UAVs) is governed by extreme Size, Weight, and Power (SWaP) constraints (with the weight < 50 g and sub-100 mW onboard processor), distinguishing it fundamentally from standard robotic paradigms.\n\u25c6 This review synthesizes the state-of-the-art in sensing, computing, and control architectures designed specifically for these sub- 100mW computational envelopes.\n\u25c6 We critically analyse the transition from classical geometry-based methods to emerging \"Edge AI\" paradigms, including quantized deep neural networks deployed on ultra-low-power System-on-Chips (SoCs) and neuromorphic event-based control.|\n",
    "2601.12377": "|2026-01-18|R-VoxelMap: Accurate Voxel Mapping with Recursive Plane Fitting for Online LiDAR Odometry|Haobo Xi\u7b49|[2601.12377](http://arxiv.org/pdf/2601.12377)|\u65e0|\u25c6 This paper proposes R-VoxelMap, a novel voxel mapping method that constructs accurate voxel maps using a geometry-driven recursive plane fitting strategy to enhance the localization accuracy of online LiDAR odometry.\n\u25c6 VoxelMap and its variants typically fit and check planes using all points in a voxel, which may lead to plane parameter deviation caused by outliers, over segmentation of large planes, and incorrect merging across different physical planes.\n\u25c6 To address these issues, R-VoxelMap utilizes a geometry-driven recursive construction strategy based on an outlier detect-and-reuse pipeline.|\n",
    "2601.11617": "|2026-01-10|PointSLAM++: Robust Dense Neural Gaussian Point Cloud-based SLAM|Xu Wang\u7b49|[2601.11617](http://arxiv.org/pdf/2601.11617)|\u65e0|\u25c6 Real-time 3D reconstruction is crucial for robotics and augmented reality, yet current simultaneous localization and mapping(SLAM) approaches often struggle to maintain structural consistency and robust pose estimation in the presence of depth noise.\n\u25c6 This work introduces PointSLAM++, a novel RGB-D SLAM system that leverages a hierarchically constrained neural Gaussian representation to preserve structural relationships while generating Gaussian primitives for scene mapping.\n\u25c6 It also employs progressive pose optimization to mitigate depth sensor noise, significantly enhancing localization accuracy.|\n",
    "2601.16020": "|2026-01-22|Keyframe-Based Feed-Forward Visual Odometry|Weichen Dai\u7b49|[2601.16020](http://arxiv.org/pdf/2601.16020)|\u65e0|\u25c6 The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network.\n\u25c6 However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately.\n\u25c6 This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information.|\n",
    "2601.15946": "|2026-01-24|Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems|Zijie Chen\u7b49|[2601.15946](http://arxiv.org/pdf/2601.15946)|\u65e0|\u25c6 Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications.\n\u25c6 Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability.\n\u25c6 Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness.|\n",
    "2601.15684": "|2026-01-22|Parallelizable Riemannian Alternating Direction Method of Multipliers for Non-convex Pose Graph Optimization|Xin Chen\u7b49|[2601.15684](http://arxiv.org/pdf/2601.15684)|\u65e0|\u25c6 Pose graph optimization (PGO) is fundamental to robot perception and navigation systems, serving as the mathematical backbone for solving simultaneous localization and mapping (SLAM).\n\u25c6 Existing solvers suffer from polynomial growth in computational complexity with graph size, hindering real-time deployment in large-scale scenarios.\n\u25c6 In this paper, by duplicating variables and introducing equality constraints, we reformulate the problem and propose a Parallelizable Riemannian Alternating Direction Method of Multipliers (PRADMM) to solve it efficiently.|\n",
    "2601.18252": "|2026-01-26|Co-PLNet: A Collaborative Point-Line Network for Prompt-Guided Wireframe Parsing|Chao Wang\u7b49|[2601.18252](http://arxiv.org/pdf/2601.18252)|\u65e0|\u25c6 Wireframe parsing aims to recover line segments and their junctions to form a structured geometric representation useful for downstream tasks such as Simultaneous Localization and Mapping (SLAM).\n\u25c6 Existing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness.\n\u25c6 We present Co-PLNet, a point-line collaborative framework that exchanges spatial cues between the two tasks, where early detections are converted into spatial prompts via a Point-Line Prompt Encoder (PLP-Encoder), which encodes geometric attributes into compact and spatially aligned maps.|\n",
    "2601.19887": "|2026-01-27|VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction|Dominic Maggio\u7b49|[2601.19887](http://arxiv.org/pdf/2601.19887)|\u65e0|\u25c6 We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT.\n\u25c6 Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics.\n\u25c6 Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures.|\n",
    "2601.19557": "|2026-01-27|The S3LI Vulcano Dataset: A Dataset for Multi-Modal SLAM in Unstructured Planetary Environments|Riccardo Giubilato\u7b49|[2601.19557](http://arxiv.org/pdf/2601.19557)|\u65e0|\u25c6 We release the S3LI Vulcano dataset, a multi-modal dataset towards development and benchmarking of Simultaneous Localization and Mapping (SLAM) and place recognition algorithms that rely on visual and LiDAR modalities.\n\u25c6 Several sequences are recorded on the volcanic island of Vulcano, from the Aeolian Islands in Sicily, Italy.\n\u25c6 The sequences provide users with data from a variety of environments, textures and terrains, including basaltic or iron-rich rocks, geological formations from old lava channels, as well as dry vegetation and water.|\n",
    "2601.19489": "|2026-01-28|Fast Converging 3D Gaussian Splatting for 1-Minute Reconstruction|Ziyu Zhang\u7b49|[2601.19489](http://arxiv.org/pdf/2601.19489)|\u65e0|\u25c6 We present a fast 3DGS reconstruction pipeline designed to converge within one minute, developed for the SIGGRAPH Asia 3DGS Fast Reconstruction Challenge.\n\u25c6 The challenge consists of an initial round using SLAM-generated camera poses (with noisy trajectories) and a final round using COLMAP poses (highly accurate).\n\u25c6 To robustly handle these heterogeneous settings, we develop a two-stage solution.|\n",
    "2601.21506": "|2026-01-29|IROS: A Dual-Process Architecture for Real-Time VLM-Based Indoor Navigation|Joonhee Lee\u7b49|[2601.21506](http://arxiv.org/pdf/2601.21506)|\u65e0|\u25c6 Indoor mobile robot navigation requires fast responsiveness and robust semantic understanding, yet existing methods struggle to provide both.\n\u25c6 Classical geometric approaches such as SLAM offer reliable localization but depend on detailed maps and cannot interpret human-targeted cues (e.g., signs, room numbers) essential for indoor reasoning.\n\u25c6 Vision-Language-Action (VLA) models introduce semantic grounding but remain strictly reactive, basing decisions only on visible frames and failing to anticipate unseen intersections or reason about distant textual cues.|\n",
    "2601.21063": "|2026-01-28|Multi-Robot Decentralized Collaborative SLAM in Planetary Analogue Environments: Dataset, Challenges, and Lessons Learned|Pierre-Yves Lajoie\u7b49|[2601.21063](http://arxiv.org/pdf/2601.21063)|\u65e0|\u25c6 Decentralized collaborative simultaneous localization and mapping (C-SLAM) is essential to enable multirobot missions in unknown environments without relying on preexisting localization and communication infrastructure.\n\u25c6 This technology is anticipated to play a key role in the exploration of the Moon, Mars, and other planets.\n\u25c6 In this article, we share insights and lessons learned from C-SLAM experiments involving three robots operating on a Mars analogue terrain and communicating over an ad hoc network.|\n"
  },
  "Visual SLAM": {
    "2512.01889": "|2025-12-01|KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM|Zaid Nasser\u7b49|[2512.01889](http://arxiv.org/pdf/2512.01889)|\u65e0|\u25c6 We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments.\n\u25c6 Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training.\n\u25c6 KM-ViPE tightly couples DINO visual features with geometric constraints through a high-level features based adaptive robust kernel that handles both moving objects and movable static objects (e.g., moving furniture in ego-centric views).|\n",
    "2512.02293": "|2025-12-02|VIGS-SLAM: Visual Inertial Gaussian Splatting SLAM|Zihan Zhu\u7b49|[2512.02293](http://arxiv.org/pdf/2512.02293)|\u65e0|\u25c6 We present VIGS-SLAM, a visual-inertial 3D Gaussian Splatting SLAM system that achieves robust real-time tracking and high-fidelity reconstruction.\n\u25c6 Although recent 3DGS-based SLAM methods achieve dense and photorealistic mapping, their purely visual design degrades under motion blur, low texture, and exposure variations.\n\u25c6 Our method tightly couples visual and inertial cues within a unified optimization framework, jointly refining camera poses, depths, and IMU states.|\n",
    "2512.06868": "|2025-12-07|Dynamic Visual SLAM using a General 3D Prior|Xingguang Zhong\u7b49|[2512.06868](http://arxiv.org/pdf/2512.06868)|\u65e0|\u25c6 Reliable incremental estimation of camera poses and 3D reconstruction is key to enable various applications including robotics, interactive visualization, and augmented reality.\n\u25c6 However, this task is particularly challenging in dynamic natural environments, where scene dynamics can severely deteriorate camera pose estimation accuracy.\n\u25c6 In this work, we propose a novel monocular visual SLAM system that can robustly estimate camera poses in dynamic scenes.|\n",
    "2512.09343": "|2025-12-11|Development and Testing for Perception Based Autonomous Landing of a Long-Range QuadPlane|Ashik E Rasul\u7b49|[2512.09343](http://arxiv.org/pdf/2512.09343)|\u65e0|\u25c6 QuadPlanes combine the range efficiency of fixed-wing aircraft with the maneuverability of multi-rotor platforms for long-range autonomous missions.\n\u25c6 In GPS-denied or cluttered urban environments, perception-based landing is vital for reliable operation.\n\u25c6 Unlike structured landing zones, real-world sites are unstructured and highly variable, requiring strong generalization capabilities from the perception system.|\n",
    "2512.10128": "|2025-12-10|Inertial Magnetic SLAM Systems Using Low-Cost Sensors|Chuan Huang\u7b49|[2512.10128](http://arxiv.org/pdf/2512.10128)|\u65e0|\u25c6 Spatially inhomogeneous magnetic fields offer a valuable, non-visual information source for positioning.\n\u25c6 Among systems leveraging this, magnetic field-based simultaneous localization and mapping (SLAM) systems are particularly attractive because they can provide positioning information and build a magnetic field map on the fly.\n\u25c6 Moreover, they have bounded error within mapped regions.|\n",
    "2512.11886": "|2025-12-09|Enabling Autonomous Navigation in a Snake Robot through Visual-Inertial Odometry and Closed-Loop Trajectory Tracking Control|Mohammed Irfan Ali|[2512.11886](http://arxiv.org/pdf/2512.11886)|\u65e0|\u25c6 Snake robots offer exceptional mobility across extreme terrain inaccessible to conventional rovers, yet their highly articulated bodies present fundamental challenges for autonomous navigation in environments lacking external tracking infrastructure.\n\u25c6 This thesis develops a complete autonomy pipeline for COBRA, an 11 degree-of-freedom modular snake robot designed for planetary exploration.\n\u25c6 While the robot's biologically inspired serpentine gaits achieve impressive mobility, prior work has relied entirely on open-loop teleoperation.|\n",
    "2512.14189": "|2025-12-16|SUPER -- A Framework for Sensitivity-based Uncertainty-aware Performance and Risk Assessment in Visual Inertial Odometry|Johannes A. Gaus\u7b49|[2512.14189](http://arxiv.org/pdf/2512.14189)|\u65e0|\u25c6 While many visual odometry (VO), visual-inertial odometry (VIO), and SLAM systems achieve high accuracy, the majority of existing methods miss to assess risks at runtime.\n\u25c6 This paper presents SUPER (Sensitivity-based Uncertainty-aware PErformance and Risk assessment) that is a generic and explainable framework that propagates uncertainties via sensitivities for real-time risk assessment in VIO.\n\u25c6 The scientific novelty lies in the derivation of a real-time risk indicator that is backend-agnostic and exploits the Schur complement blocks of the Gauss-Newton normal matrix to propagate uncertainties.|\n",
    "2512.14020": "|2025-12-16|Deep Learning Perspective of Scene Understanding in Autonomous Robots|Afia Maham\u7b49|[2512.14020](http://arxiv.org/pdf/2512.14020)|\u65e0|\u25c6 This paper provides a review of deep learning applications in scene understanding in autonomous robots, including innovations in object detection, semantic and instance segmentation, depth estimation, 3D reconstruction, and visual SLAM.\n\u25c6 It emphasizes how these techniques address limitations of traditional geometric models, improve depth perception in real time despite occlusions and textureless surfaces, and enhance semantic reasoning to understand the environment better.\n\u25c6 When these perception modules are integrated into dynamic and unstructured environments, they become more effective in decisionmaking, navigation and interaction.|\n",
    "2512.15716": "|2025-12-17|Spatia: Video Generation with Updatable Spatial Memory|Jinjing Zhao\u7b49|[2512.15716](http://arxiv.org/pdf/2512.15716)|\u65e0|\u25c6 Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals.\n\u25c6 To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory.\n\u25c6 Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM.|\n",
    "2512.17553": "|2025-12-19|Deep Learning-based Robust Autonomous Navigation of Aerial Robots in Dense Forests|Guglielmo Del Col\u7b49|[2512.17553](http://arxiv.org/pdf/2512.17553)|\u65e0|\u25c6 Autonomous aerial navigation in dense natural environments remains challenging due to limited visibility, thin and irregular obstacles, GNSS-denied operation, and frequent perceptual degradation.\n\u25c6 This work presents an improved deep learning-based navigation framework that integrates semantically enhanced depth encoding with neural motion-primitive evaluation for robust flight in cluttered forests.\n\u25c6 Several modules are incorporated on top of the original sevae-ORACLE algorithm to address limitations observed during real-world deployment, including lateral control for sharper maneuvering, a temporal consistency mechanism to suppress oscillatory planning decisions, a stereo-based visual-inertial odometry solution for drift-resilient state estimation, and a supervisory safety layer that filters unsafe actions in real time.|\n",
    "2512.17505": "|2025-12-19|Adaptive Covariance and Quaternion-Focused Hybrid Error-State EKF/UKF for Visual-Inertial Odometry|Ufuk Asil\u7b49|[2512.17505](http://arxiv.org/pdf/2512.17505)|\u65e0|\u25c6 This study presents an innovative hybrid Visual-Inertial Odometry (VIO) method for Unmanned Aerial Vehicles (UAVs) that is resilient to environmental challenges and capable of dynamically assessing sensor reliability.\n\u25c6 Built upon a loosely coupled sensor fusion architecture, the system utilizes a novel hybrid Quaternion-focused Error-State EKF/UKF (Qf-ES-EKF/UKF) architecture to process inertial measurement unit (IMU) data.\n\u25c6 This architecture first propagates the entire state using an Error-State Extended Kalman Filter (ESKF) and then applies a targeted Scaled Unscented Kalman Filter (SUKF) step to refine only the orientation.|\n",
    "2512.19110": "|2025-12-22|Trifocal Tensor and Relative Pose Estimation with Known Vertical Direction|Tao Li\u7b49|[2512.19110](http://arxiv.org/pdf/2512.19110)|\u65e0|\u25c6 This work presents two novel solvers for estimating the relative poses among views with known vertical directions.\n\u25c6 The vertical directions of camera views can be easily obtained using inertial measurement units (IMUs) which have been widely used in autonomous vehicles, mobile phones, and unmanned aerial vehicles (UAVs).\n\u25c6 Given the known vertical directions, our lgorithms only need to solve for two rotation angles and two translation vectors.|\n",
    "2512.20475": "|2025-12-23|Drift-Corrected Monocular VIO and Perception-Aware Planning for Autonomous Drone Racing|Maulana Bisyir Azhari\u7b49|[2512.20475](http://arxiv.org/pdf/2512.20475)|\u65e0|\u25c6 The Abu Dhabi Autonomous Racing League(A2RL) x Drone Champions League competition(DCL) requires teams to perform high-speed autonomous drone racing using only a single camera and a low-quality inertial measurement unit -- a minimal sensor set that mirrors expert human drone racing pilots.\n\u25c6 This sensor limitation makes the system susceptible to drift from Visual-Inertial Odometry (VIO), particularly during long and fast flights with aggressive maneuvers.\n\u25c6 This paper presents the system developed for the championship, which achieved a competitive performance.|\n",
    "2512.20355": "|2025-12-25|FAR-AVIO: Fast and Robust Schur-Complement Based Acoustic-Visual-Inertial Fusion Odometry with Sensor Calibration|Hao Wei\u7b49|[2512.20355](http://arxiv.org/pdf/2512.20355)|\u65e0|\u25c6 Underwater environments impose severe challenges to visual-inertial odometry systems, as strong light attenuation, marine snow and turbidity, together with weakly exciting motions, degrade inertial observability and cause frequent tracking failures over long-term operation.\n\u25c6 While tightly coupled acoustic-visual-inertial fusion, typically implemented through an acoustic Doppler Velocity Log (DVL) integrated with visual-inertial measurements, can provide accurate state estimation, the associated graph-based optimization is often computationally prohibitive for real-time deployment on resource-constrained platforms.\n\u25c6 Here we present FAR-AVIO, a Schur-Complement based, tightly coupled acoustic-visual-inertial odometry framework tailored for underwater robots.|\n",
    "2512.25008": "|2026-01-01|FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM|Yuchen Wu\u7b49|[2512.25008](http://arxiv.org/pdf/2512.25008)|\u65e0|\u25c6 We present FoundationSLAM, a learning-based monocular dense SLAM system that addresses the absence of geometric consistency in previous flow-based approaches for accurate and robust tracking and mapping.\n\u25c6 Our core idea is to bridge flow estimation with geometric reasoning by leveraging the guidance from foundation depth models.\n\u25c6 To this end, we first develop a Hybrid Flow Network that produces geometry-aware correspondences, enabling consistent depth and pose inference across diverse keyframes.|\n",
    "2601.00702": "|2026-01-02|DefVINS: Visual-Inertial Odometry for Deformable Scenes|Samuel Cerezo\u7b49|[2601.00702](http://arxiv.org/pdf/2601.00702)|\u65e0|\u25c6 Deformable scenes violate the rigidity assumptions underpinning classical visual-inertial odometry (VIO), often leading to over-fitting to local non-rigid motion or severe drift when deformation dominates visual parallax.\n\u25c6 We introduce DefVINS, a visual-inertial odometry framework that explicitly separates a rigid, IMU-anchored state from a non--rigid warp represented by an embedded deformation graph.\n\u25c6 The system is initialized using a standard VIO procedure that fixes gravity, velocity, and IMU biases, after which non-rigid degrees of freedom are activated progressively as the estimation becomes well conditioned.|\n",
    "2601.02309": "|2026-01-09|360DVO: Deep Visual Odometry for Monocular 360-Degree Camera|Xiaopeng Guo\u7b49|[2601.02309](http://arxiv.org/pdf/2601.02309)|\u65e0|\u25c6 Monocular omnidirectional visual odometry (OVO) systems leverage 360-degree cameras to overcome field-of-view limitations of perspective VO systems.\n\u25c6 However, existing methods, reliant on handcrafted features or photometric objectives, often lack robustness in challenging scenarios, such as aggressive motion and varying illumination.\n\u25c6 To address this, we present 360DVO, the first deep learning-based OVO framework.|\n",
    "2601.02723": "|2026-01-06|Loop Closure using AnyLoc Visual Place Recognition in DPV-SLAM|Wenzheng Zhang\u7b49|[2601.02723](http://arxiv.org/pdf/2601.02723)|\u65e0|\u25c6 Loop closure is crucial for maintaining the accuracy and consistency of visual SLAM.\n\u25c6 We propose a method to improve loop closure performance in DPV-SLAM.\n\u25c6 Our approach integrates AnyLoc, a learning-based visual place recognition technique, as a replacement for the classical Bag of Visual Words (BoVW) loop detection method.|\n",
    "2601.07156": "|2026-01-12|Nonlinear Observer Design for Visual-Inertial Odometry|Mouaad Boughellaba\u7b49|[2601.07156](http://arxiv.org/pdf/2601.07156)|\u65e0|\u25c6 This paper addresses the problem of Visual-Inertial Odometry (VIO) for rigid body systems evolving in three-dimensional space.\n\u25c6 We introduce a novel matrix Lie group structure, denoted SE_{3+n}(3), that unifies the pose, gravity, linear velocity, and landmark positions within a consistent geometric framework tailored to the VIO problem.\n\u25c6 Building upon this formulation, we design an almost globally asymptotically stable nonlinear geometric observer that tightly integrates data from an Inertial Measurement Unit (IMU) and visual sensors.|\n",
    "2601.09665": "|2026-01-14|SCE-SLAM: Scale-Consistent Monocular SLAM via Scene Coordinate Embeddings|Yuchen Wu\u7b49|[2601.09665](http://arxiv.org/pdf/2601.09665)|\u65e0|\u25c6 Monocular visual SLAM enables 3D reconstruction from internet video and autonomous navigation on resource-constrained platforms, yet suffers from scale drift, i.e., the gradual divergence of estimated scale over long sequences.\n\u25c6 Existing frame-to-frame methods achieve real-time performance through local optimization but accumulate scale drift due to the lack of global constraints among independent windows.\n\u25c6 To address this, we propose SCE-SLAM, an end-to-end SLAM system that maintains scale consistency through scene coordinate embeddings, which are learned patch-level representations encoding 3D geometric relationships under a canonical scale reference.|\n",
    "2601.16020": "|2026-01-22|Keyframe-Based Feed-Forward Visual Odometry|Weichen Dai\u7b49|[2601.16020](http://arxiv.org/pdf/2601.16020)|\u65e0|\u25c6 The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network.\n\u25c6 However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately.\n\u25c6 This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information.|\n"
  },
  "Loop Closure": {
    "2512.01850": "|2025-12-01|Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching|Yue Pan\u7b49|[2512.01850](http://arxiv.org/pdf/2512.01850)|\u65e0|\u25c6 Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization.\n\u25c6 In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered.\n\u25c6 Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud.|\n",
    "2512.01768": "|2025-12-01|Exciton-Polariton hybrid skin-topological states|Ruiqi Bao\u7b49|[2512.01768](http://arxiv.org/pdf/2512.01768)|\u65e0|\u25c6 The non Hermitian skin effect, where bulk states accumulate at system boundaries, challenges the conventional bulk boundary correspondence.\n\u25c6 Here we propose a scheme to realize hybrid skin topological states in exciton polariton honeycomb lattices by introducing sublattice dependent gain and loss.\n\u25c6 This non Hermiticity couples with the intrinsic topological edge modes, leading to relocalization of edge states.|\n",
    "2512.01194": "|2025-12-01|RoboLoc: A Benchmark Dataset for Point Place Recognition and Localization in Indoor-Outdoor Integrated Environments|Jaejin Jeon\u7b49|[2512.01194](http://arxiv.org/pdf/2512.01194)|\u65e0|\u25c6 Robust place recognition is essential for reliable localization in robotics, particularly in complex environments with fre- quent indoor-outdoor transitions.\n\u25c6 However, existing LiDAR-based datasets often focus on outdoor scenarios and lack seamless domain shifts.\n\u25c6 In this paper, we propose RoboLoc, a benchmark dataset designed for GPS-free place recognition in indoor-outdoor environments with floor transitions.|\n",
    "2512.03046": "|2025-12-02|MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues|Zichen Liu\u7b49|[2512.03046](http://arxiv.org/pdf/2512.03046)|\u65e0|\u25c6 We propose MagicQuill V2, a novel system that introduces a \\textbf{layered composition} paradigm to generative image editing, bridging the gap between the semantic power of diffusion models and the granular control of traditional graphics software.\n\u25c6 While diffusion transformers excel at holistic generation, their use of singular, monolithic prompts fails to disentangle distinct user intentions for content, position, and appearance.\n\u25c6 To overcome this, our method deconstructs creative intent into a stack of controllable visual cues: a content layer for what to create, a spatial layer for where to place it, a structural layer for how it is shaped, and a color layer for its palette.|\n",
    "2512.02897": "|2025-12-02|Polar Perspectives: Evaluating 2-D LiDAR Projections for Robust Place Recognition with Visual Foundation Models|Pierpaolo Serio\u7b49|[2512.02897](http://arxiv.org/pdf/2512.02897)|\u65e0|\u25c6 This work presents a systematic investigation into how alternative LiDAR-to-image projections affect metric place recognition when coupled with a state-of-the-art vision foundation model.\n\u25c6 We introduce a modular retrieval pipeline that controls for backbone, aggregation, and evaluation protocol, thereby isolating the influence of the 2-D projection itself.\n\u25c6 Using consistent geometric and structural channels across multiple datasets and deployment scenarios, we identify the projection characteristics that most strongly determine discriminative power, robustness to environmental variation, and suitability for real-time autonomy.|\n",
    "2512.02293": "|2025-12-02|VIGS-SLAM: Visual Inertial Gaussian Splatting SLAM|Zihan Zhu\u7b49|[2512.02293](http://arxiv.org/pdf/2512.02293)|\u65e0|\u25c6 We present VIGS-SLAM, a visual-inertial 3D Gaussian Splatting SLAM system that achieves robust real-time tracking and high-fidelity reconstruction.\n\u25c6 Although recent 3DGS-based SLAM methods achieve dense and photorealistic mapping, their purely visual design degrades under motion blur, low texture, and exposure variations.\n\u25c6 Our method tightly couples visual and inertial cues within a unified optimization framework, jointly refining camera poses, depths, and IMU states.|\n",
    "2512.02215": "|2025-12-01|Magnetoelectric effect in the mixed valence polyoxovanadate cage V$_{12}$|Piotr Koz\u0142owski|[2512.02215](http://arxiv.org/pdf/2512.02215)|\u65e0|\u25c6 Development of spintronic and quantum computing devices increases demand for efficient, energy saving method of spin manipulation at molecular scale.\n\u25c6 Polyoxovanadate molecular magnets being susceptible to both electric and magnetic fields may serve here as a good base material.\n\u25c6 In this paper two isostructural anions [V$_{12}$As$_8$O$_{40}$(HCO$_2$)]$^{n-}$ (with $n=3,5$) featuring two different mixed-valence states with itinerant and localized valence electrons are studied.|\n",
    "2512.02108": "|2025-12-04|The Dependence of Earth Milankovitch Cycles on Martian Mass|Stephen R. Kane\u7b49|[2512.02108](http://arxiv.org/pdf/2512.02108)|\u65e0|\u25c6 The Milankovitch cycles of Earth result from gravitational interactions with other bodies in the Solar System.\n\u25c6 These interactions lead to slow changes in the orbit and angular momentum vector of Earth, and correspondingly influence Earth's climate evolution.\n\u25c6 Several studies have shown that Mars may play a significant role in these Milankovitch cycles, such as the 2.4 Myr eccentricity cycle related to perihelion precession dynamics.|\n",
    "2512.04814": "|2025-12-04|Shared Multi-modal Embedding Space for Face-Voice Association|Christopher Simic\u7b49|[2512.04814](http://arxiv.org/pdf/2512.04814)|\u65e0|\u25c6 The FAME 2026 challenge comprises two demanding tasks: training face-voice associations combined with a multilingual setting that includes testing on languages on which the model was not trained.\n\u25c6 Our approach consists of separate uni-modal processing pipelines with general face and voice feature extraction, complemented by additional age-gender feature extraction to support prediction.\n\u25c6 The resulting single-modal features are projected into a shared embedding space and trained with an Adaptive Angular Margin (AAM) loss.|\n",
    "2512.04772": "|2025-12-04|TEMPO-VINE: A Multi-Temporal Sensor Fusion Dataset for Localization and Mapping in Vineyards|Mauro Martini\u7b49|[2512.04772](http://arxiv.org/pdf/2512.04772)|\u65e0|\u25c6 In recent years, precision agriculture has been introducing groundbreaking innovations in the field, with a strong focus on automation.\n\u25c6 However, research studies in robotics and autonomous navigation often rely on controlled simulations or isolated field trials.\n\u25c6 The absence of a realistic common benchmark represents a significant limitation for the diffusion of robust autonomous systems under real complex agricultural conditions.|\n",
    "2512.06983": "|2025-12-07|On Memory: A comparison of memory mechanisms in world models|Eli J. Laird\u7b49|[2512.06983](http://arxiv.org/pdf/2512.06983)|\u65e0|\u25c6 World models enable agents to plan within imagined environments by predicting future states conditioned on past observations and actions.\n\u25c6 However, their ability to plan over long horizons is limited by the effective memory span of the backbone architecture.\n\u25c6 This limitation leads to perceptual drift in long rollouts, hindering the model's capacity to perform loop closures within imagined trajectories.|\n",
    "2512.06574": "|2025-12-06|General Computation using Slidable Tiles with Deterministic Global Forces|Alberto Avila-Jimenez\u7b49|[2512.06574](http://arxiv.org/pdf/2512.06574)|\u65e0|\u25c6 We study the computational power of the Full-Tilt model of motion planning, where slidable polyominos are moved maximally around a board by way of a sequence of directional ``tilts.'' We focus on the deterministic scenario in which the tilts constitute a repeated clockwise rotation.\n\u25c6 We show that general-purpose computation is possible within this framework by providing a direct and efficient simulation of space-bounded Turing machines in which one computational step of the machine is simulated per $O(1)$ rotations.\n\u25c6 We further show that the initial tape of the machine can be programmed by an initial tilt-sequence preceding the rotations.|\n",
    "2512.06402": "|2025-12-06|Innovation, Spillovers and Economic Geography|Jos\u00e9 M. Gaspar\u7b49|[2512.06402](http://arxiv.org/pdf/2512.06402)|\u65e0|\u25c6 We develop a Schumpeterian quality-ladder spatial model in which innovation arrivals depend on regional knowledge spillovers.\n\u25c6 A parsimonious reduced-form diffusion mechanism induces the convergence of regions' average distance to the global frontier quality.\n\u25c6 As a result, regional differences in knowledge levels stem residually from asymmetries in the spatial distribution of researchers and firms.|\n",
    "2512.06147": "|2025-12-05|GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers|Hochul Hwang\u7b49|[2512.06147](http://arxiv.org/pdf/2512.06147)|\u65e0|\u25c6 While commendable progress has been made in user-centric research on mobile assistive systems for blind and low-vision (BLV) individuals, references that directly inform robot navigation design remain rare.\n\u25c6 To bridge this gap, we conducted a comprehensive human study involving interviews with 26 guide dog handlers, four white cane users, nine guide dog trainers, and one O\\&M trainer, along with 15+ hours of observing guide dog-assisted walking.\n\u25c6 After de-identification, we open-sourced the dataset to promote human-centered development and informed decision-making for assistive systems for BLV people.|\n",
    "2512.09903": "|2025-12-10|YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos|Ryan Meegan\u7b49|[2512.09903](http://arxiv.org/pdf/2512.09903)|\u65e0|\u25c6 Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning.\n\u25c6 However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive.\n\u25c6 We address the problem of visual navigation when exploration videos of a large environment are available.|\n",
    "2512.09447": "|2025-12-10|Sequential Testing for Descriptor-Agnostic LiDAR Loop Closure in Repetitive Environments|Jaehyun Kim\u7b49|[2512.09447](http://arxiv.org/pdf/2512.09447)|\u65e0|\u25c6 We propose a descriptor-agnostic, multi-frame loop closure verification method that formulates LiDAR loop closure as a truncated Sequential Probability Ratio Test (SPRT).\n\u25c6 Instead of deciding from a single descriptor comparison or using fixed thresholds with late-stage Iterative Closest Point (ICP) vetting, the verifier accumulates a short temporal stream of descriptor similarities between a query and each candidate.\n\u25c6 It then issues an accept/reject decision adaptively once sufficient multi-frame evidence has been observed, according to user-specified Type-I/II error design targets.|\n",
    "2512.09071": "|2025-12-09|Adaptive Thresholding for Visual Place Recognition using Negative Gaussian Mixture Statistics|Nick Trinh\u7b49|[2512.09071](http://arxiv.org/pdf/2512.09071)|\u65e0|\u25c6 Visual place recognition (VPR) is an important component technology for camera-based mapping and navigation applications.\n\u25c6 This is a challenging problem because images of the same place may appear quite different for reasons including seasonal changes, weather illumination, structural changes to the environment, as well as transient pedestrian or vehicle traffic.\n\u25c6 Papers focusing on generating image descriptors for VPR report their results using metrics such as recall@K and ROC curves.|\n",
    "2512.10753": "|2025-12-11|Quantifying displacement: a gentrification's consequence via persistent homology|Rita Rodr\u00edguez V\u00e1zquez\u7b49|[2512.10753](http://arxiv.org/pdf/2512.10753)|\u65e0|\u25c6 Gentrification is the process by which wealthier individuals move into a previously lower-income neighbourhood.\n\u25c6 Among the effects of this multi-faceted phenomenon are rising living costs, cultural and social changes-where local traditions, businesses, and community networks are replaced or diluted by new, more affluent lifestyles-and population displacement, where long-term, lower-income residents are priced out by rising rents and property taxes.\n\u25c6 Despite its relevance, quantifying displacement presents difficulties stemming from lack of information on motives for relocation and from the fact that a long time-span must be analysed: displacement is a gradual process (leases end or conditions change at different times), impossible to capture in one data snapshot.|\n",
    "2512.10439": "|2025-12-11|HypeR Adaptivity: Joint $hr$-Adaptive Meshing via Hypergraph Multi-Agent Deep Reinforcement Learning|Niccol\u00f2 Grillo\u7b49|[2512.10439](http://arxiv.org/pdf/2512.10439)|\u65e0|\u25c6 Adaptive mesh refinement is central to the efficient solution of partial differential equations (PDEs) via the finite element method (FEM).\n\u25c6 Classical $r$-adaptivity optimizes vertex positions but requires solving expensive auxiliary PDEs such as the Monge-Amp\u00e8re equation, while classical $h$-adaptivity modifies topology through element subdivision but suffers from expensive error indicator computation and is constrained by isotropic refinement patterns that impose accuracy ceilings.\n\u25c6 Combined $hr$-adaptive techniques naturally outperform single-modality approaches, yet inherit both computational bottlenecks and the restricted cost-accuracy trade-off.|\n",
    "2512.14428": "|2025-12-16|Odyssey: An Automotive Lidar-Inertial Odometry Dataset for GNSS-denied situations|Aaron Kurda\u7b49|[2512.14428](http://arxiv.org/pdf/2512.14428)|\u65e0|\u25c6 The development and evaluation of Lidar-Inertial Odometry (LIO) and Simultaneous Localization and Mapping (SLAM) systems requires a precise ground truth.\n\u25c6 The Global Navigation Satellite System (GNSS) is often used as a foundation for this, but its signals can be unreliable in obstructed environments due to multi-path effects or loss-of-signal.\n\u25c6 While existing datasets compensate for the sporadic loss of GNSS signals by incorporating Inertial Measurement Unit (IMU) measurements, the commonly used Micro-Electro-Mechanical Systems (MEMS) or Fiber Optic Gyroscope (FOG)-based systems do not permit the prolonged study of GNSS-denied environments.|\n",
    "2512.14189": "|2025-12-16|SUPER -- A Framework for Sensitivity-based Uncertainty-aware Performance and Risk Assessment in Visual Inertial Odometry|Johannes A. Gaus\u7b49|[2512.14189](http://arxiv.org/pdf/2512.14189)|\u65e0|\u25c6 While many visual odometry (VO), visual-inertial odometry (VIO), and SLAM systems achieve high accuracy, the majority of existing methods miss to assess risks at runtime.\n\u25c6 This paper presents SUPER (Sensitivity-based Uncertainty-aware PErformance and Risk assessment) that is a generic and explainable framework that propagates uncertainties via sensitivities for real-time risk assessment in VIO.\n\u25c6 The scientific novelty lies in the derivation of a real-time risk indicator that is backend-agnostic and exploits the Schur complement blocks of the Gauss-Newton normal matrix to propagate uncertainties.|\n",
    "2512.14032": "|2025-12-16|ACE-SLAM: Scene Coordinate Regression for Neural Implicit Real-Time SLAM|Ignacio Alzugaray\u7b49|[2512.14032](http://arxiv.org/pdf/2512.14032)|\u65e0|\u25c6 We present a novel neural RGB-D Simultaneous Localization And Mapping (SLAM) system that learns an implicit map of the scene in real time.\n\u25c6 For the first time, we explore the use of Scene Coordinate Regression (SCR) as the core implicit map representation in a neural SLAM pipeline, a paradigm that trains a lightweight network to directly map 2D image features to 3D global coordinates.\n\u25c6 SCR networks provide efficient, low-memory 3D map representations, enable extremely fast relocalization, and inherently preserve privacy, making them particularly suitable for neural implicit SLAM.|\n",
    "2512.13276": "|2025-12-15|CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing|Yan Li\u7b49|[2512.13276](http://arxiv.org/pdf/2512.13276)|\u65e0|\u25c6 Instruction-based image editing with diffusion models has achieved impressive results, yet existing methods struggle with fine-grained instructions specifying precise attributes such as colors, positions, and quantities.\n\u25c6 While recent approaches employ Group Relative Policy Optimization (GRPO) for alignment, they optimize only at individual sampling steps, providing sparse feedback that limits trajectory-level control.\n\u25c6 We propose a unified framework CogniEdit, combining multi-modal reasoning with dense reward optimization that propagates gradients across consecutive denoising steps, enabling trajectory-level gradient flow through the sampling process.|\n",
    "2512.13055": "|2025-12-15|Towards Test-time Efficient Visual Place Recognition via Asymmetric Query Processing|Jaeyoon Kim\u7b49|[2512.13055](http://arxiv.org/pdf/2512.13055)|\u65e0|\u25c6 Visual Place Recognition (VPR) has advanced significantly with high-capacity foundation models like DINOv2, achieving remarkable performance.\n\u25c6 Nonetheless, their substantial computational cost makes deployment on resource-constrained devices impractical.\n\u25c6 In this paper, we introduce an efficient asymmetric VPR framework that incorporates a high-capacity gallery model for offline feature extraction with a lightweight query network for online processing.|\n",
    "2512.12954": "|2025-12-15|Linear convergence of relocated fixed-point iterations|Felipe Atenas\u7b49|[2512.12954](http://arxiv.org/pdf/2512.12954)|\u65e0|\u25c6 We establish linear convergence of relocated fixed-point iterations as introduced by Atenas et al.\n\u25c6 (2025) assuming the algorithmic operator satisfies a linear error bound.\n\u25c6 In particular, this framework applies to the setting where the algorithmic operator is a contraction.|\n",
    "2512.15080": "|2025-12-17|NAP3D: NeRF Assisted 3D-3D Pose Alignment for Autonomous Vehicles|Gaurav Bansal|[2512.15080](http://arxiv.org/pdf/2512.15080)|\u65e0|\u25c6 Accurate localization is essential for autonomous vehicles, yet sensor noise and drift over time can lead to significant pose estimation errors, particularly in long-horizon environments.\n\u25c6 A common strategy for correcting accumulated error is visual loop closure in SLAM, which adjusts the pose graph when the agent revisits previously mapped locations.\n\u25c6 These techniques typically rely on identifying visual mappings between the current view and previously observed scenes and often require fusing data from multiple sensors.|\n",
    "2512.17893": "|2025-12-19|Exploring the Effect of Basis Rotation on NQS Performance|Sven Benjamin Ko\u017ei\u0107\u7b49|[2512.17893](http://arxiv.org/pdf/2512.17893)|\u65e0|\u25c6 Neural Quantum States (NQS) use neural networks to represent wavefunctions of quantum many-body systems, but their performance depends on the choice of basis, yet the underlying mechanism remains poorly understood.\n\u25c6 We use a fully solvable one-dimensional Ising model to show that local basis rotations leave the loss landscape unchanged while relocating the exact wavefunction in parameter space, effectively increasing its geometric distance from typical initializations.\n\u25c6 By sweeping a rotation angle, we compute quantum Fisher information and Fubini-Study distances to quantify how the rotated wavefunction moves within the loss landscape.|\n",
    "2512.17510": "|2025-12-19|Autonomous Picosecond-Precision Synchronization in Measurement-Device-Independent Quantum Key Distribution|A. P. Pljonkin|[2512.17510](http://arxiv.org/pdf/2512.17510)|\u65e0|\u25c6 Measurement-device-independent quantum key distribution (MDI-QKD) eliminates detector side-channel attacks by relocating all measurements to an untrusted intermediate node.\n\u25c6 However, its practical implementation critically relies on picosecond-level temporal synchronization between spatially separated users.\n\u25c6 In this work, we present a physically motivated autonomous synchronization algorithm for fiber-based MDI-QKD networks that does not require auxiliary optical channels or shared clock references.|\n",
    "2512.18680": "|2025-12-21|Bridging the divide: Economic exchange and segregation in dual-income cities|D. Ortega\u7b49|[2512.18680](http://arxiv.org/pdf/2512.18680)|\u65e0|\u25c6 Segregation is a growing concern around the world.\n\u25c6 One of its main manifestations is the creation of ghettos, whose inhabitants have difficult access to well-paid jobs, which are often located far from their homes.\n\u25c6 In order to study this phenomenon, we propose an extension of Schelling's model of segregation to take into account the existence of economic exchanges.|\n",
    "2512.18613": "|2025-12-21|Text2Graph VPR: A Text-to-Graph Expert System for Explainable Place Recognition in Changing Environments|Saeideh Yousefzadeh\u7b49|[2512.18613](http://arxiv.org/pdf/2512.18613)|\u65e0|\u25c6 Visual Place Recognition (VPR) in long-term deployment requires reasoning beyond pixel similarity: systems must make transparent, interpretable decisions that remain robust under lighting, weather and seasonal change.\n\u25c6 We present Text2Graph VPR, an explainable semantic localization system that converts image sequences into textual scene descriptions, parses those descriptions into structured scene graphs, and reasons over the resulting graphs to identify places.\n\u25c6 Scene graphs capture objects, attributes and pairwise relations; we aggregate per-frame graphs into a compact place representation and perform retrieval with a dual-similarity mechanism that fuses learned Graph Attention Network (GAT) embeddings and a Shortest-Path (SP) kernel for structural matching.|\n",
    "2512.18279": "|2025-12-23|UniMPR: A Unified Framework for Multimodal Place Recognition with Heterogeneous Sensor Configurations|Zhangshuo Qi\u7b49|[2512.18279](http://arxiv.org/pdf/2512.18279)|\u65e0|\u25c6 Place recognition is a critical component of autonomous vehicles and robotics, enabling global localization in GPS-denied environments.\n\u25c6 Recent advances have spurred significant interest in multimodal place recognition (MPR), which leverages complementary strengths of multiple modalities.\n\u25c6 Despite its potential, most existing MPR methods still face three key challenges: (1) dynamically adapting to arbitrary modality inputs within a unified framework, (2) maintaining robustness with missing or degraded modalities, and (3) generalizing across diverse sensor configurations and setups.|\n",
    "2512.21078": "|2025-12-28|UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer|Tianchen Deng\u7b49|[2512.21078](http://arxiv.org/pdf/2512.21078)|\u65e0|\u25c6 Visual Place Recognition (VPR) has been traditionally formulated as a single-image retrieval task.\n\u25c6 Using multiple views offers clear advantages, yet this setting remains relatively underexplored and existing methods often struggle to generalize across diverse environments.\n\u25c6 In this work we introduce UniPR-3D, the first VPR architecture that effectively integrates information from multiple views.|\n",
    "2512.21883": "|2025-12-26|Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer|Tianchen Deng\u7b49|[2512.21883](http://arxiv.org/pdf/2512.21883)|\u65e0|\u25c6 Visual localization has traditionally been formulated as a pair-wise pose regression problem.\n\u25c6 Existing approaches mainly estimate relative poses between two images and employ a late-fusion strategy to obtain absolute pose estimates.\n\u25c6 However, the late motion average is often insufficient for effectively integrating spatial information, and its accuracy degrades in complex environments.|\n",
    "2512.24939": "|2025-12-31|Vibe Coding, Interface Flattening|Hongrui Jin|[2512.24939](http://arxiv.org/pdf/2512.24939)|\u65e0|\u25c6 Large language models are reshaping programming by enabling 'vibe coding': the development of softwares through natural-language interaction with model-driven toolchains.\n\u25c6 This article argues that vibe coding is best understood as interface flattening, a reconfiguration in which previously distinct modalities (GUI, CLI, and API) appear to converge into a single conversational surface, even as the underlying chain of translation from intention to machinic effect lengthens and thickens.\n\u25c6 Drawing on Friedrich Kittler's materialist media theory and Alexander Galloway's account of interfaces as sites of protocol control, the paper situates programming as a historically localised interface arrangement rather than an essential relation to computation.|\n",
    "2512.24657": "|2025-12-31|Antagonistic Bowden-Cable Actuation of a Lightweight Robotic Hand: Toward Dexterous Manipulation for Payload Constrained Humanoids|Sungjae Min\u7b49|[2512.24657](http://arxiv.org/pdf/2512.24657)|\u65e0|\u25c6 Humanoid robots toward human-level dexterity require robotic hands capable of simultaneously providing high grasping force, rapid actuation speeds, multiple degrees of freedom, and lightweight structures within human-like size constraints.\n\u25c6 Meeting these conflicting requirements remains challenging, as satisfying this combination typically necessitates heavier actuators and bulkier transmission systems, significantly restricting the payload capacity of robot arms.\n\u25c6 In this letter, we present a lightweight anthropomorphic hand actuated by Bowden cables, which uniquely combines rolling-contact joint optimization with antagonistic cable actuation, enabling single-motor-per-joint control with negligible cable-length deviation.|\n",
    "2512.24384": "|2025-12-30|Geometric Multi-Session Map Merging with Learned Local Descriptors|Yanlong Ma\u7b49|[2512.24384](http://arxiv.org/pdf/2512.24384)|\u65e0|\u25c6 Multi-session map merging is crucial for extended autonomous operations in large-scale environments.\n\u25c6 In this paper, we present GMLD, a learning-based local descriptor framework for large-scale multi-session point cloud map merging that systematically aligns maps collected across different sessions with overlapping regions.\n\u25c6 The proposed framework employs a keypoint-aware encoder and a plane-based geometric transformer to extract discriminative features for loop closure detection and relative pose estimation.|\n",
    "2601.01293": "|2026-01-03|From Random Walks to Thermal Rides: Universal Anomalous Transport in Soaring Flights|J\u00e9r\u00e9mie Vilpellet\u7b49|[2601.01293](http://arxiv.org/pdf/2601.01293)|\u65e0|\u25c6 Cross-country soaring flights rely on intermittent atmospheric updrafts to cover long distances, producing trajectories that alternate between rapid relocation and local exploration.\n\u25c6 From a large dataset of paraglider, hang glider, and sailplane flights, we uncover a universal transport law: beyond short ballistic times, horizontal motion is persistently sub-ballistic, with a Hurst exponent $\\approx 0.88$ largely independent of aircraft type.\n\u25c6 Phase-resolved analysis using a probabilistic segmentation method shows that this scaling arises from the fundamentally intermittent, two-dimensional, and directionally correlated nature of soaring transport, in which successive ballistic segments do not add coherently.|\n",
    "2601.02723": "|2026-01-06|Loop Closure using AnyLoc Visual Place Recognition in DPV-SLAM|Wenzheng Zhang\u7b49|[2601.02723](http://arxiv.org/pdf/2601.02723)|\u65e0|\u25c6 Loop closure is crucial for maintaining the accuracy and consistency of visual SLAM.\n\u25c6 We propose a method to improve loop closure performance in DPV-SLAM.\n\u25c6 Our approach integrates AnyLoc, a learning-based visual place recognition technique, as a replacement for the classical Bag of Visual Words (BoVW) loop detection method.|\n",
    "2601.03777": "|2026-01-07|Multi-agent Optimization of Non-cooperative Multimodal Mobility Systems|Md Nafees Fuad Rafi\u7b49|[2601.03777](http://arxiv.org/pdf/2601.03777)|\u65e0|\u25c6 While multimodal mobility systems have the potential to bring many benefits to travelers, drivers, the environment, and traffic congestion, such systems typically involve multiple non-cooperative decision-makers who may selfishly optimize their own objectives without considering the overall system benefits.\n\u25c6 This paper aims to investigate market-based interactions of travelers and ride-sourcing drivers in the context of multimodal mobility systems.\n\u25c6 We propose a unified mathematical modeling framework to capture the decentralized travelers and drivers' decision-making process and balance the network's demand and supply by equilibrium pricing.|\n",
    "2601.04829": "|2026-01-08|Earthquakes and cluster dynamics during Interseismic phases between the Northern and Central Apennines (Italy)|Marion Baques\u7b49|[2601.04829](http://arxiv.org/pdf/2601.04829)|\u65e0|\u25c6 In the last thirty years, the Northern and Central Apennines (Italy) have been affected by three main destructive seismic sequences: the 1997 Colfiorito (three events $M_L > 5.5$), the 2009 L'Aquila (one event $M_L > 5.5$), and the 2016--2017 Amatrice--Visso--Norcia (three events $M_L > 5.5$).\n\u25c6 Several studies have analysed the spatio-temporal evolution and processes driving each sequence, focusing mainly on the foreshock--mainshock--aftershock periods.\n\u25c6 Here, we focus on the 2018--2024 interseismic phase, aiming to unravel the long-term seismogenic behaviour of this region.|\n",
    "2601.05805": "|2026-01-09|InsSo3D: Inertial Navigation System and 3D Sonar SLAM for turbid environment inspection|Simon Archieri\u7b49|[2601.05805](http://arxiv.org/pdf/2601.05805)|\u65e0|\u25c6 This paper presents InsSo3D, an accurate and efficient method for large-scale 3D Simultaneous Localisation and Mapping (SLAM) using a 3D Sonar and an Inertial Navigation System (INS).\n\u25c6 Unlike traditional sonar, which produces 2D images containing range and azimuth information but lacks elevation information, 3D Sonar produces a 3D point cloud, which therefore does not suffer from elevation ambiguity.\n\u25c6 We introduce a robust and modern SLAM framework adapted to the 3D Sonar data using INS as prior, detecting loop closure and performing pose graph optimisation.|\n",
    "2601.07578": "|2026-01-12|Anisotropic anomalous Hall effect in distorted kagome GdTi3Bi4|Avdhesh K. Sharma\u7b49|[2601.07578](http://arxiv.org/pdf/2601.07578)|\u65e0|\u25c6 Topological kagome magnets offer a rich landscape for exploring the intricate interplay of quantum interactions among geometry, topology, spin, and correlation.\n\u25c6 GdTi3Bi4 crystallizes in layered Ti based kagome nets intertwined with zigzag Gd chains along the a axis and orders antiferromagnetically below 15 K.\n\u25c6 Here, we present the temperature and field dependent electrical transport of GdTi3Bi4 in different directions.|\n",
    "2601.06831": "|2026-01-11|SARA: Scene-Aware Reconstruction Accelerator|Jee Won Lee\u7b49|[2601.06831](http://arxiv.org/pdf/2601.06831)|\u65e0|\u25c6 We present SARA (Scene-Aware Reconstruction Accelerator), a geometry-driven pair selection module for Structure-from-Motion (SfM).\n\u25c6 Unlike conventional pipelines that select pairs based on visual similarity alone, SARA introduces geometry-first pair selection by scoring reconstruction informativeness - the product of overlap and parallax - before expensive matching.\n\u25c6 A lightweight pre-matching stage uses mutual nearest neighbors and RANSAC to estimate these cues, then constructs an Information-Weighted Spanning Tree (IWST) augmented with targeted edges for loop closure, long-baseline anchors, and weak-view reinforcement.|\n",
    "2601.06749": "|2026-01-11|Reconfiguration of Hamiltonian Paths and Cycles in Rectangular Grid Graphs|Albi Kazazi|[2601.06749](http://arxiv.org/pdf/2601.06749)|\u65e0|\u25c6 \\noindent An \\textit{\\(m \\times n\\) grid graph} is the induced subgraph of the square lattice whose vertex set consists of all integer grid points \\(\\{(i,j) : 0 \\leq i < m,\\ 0 \\leq j < n\\}\\).\n\u25c6 Let $H$ and $K$ be Hamiltonian cycles in an $m \\times n$ grid graph $G$.\n\u25c6 We study the problem of reconfiguring $H$ into $K$ using a sequence of local transformations called \\textit{moves}.|\n",
    "2601.06746": "|2026-01-11|Imaginary Gauge-steerable Edge Modes In Non-Hermitian Aubry-Andr\u00e9-Harper Model|Yazhuang Miao\u7b49|[2601.06746](http://arxiv.org/pdf/2601.06746)|\u65e0|\u25c6 We investigate a non-Hermitian Aubry-Andr\u00e9-Harper lattice exhibiting quasiperiodicity, featuring an imaginary gauge field that varies spatially but averages to zero.\n\u25c6 In the presence of open boundary conditions, this system is precisely mapped, through a nonunitary gauge transformation, to the Hermitian AAH model with balanced hopping terms.\n\u25c6 The mapping leaves the spectrum unchanged but reshapes each eigenfunction by a realization-dependent random-walk envelope.|\n",
    "2601.06442": "|2026-01-10|WHU-PCPR: A cross-platform heterogeneous point cloud dataset for place recognition in complex urban scenes|Xianghong Zou\u7b49|[2601.06442](http://arxiv.org/pdf/2601.06442)|\u65e0|\u25c6 Point Cloud-based Place Recognition (PCPR) demonstrates considerable potential in applications such as autonomous driving, robot localization and navigation, and map update.\n\u25c6 In practical applications, point clouds used for place recognition are often acquired from different platforms and LiDARs across varying scene.\n\u25c6 However, existing PCPR datasets lack diversity in scenes, platforms, and sensors, which limits the effective development of related research.|\n",
    "2601.08520": "|2026-01-13|Keyframe-based Dense Mapping with the Graph of View-Dependent Local Maps|Krzysztof Zielinski\u7b49|[2601.08520](http://arxiv.org/pdf/2601.08520)|\u65e0|\u25c6 In this article, we propose a new keyframe-based mapping system.\n\u25c6 The proposed method updates local Normal Distribution Transform maps (NDT) using data from an RGB-D sensor.\n\u25c6 The cells of the NDT are stored in 2D view-dependent structures to better utilize the properties and uncertainty model of RGB-D cameras.|\n",
    "2601.08175": "|2026-01-13|CogniMap3D: Cognitive 3D Mapping and Rapid Retrieval|Feiran Wang\u7b49|[2601.08175](http://arxiv.org/pdf/2601.08175)|\u65e0|\u25c6 We present CogniMap3D, a bioinspired framework for dynamic 3D scene understanding and reconstruction that emulates human cognitive processes.\n\u25c6 Our approach maintains a persistent memory bank of static scenes, enabling efficient spatial knowledge storage and rapid retrieval.\n\u25c6 CogniMap3D integrates three core capabilities: a multi-stage motion cue framework for identifying dynamic objects, a cognitive mapping system for storing, recalling, and updating static scenes across multiple visits, and a factor graph optimization strategy for refining camera poses.|\n",
    "2601.09248": "|2026-01-14|Hybrid guided variational autoencoder for visual place recognition|Ni Wang\u7b49|[2601.09248](http://arxiv.org/pdf/2601.09248)|\u65e0|\u25c6 Autonomous agents such as cars, robots and drones need to precisely localize themselves in diverse environments, including in GPS-denied indoor environments.\n\u25c6 One approach for precise localization is visual place recognition (VPR), which estimates the place of an image based on previously seen places.\n\u25c6 State-of-the-art VPR models require high amounts of memory, making them unwieldy for mobile deployment, while more compact models lack robustness and generalization capabilities.|\n",
    "2601.10225": "|2026-01-15|A Unified Framework for Kinematic Simulation of Rigid Foldable Structures|Dongwook Kwak\u7b49|[2601.10225](http://arxiv.org/pdf/2601.10225)|\u65e0|\u25c6 Origami-inspired structures with rigid panels now span thick, kirigami, and multi-sheet realizations, making unified kinematic analysis essential.\n\u25c6 Yet a general method that consolidates their loop constraints has been lacking.\n\u25c6 We present an automated approach that generates the Pfaffian constraint matrix for arbitrary rigid foldable structures (RFS).|\n",
    "2601.11107": "|2026-01-16|Modular and Mobile Capacity Planning for Hyperconnected Supply Chain Networks|Xiaoyue Liu\u7b49|[2601.11107](http://arxiv.org/pdf/2601.11107)|\u65e0|\u25c6 The increased volatility of markets and the pressing need for resource sustainability are driving supply chains towards more agile, distributed, and dynamic designs.\n\u25c6 Motivated by the Physical Internet initiative, we introduce the Dynamic Stochastic Modular and Mobile Capacity Planning (DSMMCP) problem, which fosters hyperconnectivity through a network-of-networks architecture with modular and mobile capacities.\n\u25c6 The problem addresses both demand and supply uncertainties by incorporating short-term leasing of modular facilities and dynamic relocation of resources.|\n",
    "2601.13809": "|2026-01-21|DroneVLA: VLA based Aerial Manipulation|Fawad Mehboob\u7b49|[2601.13809](http://arxiv.org/pdf/2601.13809)|\u65e0|\u25c6 As aerial platforms evolve from passive observers to active manipulators, the challenge shifts toward designing intuitive interfaces that allow non-expert users to command these systems naturally.\n\u25c6 This work introduces a novel concept of autonomous aerial manipulation system capable of interpreting high-level natural language commands to retrieve objects and deliver them to a human user.\n\u25c6 The system is intended to integrate a MediaPipe based on Grounding DINO and a Vision-Language-Action (VLA) model with a custom-built drone equipped with a 1-DOF gripper and an Intel RealSense RGB-D camera.|\n",
    "2601.13655": "|2026-01-20|Why Does the LLM Stop Computing: An Empirical Study of User-Reported Failures in Open-Source LLMs|Guangba Yu\u7b49|[2601.13655](http://arxiv.org/pdf/2601.13655)|\u65e0|\u25c6 The democratization of open-source Large Language Models (LLMs) allows users to fine-tune and deploy models on local infrastructure but exposes them to a First Mile deployment landscape.\n\u25c6 Unlike black-box API consumption, the reliability of user-managed orchestration remains a critical blind spot.\n\u25c6 To bridge this gap, we conduct the first large-scale empirical study of 705 real-world failures from the open-source DeepSeek, Llama, and Qwen ecosystems.|\n",
    "2601.12729": "|2026-01-19|DC-VLAQ: Query-Residual Aggregation for Robust Visual Place Recognition|Hanyu Zhu\u7b49|[2601.12729](http://arxiv.org/pdf/2601.12729)|\u65e0|\u25c6 One of the central challenges in visual place recognition (VPR) is learning a robust global representation that remains discriminative under large viewpoint changes, illumination variations, and severe domain shifts.\n\u25c6 While visual foundation models (VFMs) provide strong local features, most existing methods rely on a single model, overlooking the complementary cues offered by different VFMs.\n\u25c6 However, exploiting such complementary information inevitably alters token distributions, which challenges the stability of existing query-based global aggregation schemes.|\n",
    "2601.14934": "|2026-01-21|Designing DNA nanostar hydrogels with programmable degradation and antibody release|Giorgia Palombo\u7b49|[2601.14934](http://arxiv.org/pdf/2601.14934)|\u65e0|\u25c6 DNA nanostar (DNAns) hydrogels are promising materials for in vivo applications, including tissue regeneration and drug and antibody delivery.\n\u25c6 However, a systematic and quantitative understanding of the design principles controlling their degradation is lacking.\n\u25c6 Here, we investigate hydrogels made of three-armed DNAns with varying flexible joints, arm lengths, and mesh sizes and use restriction enzymes to cut the DNAns structures while monitoring the gel's degradation.|\n",
    "2601.15531": "|2026-01-21|Variable Stepsize Distributed Forward-Backward Splitting Methods as Relocated Fixed-Point Iterations|Felipe Atenas\u7b49|[2601.15531](http://arxiv.org/pdf/2601.15531)|\u65e0|\u25c6 We present a family of distributed forward-backward methods with variable stepsizes to find a solution of structured monotone inclusion problems.\n\u25c6 The framework is constructed by means of relocated fixed-point iterations, extending the approach introduced in arXiv:2507.07428 to conically averaged operators, thus including iteration operators for methods of forward-backward type devised by graphs.\n\u25c6 The family of methods we construct preserve the per-iteration computational cost and the convergence properties of their constant stepsize counterparts.|\n",
    "2601.15401": "|2026-01-21|Multi-Input Ciphertext Multiplication for Homomorphic Encryption|Sajjad Akherati\u7b49|[2601.15401](http://arxiv.org/pdf/2601.15401)|\u65e0|\u25c6 Homomorphic encryption (HE) enables arithmetic operations to be performed directly on encrypted data.\n\u25c6 It is essential for privacy-preserving applications such as machine learning, medical diagnosis, and financial data analysis.\n\u25c6 In popular HE schemes, ciphertext multiplication is only defined for two inputs.|\n",
    "2601.18714": "|2026-01-26|Low Cost, High Efficiency: LiDAR Place Recognition in Vineyards with Matryoshka Representation Learning|Judith Vilella-Cantos\u7b49|[2601.18714](http://arxiv.org/pdf/2601.18714)|\u65e0|\u25c6 Localization in agricultural environments is challenging due to their unstructured nature and lack of distinctive landmarks.\n\u25c6 Although agricultural settings have been studied in the context of object classification and segmentation, the place recognition task for mobile robots is not trivial in the current state of the art.\n\u25c6 In this study, we propose MinkUNeXt-VINE, a lightweight, deep-learning-based method that surpasses state-of-the-art methods in vineyard environments thanks to its pre-processing and Matryoshka Representation Learning multi-loss approach.|\n",
    "2601.18328": "|2026-01-26|MarioChart: Autonomous Tangibles as Active Proxy Interfaces for Embodied Casual Data Exploration|Shaozhang Dai\u7b49|[2601.18328](http://arxiv.org/pdf/2601.18328)|\u65e0|\u25c6 We introduce the notion of an Active Proxy interface, i.e.\n\u25c6 tangible models as proxies for physical data referents, supporting interactive exploration of data through active manipulation.\n\u25c6 We realise an active proxy data visualisation system, \"MarioChart\", using robot carts relocating themselves on a tabletop, e.g., to align with their data referents in a map or other visual layout.|\n",
    "2601.19887": "|2026-01-27|VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction|Dominic Maggio\u7b49|[2601.19887](http://arxiv.org/pdf/2601.19887)|\u65e0|\u25c6 We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT.\n\u25c6 Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics.\n\u25c6 Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures.|\n",
    "2601.19557": "|2026-01-27|The S3LI Vulcano Dataset: A Dataset for Multi-Modal SLAM in Unstructured Planetary Environments|Riccardo Giubilato\u7b49|[2601.19557](http://arxiv.org/pdf/2601.19557)|\u65e0|\u25c6 We release the S3LI Vulcano dataset, a multi-modal dataset towards development and benchmarking of Simultaneous Localization and Mapping (SLAM) and place recognition algorithms that rely on visual and LiDAR modalities.\n\u25c6 Several sequences are recorded on the volcanic island of Vulcano, from the Aeolian Islands in Sicily, Italy.\n\u25c6 The sequences provide users with data from a variety of environments, textures and terrains, including basaltic or iron-rich rocks, geological formations from old lava channels, as well as dry vegetation and water.|\n",
    "2601.19144": "|2026-01-27|Robust Out-of-Order Retrieval for Grid-Based Storage at Maximum Capacity|Tzvika Geft\u7b49|[2601.19144](http://arxiv.org/pdf/2601.19144)|\u65e0|\u25c6 This paper proposes a framework for improving the operational efficiency of automated storage systems under uncertainty.\n\u25c6 It considers a 2D grid-based storage for uniform-sized loads (e.g., containers, pallets, or totes), which are moved by a robot (or other manipulator) along a collision-free path in the grid.\n\u25c6 The loads are labeled (i.e., unique) and must be stored in a given sequence, and later be retrieved in a different sequence -- an operational pattern that arises in logistics applications, such as last-mile distribution centers and shipyards.|\n",
    "2601.18856": "|2026-01-26|Operationally induced preferred basis in unitary quantum mechanics|Vitaly Pronskikh|[2601.18856](http://arxiv.org/pdf/2601.18856)|\u65e0|\u25c6 The preferred-basis problem and the definite-outcome aspect of the measurement problem persist even if the detector is modeled unitarily, because experimental data are necessarily represented in a Boolean event algebra of mutually exclusive records whereas the theoretical description is naturally formulated in a noncommutative operator algebra with continuous unitary symmetry.\n\u25c6 This change of mathematical type constitutes the core of the 'cut': a structurally necessary interface from group-based kinematics to set-based counting.\n\u25c6 In the presented view the basis relevant for recorded outcomes is not determined by the system Hamiltonian alone; it is induced by the measurement mapping, i.e., by the detector channel together with the coarse-grained readout that defines an instrument.|\n",
    "2601.22198": "|2026-01-29|Advanced techniques and applications of LiDAR Place Recognition in Agricultural Environments: A Comprehensive Survey|Judith Vilella-Cantos\u7b49|[2601.22198](http://arxiv.org/pdf/2601.22198)|\u65e0|\u25c6 An optimal solution to the localization problem is essential for developing autonomous robotic systems.\n\u25c6 Apart from autonomous vehicles, precision agriculture is one of the elds that can bene t most from these systems.\n\u25c6 Although LiDAR place recognition is a widely used technique in recent years to achieve accurate localization, it is mostly used in urban settings.|\n"
  },
  "Image Matching": {
    "2512.01908": "|2025-12-01|SARL: Spatially-Aware Self-Supervised Representation Learning for Visuo-Tactile Perception|Gurmeher Khurana\u7b49|[2512.01908](http://arxiv.org/pdf/2512.01908)|\u65e0|\u25c6 Contact-rich robotic manipulation requires representations that encode local geometry.\n\u25c6 Vision provides global context but lacks direct measurements of properties such as texture and hardness, whereas touch supplies these cues.\n\u25c6 Modern visuo-tactile sensors capture both modalities in a single fused image, yielding intrinsically aligned inputs that are well suited to manipulation tasks requiring visual and tactile information.|\n",
    "2512.01850": "|2025-12-01|Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching|Yue Pan\u7b49|[2512.01850](http://arxiv.org/pdf/2512.01850)|\u65e0|\u25c6 Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization.\n\u25c6 In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered.\n\u25c6 Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud.|\n",
    "2512.01816": "|2025-12-01|Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights|Juanxi Tian\u7b49|[2512.01816](http://arxiv.org/pdf/2512.01816)|\u65e0|\u25c6 Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency.\n\u25c6 However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time.\n\u25c6 To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation.|\n",
    "2512.01643": "|2025-12-01|ViT$^3$: Unlocking Test-Time Training in Vision|Dongchen Han\u7b49|[2512.01643](http://arxiv.org/pdf/2512.01643)|\u65e0|\u25c6 Test-Time Training (TTT) has recently emerged as a promising direction for efficient sequence modeling.\n\u25c6 TTT reformulates attention operation as an online learning problem, constructing a compact inner model from key-value pairs at test time.\n\u25c6 This reformulation opens a rich and flexible design space while achieving linear computational complexity.|\n",
    "2512.01611": "|2025-12-01|Depth Matching Method Based on ShapeDTW for Oil-Based Mud Imager|Fengfeng Li\u7b49|[2512.01611](http://arxiv.org/pdf/2512.01611)|\u65e0|\u25c6 In well logging operations using the oil-based mud (OBM) microresistivity imager, which employs an interleaved design with upper and lower pad sets, depth misalignment issues persist between the pad images even after velocity correction.\n\u25c6 This paper presents a depth matching method for borehole images based on the Shape Dynamic Time Warping (ShapeDTW) algorithm.\n\u25c6 The method extracts local shape features to construct a morphologically sensitive distance matrix, better preserving structural similarity between sequences during alignment.|\n",
    "2512.01510": "|2025-12-01|Semantic-aware Random Convolution and Source Matching for Domain Generalization in Medical Image Segmentation|Franz Thaler\u7b49|[2512.01510](http://arxiv.org/pdf/2512.01510)|\u65e0|\u25c6 We tackle the challenging problem of single-source domain generalization (DG) for medical image segmentation.\n\u25c6 To this end, we aim for training a network on one domain (e.g., CT) and directly apply it to a different domain (e.g., MR) without adapting the model and without requiring images or annotations from the new domain during training.\n\u25c6 We propose a novel method for promoting DG when training deep segmentation networks, which we call SRCSM.|\n",
    "2512.01479": "|2025-12-01|Non-Markovian dynamics in ice nucleation|Pablo Montero de Hijes\u7b49|[2512.01479](http://arxiv.org/pdf/2512.01479)|\u65e0|\u25c6 In simulation studies of crystallisation, the size of the largest crystalline nucleus is often used as a reaction coordinate to monitor the progress of the nucleation process.\n\u25c6 Here, we investigate, for the case of homogeneous ice nucleation, whether the nucleus size exhibits Markovian dynamics, as assumed in classical nucleation theory.\n\u25c6 Using 300 independent nucleation trajectories generated by molecular dynamics, we evaluate the mean recurrence time required to reach selected values of the largest nucleus size.|\n",
    "2512.02944": "|2025-12-02|The Convex Matching Distance in Multiparameter Persistence|Patrizio Frosini\u7b49|[2512.02944](http://arxiv.org/pdf/2512.02944)|\u65e0|\u25c6 We introduce the convex matching distance, a novel metric for comparing functions with values in the real plane.\n\u25c6 This metric measures the maximal bottleneck distance between the persistence diagrams associated with the convex combinations of the two function components.\n\u25c6 Similarly to the traditional matching distance, the convex matching distance aggregates the information provided by two real-valued components.|\n",
    "2512.02920": "|2025-12-02|Learning Multimodal Embeddings for Traffic Accident Prediction and Causal Estimation|Ziniu Zhang\u7b49|[2512.02920](http://arxiv.org/pdf/2512.02920)|\u65e0|\u25c6 We consider analyzing traffic accident patterns using both road network data and satellite images aligned to road graph nodes.\n\u25c6 Previous work for predicting accident occurrences relies primarily on road network structural features while overlooking physical and environmental information from the road surface and its surroundings.\n\u25c6 In this work, we construct a large multimodal dataset across six U.S.|\n",
    "2512.02889": "|2025-12-02|Terahertz Emission from Spintronic Stack Nanodecorated with Drop-Cast Core-Shell Plasmonic Nanoparticles|Vittorio Cecconi\u7b49|[2512.02889](http://arxiv.org/pdf/2512.02889)|\u65e0|\u25c6 Spintronic emitters promise to revolutionise terahertz (THz) sources by converting ultrafast optical pulses into broadband THz radiation without phase-matching constraints.\n\u25c6 Because the conversion relies on spin-current injection across a nanometre-thin magnetic layer, its efficiency is ordinarily limited by weak optical coupling.\n\u25c6 Here, we present a demonstration of a drop-casting based approach to introduce ultrafast plasmonic-mediated coupling: a sparse-layer of silica-gold core-shell nanoparticles is deposited directly onto a W/Fe/Pt spintronic trilayer.|\n",
    "2512.02833": "|2025-12-02|A Comparative Study on How Data Normalization Affects Zero-Shot Generalization in Time Series Foundation Models|Ihab Ahmed\u7b49|[2512.02833](http://arxiv.org/pdf/2512.02833)|\u65e0|\u25c6 We investigate input normalization methods for Time-Series Foundation Models (TSFMs).\n\u25c6 While normalization is well-studied in dataset-specific time-series models, it remains overlooked in TSFMs where generalization is critical.\n\u25c6 Time-series data, unlike text or images, exhibits significant scale variation across domains and channels, coupled with non-stationarity, can undermine TSFM performance regardless of architectural complexity.|\n",
    "2512.02826": "|2025-12-02|From Navigation to Refinement: Revealing the Two-Stage Nature of Flow-based Diffusion Models through Oracle Velocity|Haoming Liu\u7b49|[2512.02826](http://arxiv.org/pdf/2512.02826)|\u65e0|\u25c6 Flow-based diffusion models have emerged as a leading paradigm for training generative models across images and videos.\n\u25c6 However, their memorization-generalization behavior remains poorly understood.\n\u25c6 In this work, we revisit the flow matching (FM) objective and study its marginal velocity field, which admits a closed-form expression, allowing exact computation of the oracle FM target.|\n",
    "2512.02805": "|2025-12-02|Direct observational evidence that higher-luminosity type 1 active galactic nuclei are most commonly triggered by galaxy mergers|Yongmin Yoon\u7b49|[2512.02805](http://arxiv.org/pdf/2512.02805)|\u65e0|\u25c6 We examine the connection between galaxy mergers and the triggering of active galactic nuclei (AGNs) using a sample of 614 type 1 AGNs at $z<0.07$, along with a control sample of inactive galaxies matched to the AGNs for comparison.\n\u25c6 We used tidal features, detected in deep images from the DESI Legacy Imaging Survey, as direct evidence of recent mergers.\n\u25c6 We find that the fraction of type 1 AGN hosts with tidal features ($f_T$) is higher for AGNs with higher luminosities and (to a lesser extent) more massive black holes.|\n",
    "2512.02783": "|2025-12-02|Exploring Definitions of Quality and Diversity in Sonic Measurement Spaces|Bj\u00f6rn \u00de\u00f3r J\u00f3nsson\u7b49|[2512.02783](http://arxiv.org/pdf/2512.02783)|\u65e0|\u25c6 Digital sound synthesis presents the opportunity to explore vast parameter spaces containing millions of configurations.\n\u25c6 Quality diversity (QD) evolutionary algorithms offer a promising approach to harness this potential, yet their success hinges on appropriate sonic feature representations.\n\u25c6 Existing QD methods predominantly employ handcrafted descriptors or supervised classifiers, potentially introducing unintended exploration biases and constraining discovery to familiar sonic regions.|\n",
    "2512.02768": "|2025-12-02|Diffusion-Prior Split Gibbs Sampling for Synthetic Aperture Radar Imaging under Incomplete Measurements|Hefei Gao\u7b49|[2512.02768](http://arxiv.org/pdf/2512.02768)|\u65e0|\u25c6 Synthetic aperture radar (SAR) imaging plays a critical role in all-weather, day-and-night remote sensing, yet reconstruction is often challenged by noise, undersampling, and complex scattering scenarios.\n\u25c6 Conventional methods, including matched filtering and sparsity-based compressed sensing, are limited in capturing intricate scene structures and frequently suffer from artifacts, elevated sidelobes, and loss of fine details.\n\u25c6 Recent diffusion models have demonstrated superior capability in representing high-order priors; however, existing diffusion-based SAR methods still yield degraded reconstructions due to oversimplified likelihood approximations in guided sampling.|\n",
    "2512.02737": "|2025-12-02|Beyond Paired Data: Self-Supervised UAV Geo-Localization from Reference Imagery Alone|Tristan Amadei\u7b49|[2512.02737](http://arxiv.org/pdf/2512.02737)|\u65e0|\u25c6 Image-based localization in GNSS-denied environments is critical for UAV autonomy.\n\u25c6 Existing state-of-the-art approaches rely on matching UAV images to geo-referenced satellite images; however, they typically require large-scale, paired UAV-satellite datasets for training.\n\u25c6 Such data are costly to acquire and often unavailable, limiting their applicability.|\n",
    "2512.02697": "|2025-12-02|GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization|Zixuan Song\u7b49|[2512.02697](http://arxiv.org/pdf/2512.02697)|\u65e0|\u25c6 Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image.\n\u25c6 However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable.\n\u25c6 It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image).|\n",
    "2512.03880": "|2025-12-03|Leveraging topological data analysis to estimate bone strength from micro-CT as a surrogate for advanced imaging|John Rick Manzanares\u7b49|[2512.03880](http://arxiv.org/pdf/2512.03880)|\u65e0|\u25c6 Accurate bone strength prediction is essential for assessing fracture risk, particularly in aging populations and individuals with osteoporosis.\n\u25c6 Bone imaging has evolved from X-rays and DXA to clinical computed tomography (CT), and now to advanced modalities such as high-resolution peripheral quantitative CT and synchrotron radiation CT, which offer unprecedented resolution of bone microarchitecture.\n\u25c6 However, analytical methods have not kept pace with these imaging advances.|\n",
    "2512.03715": "|2025-12-03|DINO-RotateMatch: A Rotation-Aware Deep Framework for Robust Image Matching in Large-Scale 3D Reconstruction|Kaichen Zhang\u7b49|[2512.03715](http://arxiv.org/pdf/2512.03715)|\u65e0|\u25c6 This paper presents DINO-RotateMatch, a deep-learning framework designed to address the chal lenges of image matching in large-scale 3D reconstruction from unstructured Internet images.\n\u25c6 The   method integrates a dataset-adaptive image pairing strategy with rotation-aware keypoint extraction and   matching.\n\u25c6 DINO is employed to retrieve semantically relevant image pairs in large collections, while   rotation-based augmentation captures orientation-dependent local features using ALIKED and Light Glue.|\n",
    "2512.03684": "|2025-12-03|A Novel Approach to Tomato Harvesting Using a Hybrid Gripper with Semantic Segmentation and Keypoint Detection|Shahid Ansari\u7b49|[2512.03684](http://arxiv.org/pdf/2512.03684)|\u65e0|\u25c6 This paper presents an autonomous tomato-harvesting system built around a hybrid robotic gripper that combines six soft auxetic fingers with a rigid exoskeleton and a latex basket to achieve gentle, cage-like grasping.\n\u25c6 The gripper is driven by a servo-actuated Scotch--yoke mechanism, and includes separator leaves that form a conical frustum for fruit isolation, with an integrated micro-servo cutter for pedicel cutting.\n\u25c6 For perception, an RGB--D camera and a Detectron2-based pipeline perform semantic segmentation of ripe/unripe tomatoes and keypoint localization of the pedicel and fruit center under occlusion and variable illumination.|\n",
    "2512.03660": "|2025-12-03|Linking Aneurysmal Geometry and Hemodynamics Using Computational Fluid Dynamics|Spyridon C. Katsoudas\u7b49|[2512.03660](http://arxiv.org/pdf/2512.03660)|\u65e0|\u25c6 The development and progression of abdominal aortic aneurysms (AAA) are related to complex flow patterns and wall-shear-driven mechanobiological stimuli, yet the quantitative relationship between aneurysmal geometry and hemodynamics remains poorly defined.\n\u25c6 In this study, we conducted a comprehensive hemodynamic analysis of 74 patient-specific abdominal aortas, representing one of the largest Computational Fluid Dynamics (CFD) cohorts reported to date.\n\u25c6 A multiscale framework coupling 0D-1D systemic circulation models with 3D stabilized finite-element simulations is used to generate physiologically consistent boundary conditions and high-fidelity flow fields.|\n",
    "2512.03598": "|2025-12-03|Memory-Guided Point Cloud Completion for Dental Reconstruction|Jianan Sun\u7b49|[2512.03598](http://arxiv.org/pdf/2512.03598)|\u65e0|\u25c6 Partial dental point clouds often suffer from large missing regions caused by occlusion and limited scanning views, which bias encoder-only global features and force decoders to hallucinate structures.\n\u25c6 We propose a retrieval-augmented framework for tooth completion that integrates a prototype memory into standard encoder--decoder pipelines.\n\u25c6 After encoding a partial input into a global descriptor, the model retrieves the nearest manifold prototype from a learnable memory and fuses it with the query feature through confidence-gated weighting before decoding.|\n",
    "2512.03346": "|2025-12-03|Hierarchical Attention for Sparse Volumetric Anomaly Detection in Subclinical Keratoconus|Lynn Kandakji\u7b49|[2512.03346](http://arxiv.org/pdf/2512.03346)|\u65e0|\u25c6 The detection of weak, spatially distributed anomalies in volumetric medical imaging remains a major challenge.\n\u25c6 The subtle, non-adjacent nature of early disease signals is often lost due to suboptimal architectural inductive biases: 2D/3D CNNs impose strong locality, while ViTs diffuse unconstrained global attention.\n\u25c6 This conflict leaves the optimal inductive structure for robust, sparse volumetric pattern recognition unresolved.|\n",
    "2512.05116": "|2025-12-04|Value Gradient Guidance for Flow Matching Alignment|Zhen Liu\u7b49|[2512.05116](http://arxiv.org/pdf/2512.05116)|\u65e0|\u25c6 While methods exist for aligning flow matching models--a popular and effective class of generative models--with human preferences, existing approaches fail to achieve both adaptation efficiency and probabilistically sound prior preservation.\n\u25c6 In this work, we leverage the theory of optimal control and propose VGG-Flow, a gradient-matching-based method for finetuning pretrained flow matching models.\n\u25c6 The key idea behind this algorithm is that the optimal difference between the finetuned velocity field and the pretrained one should be matched with the gradient field of a value function.|\n",
    "2512.05114": "|2025-12-04|Deep infant brain segmentation from multi-contrast MRI|Malte Hoffmann\u7b49|[2512.05114](http://arxiv.org/pdf/2512.05114)|\u65e0|\u25c6 Segmentation of magnetic resonance images (MRI) facilitates analysis of human brain development by delineating anatomical structures.\n\u25c6 However, in infants and young children, accurate segmentation is challenging due to development and imaging constraints.\n\u25c6 Pediatric brain MRI is notoriously difficult to acquire, with inconsistent availability of imaging modalities, substantial non-head anatomy in the field of view, and frequent motion artifacts.|\n",
    "2512.05081": "|2025-12-04|Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression|Jung Yi\u7b49|[2512.05081](http://arxiv.org/pdf/2512.05081)|\u65e0|\u25c6 Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration.\n\u25c6 We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation.\n\u25c6 To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning.|\n",
    "2512.05078": "|2025-12-04|Improving Posterior Inference of Galaxy Properties with Image-Based Conditional Flow Matching|Mikaeel Yunus\u7b49|[2512.05078](http://arxiv.org/pdf/2512.05078)|\u65e0|\u25c6 Estimating physical properties of galaxies from wide-field surveys remains a central challenge in astrophysics.\n\u25c6 While spectroscopy provides precise measurements, it is observationally expensive, and photometry discards morphological information that correlates with mass, star formation history, metallicity, and dust.\n\u25c6 We present a conditional flow matching (CFM) framework that leverages pixel-level imaging alongside photometry to improve posterior inference of galaxy properties.|\n",
    "2512.05016": "|2025-12-04|Generative Neural Video Compression via Video Diffusion Prior|Qi Mao\u7b49|[2512.05016](http://arxiv.org/pdf/2512.05016)|\u65e0|\u25c6 We present GNVC-VD, the first DiT-based generative neural video compression framework built upon an advanced video generation foundation model, where spatio-temporal latent compression and sequence-level generative refinement are unified within a single codec.\n\u25c6 Existing perceptual codecs primarily rely on pre-trained image generative priors to restore high-frequency details, but their frame-wise nature lacks temporal modeling and inevitably leads to perceptual flickering.\n\u25c6 To address this, GNVC-VD introduces a unified flow-matching latent refinement module that leverages a video diffusion transformer to jointly enhance intra- and inter-frame latents through sequence-level denoising, ensuring consistent spatio-temporal details.|\n",
    "2512.04966": "|2025-12-04|Environment-Aware Channel Inference via Cross-Modal Flow: From Multimodal Sensing to Wireless Channels|Guangming Liang\u7b49|[2512.04966](http://arxiv.org/pdf/2512.04966)|\u65e0|\u25c6 Accurate channel state information (CSI) underpins reliable and efficient wireless communication.\n\u25c6 However, acquiring CSI via pilot estimation incurs substantial overhead, especially in massive multiple-input multiple-output (MIMO) systems operating in high-Doppler environments.\n\u25c6 By leveraging the growing availability of environmental sensing data, this treatise investigates pilot-free channel inference that estimates complete CSI directly from multimodal observations, including camera images, LiDAR point clouds, and GPS coordinates.|\n",
    "2512.04821": "|2025-12-04|LatentFM: A Latent Flow Matching Approach for Generative Medical Image Segmentation|Huynh Trinh Ngoc\u7b49|[2512.04821](http://arxiv.org/pdf/2512.04821)|\u65e0|\u25c6 Generative models have achieved remarkable progress with the emergence of flow matching (FM).\n\u25c6 It has demonstrated strong generative capabilities and attracted significant attention as a simulation-free flow-based framework capable of learning exact data densities.\n\u25c6 Motivated by these advances, we propose LatentFM, a flow-based model operating in the latent space for medical image segmentation.|\n",
    "2512.04804": "|2025-12-04|Unveiling gravitational waves from core-collapse supernovae with MUSE|Alessandro Veutro\u7b49|[2512.04804](http://arxiv.org/pdf/2512.04804)|\u65e0|\u25c6 The core collapse of a massive star at the end of its life can give rise to one of the most powerful phenomena in the Universe.\n\u25c6 Because of violent mass motions that take place during the explosion, core-collapse supernovae have been considered a potential source of detectable gravitational waveforms for decades.\n\u25c6 However, their intrinsic stochasticity makes ineffective the use of modelled techniques such as matched filtering, forcing us to develop model independent technique to unveil their nature.|\n",
    "2512.04677": "|2025-12-04|Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length|Yubo Huang\u7b49|[2512.04677](http://arxiv.org/pdf/2512.04677)|\u65e0|\u25c6 Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis.\n\u25c6 We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model.\n\u25c6 Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming.|\n",
    "2512.04662": "|2025-12-04|Spectral micro-CT for quantitative analysis of calcification in fibrocartilage|Vittoria Mazzini\u7b49|[2512.04662](http://arxiv.org/pdf/2512.04662)|\u65e0|\u25c6 This work introduces a quantitative method for assessing calcification in fibrocartilage using spectral micro-computed tomography ($\u03bc$CT).\n\u25c6 Tissue samples of hip acetabular labrum from patients with osteoarthritis and femoroacetabular impingement were imaged with a laboratory-based spectral $\u03bc$CT system equipped with a small-pixel photon-counting detector.\n\u25c6 The detector operated with two energy thresholds, allowing the simultaneous acquisition of two CT datasets at different X-ray energies.|\n",
    "2512.07712": "|2025-12-16|UnCageNet: Tracking and Pose Estimation of Caged Animal|Sayak Dutta\u7b49|[2512.07712](http://arxiv.org/pdf/2512.07712)|\u65e0|\u25c6 Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions.\n\u25c6 We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames.\n\u25c6 Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods.|\n",
    "2512.10379": "|2025-12-11|Self-Supervised Contrastive Embedding Adaptation for Endoscopic Image Matching|Alberto Rota\u7b49|[2512.10379](http://arxiv.org/pdf/2512.10379)|\u65e0|\u25c6 Accurate spatial understanding is essential for image-guided surgery, augmented reality integration and context awareness.\n\u25c6 In minimally invasive procedures, where visual input is the sole intraoperative modality, establishing precise pixel-level correspondences between endoscopic frames is critical for 3D reconstruction, camera tracking, and scene interpretation.\n\u25c6 However, the surgical domain presents distinct challenges: weak perspective cues, non-Lambertian tissue reflections, and complex, deformable anatomy degrade the performance of conventional computer vision techniques.|\n",
    "2512.10284": "|2025-12-14|MotionEdit: Benchmarking and Learning Motion-Centric Image Editing|Yixin Wan\u7b49|[2512.10284](http://arxiv.org/pdf/2512.10284)|\u65e0|\u25c6 We introduce MotionEdit, a novel dataset for motion-centric image editing-the task of modifying subject actions and interactions while preserving identity, structure, and physical plausibility.\n\u25c6 Unlike existing image editing datasets that focus on static appearance changes or contain only sparse, low-quality motion edits, MotionEdit provides high-fidelity image pairs depicting realistic motion transformations extracted and verified from continuous videos.\n\u25c6 This new task is not only scientifically challenging but also practically significant, powering downstream applications such as frame-controlled video synthesis and animation.|\n",
    "2512.13677": "|2025-12-15|JoVA: Unified Multimodal Learning for Joint Video-Audio Generation|Xiaohu Huang\u7b49|[2512.13677](http://arxiv.org/pdf/2512.13677)|\u65e0|\u25c6 In this paper, we present JoVA, a unified framework for joint video-audio generation.\n\u25c6 Despite recent encouraging advances, existing methods face two critical limitations.\n\u25c6 First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements.|\n",
    "2512.15542": "|2026-01-11|BLANKET: Anonymizing Faces in Infant Video Recordings|Ditmar Hadera\u7b49|[2512.15542](http://arxiv.org/pdf/2512.15542)|\u65e0|\u25c6 Ensuring the ethical use of video data involving human subjects, particularly infants, requires robust anonymization methods.\n\u25c6 We propose BLANKET (Baby-face Landmark-preserving ANonymization with Keypoint dEtection consisTency), a novel approach designed to anonymize infant faces in video recordings while preserving essential facial attributes.\n\u25c6 Our method comprises two stages.|\n",
    "2512.15508": "|2025-12-17|Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting|Arthur Moreau\u7b49|[2512.15508](http://arxiv.org/pdf/2512.15508)|\u65e0|\u25c6 Feed-forward 3D Gaussian Splatting (3DGS) models enable real-time scene generation but are hindered by suboptimal pixel-aligned primitive placement, which relies on a dense, rigid grid and limits both quality and efficiency.\n\u25c6 We introduce a new feed-forward architecture that detects 3D Gaussian primitives at a sub-pixel level, replacing the pixel grid with an adaptive, \"Off The Grid\" distribution.\n\u25c6 Inspired by keypoint detection, our multi-resolution decoder learns to distribute primitives across image patches.|\n",
    "2512.15949": "|2025-12-17|The Perceptual Observatory Characterizing Robustness and Grounding in MLLMs|Tejas Anvekar\u7b49|[2512.15949](http://arxiv.org/pdf/2512.15949)|\u65e0|\u25c6 Recent advances in multimodal large language models (MLLMs) have yielded increasingly powerful models, yet their perceptual capacities remain poorly characterized.\n\u25c6 In practice, most model families scale language component while reusing nearly identical vision encoders (e.g., Qwen2.5-VL 3B/7B/72B), which raises pivotal concerns about whether progress reflects genuine visual grounding or reliance on internet-scale textual world knowledge.\n\u25c6 Existing evaluation methods emphasize end-task accuracy, overlooking robustness, attribution fidelity, and reasoning under controlled perturbations.|\n",
    "2512.18451": "|2025-12-20|Analog Quantum Image Representation with Qubit-Frugal Encoding|Vikrant Sharma\u7b49|[2512.18451](http://arxiv.org/pdf/2512.18451)|\u65e0|\u25c6 In this work, we introduce a fundamentally new paradigm for quantum image representation tailored for neutral-atom quantum devices.\n\u25c6 The proposed method constructs a qubit-efficient image representation by first applying a cartographic generalization algorithm to a classical edge-extracted input image, yielding a highly optimized sparse-dot based geometric description.\n\u25c6 While ensuring the structural integrity of the image, this sparse representation is then embedded into the atomic configuration of Aquila (QuEra Computing Inc.), modeled through the Bloqade simulation software stack.|\n",
    "2512.21194": "|2025-12-24|VisRes Bench: On Evaluating the Visual Reasoning Capabilities of VLMs|Brigitta Malagurski T\u00f6rtei\u7b49|[2512.21194](http://arxiv.org/pdf/2512.21194)|\u65e0|\u25c6 Vision-Language Models (VLMs) have achieved remarkable progress across tasks such as visual question answering and image captioning.\n\u25c6 Yet, the extent to which these models perform visual reasoning as opposed to relying on linguistic priors remains unclear.\n\u25c6 To address this, we introduce VisRes Bench, a benchmark designed to study visual reasoning in naturalistic settings without contextual language supervision.|\n",
    "2512.24834": "|2025-12-31|GenZ: Foundational models as latent variable generators within traditional statistical models|Marko Jojic\u7b49|[2512.24834](http://arxiv.org/pdf/2512.24834)|\u65e0|\u25c6 We present GenZ, a hybrid model that bridges foundational models and statistical modeling through interpretable semantic features.\n\u25c6 While large language models possess broad domain knowledge, they often fail to capture dataset-specific patterns critical for prediction tasks.\n\u25c6 Our approach addresses this by discovering semantic feature descriptions through an iterative process that contrasts groups of items identified via statistical modeling errors, rather than relying solely on the foundational model's domain understanding.|\n",
    "2512.24687": "|2025-12-31|Quantum Visual Word Sense Disambiguation: Unraveling Ambiguities Through Quantum Inference Model|Wenbo Qiao\u7b49|[2512.24687](http://arxiv.org/pdf/2512.24687)|\u65e0|\u25c6 Visual word sense disambiguation focuses on polysemous words, where candidate images can be easily confused.\n\u25c6 Traditional methods use classical probability to calculate the likelihood of an image matching each gloss of the target word, summing these to form a posterior probability.\n\u25c6 However, due to the challenge of semantic uncertainty, glosses from different sources inevitably carry semantic biases, which can lead to biased disambiguation results.|\n",
    "2601.01869": "|2026-01-05|Exact Clique Number Manipulation via Edge Interdiction|Yi Zhou\u7b49|[2601.01869](http://arxiv.org/pdf/2601.01869)|\u65e0|\u25c6 The Edge Interdiction Clique Problem (EICP) aims to remove at most $k$ edges from a graph so as to minimize the size of the largest clique in the remaining graph.\n\u25c6 This problem captures a fundamental question in graph manipulation: which edges are structurally critical for preserving large cliques?\n\u25c6 Such a problem is also motivated by practical applications including protein function maintenance and image matching.|\n",
    "2601.00991": "|2026-01-02|UnrealPose: Leveraging Game Engine Kinematics for Large-Scale Synthetic Human Pose Data|Joshua Kawaguchi\u7b49|[2601.00991](http://arxiv.org/pdf/2601.00991)|\u65e0|\u25c6 Diverse, accurately labeled 3D human pose data is expensive and studio-bound, while in-the-wild datasets lack known ground truth.\n\u25c6 We introduce UnrealPose-Gen, an Unreal Engine 5 pipeline built on Movie Render Queue for high-quality offline rendering.\n\u25c6 Our generated frames include: (i) 3D joints in world and camera coordinates, (ii) 2D projections and COCO-style keypoints with occlusion and joint-visibility flags, (iii) person bounding boxes, and (iv) camera intrinsics and extrinsics.|\n",
    "2601.05695": "|2026-01-09|Stationaere Kurven auf endlichdimensionalen Mannigfaltigkeiten|Tobias Starke|[2601.05695](http://arxiv.org/pdf/2601.05695)|\u65e0|\u25c6 In this work we discuss the notion of stationary curves of the length functional, the so-called (weak) geodesics, on a Riemannian manifold.\n\u25c6 The motivation behind this work is to give a detailed description of many key concepts from differential geometry that one needs in order to understand the important notion of a (weak) geodesic.\n\u25c6 For this, we mainly focus on finite-dimensional smooth manifolds, so that we can develop an intuitive and geometric understanding of the concepts that we want to discuss.|\n",
    "2601.08798": "|2026-01-13|Near-perfect photo-ID of the Hula painted frog with zero-shot deep local-feature matching|Maayan Yesharim\u7b49|[2601.08798](http://arxiv.org/pdf/2601.08798)|\u65e0|\u25c6 Accurate individual identification is essential for monitoring rare amphibians, yet invasive marking is often unsuitable for critically endangered species.\n\u25c6 We evaluate state-of-the-art computer-vision methods for photographic re-identification of the Hula painted frog (Latonia nigriventer) using 1,233 ventral images from 191 individuals collected during 2013-2020 capture-recapture surveys.\n\u25c6 We compare deep local-feature matching in a zero-shot setting with deep global-feature embedding models.|\n",
    "2601.08182": "|2026-01-13|Second-order Gaussian directional derivative representations for image high-resolution corner detection|Dongbo Xie\u7b49|[2601.08182](http://arxiv.org/pdf/2601.08182)|\u65e0|\u25c6 Corner detection is widely used in various computer vision tasks, such as image matching and 3D reconstruction.\n\u25c6 Our research indicates that there are theoretical flaws in Zhang et al.'s use of a simple corner model to obtain a series of corner characteristics, as the grayscale information of two adjacent corners can affect each other.\n\u25c6 In order to address the above issues, a second-order Gaussian directional derivative (SOGDD) filter is used in this work to smooth two typical high-resolution angle models (i.e.|\n",
    "2601.09230": "|2026-01-14|CLIDD: Cross-Layer Independent Deformable Description for Efficient and Discriminative Local Feature Representation|Haodi Yao\u7b49|[2601.09230](http://arxiv.org/pdf/2601.09230)|\u65e0|\u25c6 Robust local feature representations are essential for spatial intelligence tasks such as robot navigation and augmented reality.\n\u25c6 Establishing reliable correspondences requires descriptors that provide both high discriminative power and computational efficiency.\n\u25c6 To address this, we introduce Cross-Layer Independent Deformable Description (CLIDD), a method that achieves superior distinctiveness by sampling directly from independent feature hierarchies.|\n",
    "2601.13126": "|2026-01-19|A Streamlined Attention-Based Network for Descriptor Extraction|Mattia D'Urso\u7b49|[2601.13126](http://arxiv.org/pdf/2601.13126)|\u65e0|\u25c6 We introduce SANDesc, a Streamlined Attention-Based Network for Descriptor extraction that aims to improve on existing architectures for keypoint description.\n\u25c6 Our descriptor network learns to compute descriptors that improve matching without modifying the underlying keypoint detector.\n\u25c6 We employ a revised U-Net-like architecture enhanced with Convolutional Block Attention Modules and residual paths, enabling effective local representation while maintaining computational efficiency.|\n",
    "2601.12530": "|2026-01-18|XRefine: Attention-Guided Keypoint Match Refinement|Jan Fabian Schmid\u7b49|[2601.12530](http://arxiv.org/pdf/2601.12530)|\u65e0|\u25c6 Sparse keypoint matching is crucial for 3D vision tasks, yet current keypoint detectors often produce spatially inaccurate matches.\n\u25c6 Existing refinement methods mitigate this issue through alignment of matched keypoint locations, but they are typically detector-specific, requiring retraining for each keypoint detector.\n\u25c6 We introduce XRefine, a novel, detector-agnostic approach for sub-pixel keypoint refinement that operates solely on image patches centered at matched keypoints.|\n",
    "2601.11930": "|2026-01-17|SupScene: Learning Overlap-Aware Global Descriptor for Unconstrained SfM|Xulei Shi\u7b49|[2601.11930](http://arxiv.org/pdf/2601.11930)|\u65e0|\u25c6 Image retrieval is a critical step for alleviating the quadratic complexity of image matching in unconstrained Structure-from-Motion (SfM).\n\u25c6 However, in this context, image retrieval typically focuses more on the image pairs of geometric matchability than on those of semantic similarity, a nuance that most existing deep learning-based methods guided by batched binaries (overlapping vs.\n\u25c6 non-overlapping pairs) fail to capture.|\n",
    "2601.15212": "|2026-01-21|ZENITH: Automated Gradient Norm Informed Stochastic Optimization|Dhrubo Saha|[2601.15212](http://arxiv.org/pdf/2601.15212)|\u65e0|\u25c6 Training deep computer vision models requires manual oversight or hyperparameter tuning of the learning rate (LR) schedule.\n\u25c6 While existing adaptive optimizers schedule the LR automatically, they suffer from computational and memory overhead, incompatibility with regularization, and suboptimal LR choices.\n\u25c6 In this work, we introduce the ZENITH (Zero-overhead Evolution using Norm-Informed Training History) optimizer, which adapts the LR using the temporal evolution of the gradient norm.|\n",
    "2601.16348": "|2026-01-22|Coarse-to-Fine Non-rigid Multi-modal Image Registration for Historical Panel Paintings based on Crack Structures|Aline Sindel\u7b49|[2601.16348](http://arxiv.org/pdf/2601.16348)|\u65e0|\u25c6 Art technological investigations of historical panel paintings rely on acquiring multi-modal image data, including visual light photography, infrared reflectography, ultraviolet fluorescence photography, x-radiography, and macro photography.\n\u25c6 For a comprehensive analysis, the multi-modal images require pixel-wise alignment, which is still often performed manually.\n\u25c6 Multi-modal image registration can reduce this laborious manual work, is substantially faster, and enables higher precision.|\n"
  },
  "3DGS": {
    "2512.01296": "|2025-12-01|EGG-Fusion: Efficient 3D Reconstruction with Geometry-aware Gaussian Surfel on the Fly|Xiaokun Pan\u7b49|[2512.01296](http://arxiv.org/pdf/2512.01296)|\u65e0|\u25c6 Real-time 3D reconstruction is a fundamental task in computer graphics.\n\u25c6 Recently, differentiable-rendering-based SLAM system has demonstrated significant potential, enabling photorealistic scene rendering through learnable scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS).\n\u25c6 Current differentiable rendering methods face dual challenges in real-time computation and sensor noise sensitivity, leading to degraded geometric fidelity in scene reconstruction and limited practicality.|\n",
    "2512.02932": "|2025-12-02|EGGS: Exchangeable 2D/3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis|Yancheng Zhang\u7b49|[2512.02932](http://arxiv.org/pdf/2512.02932)|\u65e0|\u25c6 Novel view synthesis (NVS) is crucial in computer vision and graphics, with wide applications in AR, VR, and autonomous driving.\n\u25c6 While 3D Gaussian Splatting (3DGS) enables real-time rendering with high appearance fidelity, it suffers from multi-view inconsistencies, limiting geometric accuracy.\n\u25c6 In contrast, 2D Gaussian Splatting (2DGS) enforces multi-view consistency but compromises texture details.|\n",
    "2512.02664": "|2025-12-02|PolarGuide-GSDR: 3D Gaussian Splatting Driven by Polarization Priors and Deferred Reflection for Real-World Reflective Scenes|Derui Shan\u7b49|[2512.02664](http://arxiv.org/pdf/2512.02664)|\u65e0|\u25c6 Polarization-aware Neural Radiance Fields (NeRF) enable novel view synthesis of specular-reflection scenes but face challenges in slow training, inefficient rendering, and strong dependencies on material/viewpoint assumptions.\n\u25c6 However, 3D Gaussian Splatting (3DGS) enables real-time rendering yet struggles with accurate reflection reconstruction from reflection-geometry entanglement, adding a deferred reflection module introduces environment map dependence.\n\u25c6 We address these limitations by proposing PolarGuide-GSDR, a polarization-forward-guided paradigm establishing a bidirectional coupling mechanism between polarization and 3DGS: first 3DGS's geometric priors are leveraged to resolve polarization ambiguity, and then the refined polarization information cues are used to guide 3DGS's normal and spherical harmonic representation.|\n",
    "2512.02293": "|2025-12-02|VIGS-SLAM: Visual Inertial Gaussian Splatting SLAM|Zihan Zhu\u7b49|[2512.02293](http://arxiv.org/pdf/2512.02293)|\u65e0|\u25c6 We present VIGS-SLAM, a visual-inertial 3D Gaussian Splatting SLAM system that achieves robust real-time tracking and high-fidelity reconstruction.\n\u25c6 Although recent 3DGS-based SLAM methods achieve dense and photorealistic mapping, their purely visual design degrades under motion blur, low texture, and exposure variations.\n\u25c6 Our method tightly couples visual and inertial cues within a unified optimization framework, jointly refining camera poses, depths, and IMU states.|\n",
    "2512.02172": "|2025-12-01|SplatSuRe: Selective Super-Resolution for Multi-view Consistent 3D Gaussian Splatting|Pranav Asthana\u7b49|[2512.02172](http://arxiv.org/pdf/2512.02172)|\u65e0|\u25c6 3D Gaussian Splatting (3DGS) enables high-quality novel view synthesis, motivating interest in generating higher-resolution renders than those available during training.\n\u25c6 A natural strategy is to apply super-resolution (SR) to low-resolution (LR) input views, but independently enhancing each image introduces multi-view inconsistencies, leading to blurry renders.\n\u25c6 Prior methods attempt to mitigate these inconsistencies through learned neural components, temporally consistent video priors, or joint optimization on LR and SR views, but all uniformly apply SR across every image.|\n",
    "2512.03621": "|2025-12-03|ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation|Yaokun Li\u7b49|[2512.03621](http://arxiv.org/pdf/2512.03621)|\u65e0|\u25c6 We propose ReCamDriving, a purely vision-based, camera-controlled novel-trajectory video generation framework.\n\u25c6 While repair-based methods fail to restore complex artifacts and LiDAR-based approaches rely on sparse and incomplete cues, ReCamDriving leverages dense and scene-complete 3DGS renderings for explicit geometric guidance, achieving precise camera-controllable generation.\n\u25c6 To mitigate overfitting to restoration behaviors when conditioned on 3DGS renderings, ReCamDriving adopts a two-stage training paradigm: the first stage uses camera poses for coarse control, while the second stage incorporates 3DGS renderings for fine-grained viewpoint and geometric guidance.|\n",
    "2512.03422": "|2025-12-03|What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models|Tianchen Deng\u7b49|[2512.03422](http://arxiv.org/pdf/2512.03422)|\u65e0|\u25c6 In this paper, we provide a comprehensive overview of existing scene representation methods for robotics, covering traditional representations such as point clouds, voxels, signed distance functions (SDF), and scene graphs, as well as more recent neural representations like Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and the emerging Foundation Models.\n\u25c6 While current SLAM and localization systems predominantly rely on sparse representations like point clouds and voxels, dense scene representations are expected to play a critical role in downstream tasks such as navigation and obstacle avoidance.\n\u25c6 Moreover, neural representations such as NeRF, 3DGS, and foundation models are well-suited for integrating high-level semantic features and language-based priors, enabling more comprehensive 3D scene understanding and embodied intelligence.|\n",
    "2512.03210": "|2025-12-02|Flux4D: Flow-based Unsupervised 4D Reconstruction|Jingkang Wang\u7b49|[2512.03210](http://arxiv.org/pdf/2512.03210)|\u65e0|\u25c6 Reconstructing large-scale dynamic scenes from visual observations is a fundamental challenge in computer vision, with critical implications for robotics and autonomous systems.\n\u25c6 While recent differentiable rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved impressive photorealistic reconstruction, they suffer from scalability limitations and require annotations to decouple actor motion.\n\u25c6 Existing self-supervised methods attempt to eliminate explicit annotations by leveraging motion cues and geometric priors, yet they remain constrained by per-scene optimization and sensitivity to hyperparameter tuning.|\n",
    "2512.04815": "|2025-12-04|RobustSplat++: Decoupling Densification, Dynamics, and Illumination for In-the-Wild 3DGS|Chuanyu Fu\u7b49|[2512.04815](http://arxiv.org/pdf/2512.04815)|\u65e0|\u25c6 3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling.\n\u25c6 However, existing methods struggle with accurately modeling in-the-wild scenes affected by transient objects and illuminations, leading to artifacts in the rendered images.\n\u25c6 We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances and illumination variations.|\n",
    "2512.04542": "|2025-12-04|Gaussian Entropy Fields: Driving Adaptive Sparsity in 3D Gaussian Optimization|Hong Kuang\u7b49|[2512.04542](http://arxiv.org/pdf/2512.04542)|\u65e0|\u25c6 3D Gaussian Splatting (3DGS) has emerged as a leading technique for novel view synthesis, demonstrating exceptional rendering efficiency.\n\u25c6 \\replaced[]{Well-reconstructed surfaces can be characterized by low configurational entropy, where dominant primitives clearly define surface geometry while redundant components are suppressed.}{The key insight is that well-reconstructed surfaces naturally exhibit low configurational entropy, where dominant primitives clearly define surface geometry while suppressing redundant components.} Three complementary technical contributions are introduced: (1) entropy-driven surface modeling via entropy minimization for low configurational entropy in primitive distributions; (2) adaptive spatial regularization using the Surface Neighborhood Redundancy Index (SNRI) and image entropy-guided weighting; (3) multi-scale geometric preservation through competitive cross-scale entropy alignment.\n\u25c6 Extensive experiments demonstrate that GEF achieves competitive geometric precision on DTU and T\\&T benchmarks, while delivering superior rendering quality compared to existing methods on Mip-NeRF 360.|\n",
    "2512.07527": "|2025-12-09|From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images|Fei Yu\u7b49|[2512.07527](http://arxiv.org/pdf/2512.07527)|\u65e0|\u25c6 City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax.\n\u25c6 This requires inferring nearly $90^\\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail.\n\u25c6 To address this problem, we propose two design choices tailored for city structures and satellite inputs.|\n",
    "2512.07247": "|2025-12-08|AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing|Ziming Hong\u7b49|[2512.07247](http://arxiv.org/pdf/2512.07247)|\u65e0|\u25c6 Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation.\n\u25c6 However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering.\n\u25c6 Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability.|\n",
    "2512.07230": "|2025-12-08|STRinGS: Selective Text Refinement in Gaussian Splatting|Abhinav Raundhal\u7b49|[2512.07230](http://arxiv.org/pdf/2512.07230)|\u65e0|\u25c6 Text as signs, labels, or instructions is a critical element of real-world scenes as they can convey important contextual information.\n\u25c6 3D representations such as 3D Gaussian Splatting (3DGS) struggle to preserve fine-grained text details, while achieving high visual fidelity.\n\u25c6 Small errors in textual element reconstruction can lead to significant semantic loss.|\n",
    "2512.07197": "|2025-12-08|SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting|Seokhyun Youn\u7b49|[2512.07197](http://arxiv.org/pdf/2512.07197)|\u65e0|\u25c6 3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis.\n\u25c6 However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians.\n\u25c6 These challenges become even more severe in 4D dynamic scenes.|\n",
    "2512.07107": "|2025-12-09|COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision|Jaeyoon Lee\u7b49|[2512.07107](http://arxiv.org/pdf/2512.07107)|\u65e0|\u25c6 We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and a Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting.\n\u25c6 While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition.\n\u25c6 To address these limitations, COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space.|\n",
    "2512.07052": "|2025-12-07|RAVE: Rate-Adaptive Visual Encoding for 3D Gaussian Splatting|Hoang-Nhat Tran\u7b49|[2512.07052](http://arxiv.org/pdf/2512.07052)|\u65e0|\u25c6 Recent advances in neural scene representations have transformed immersive multimedia, with 3D Gaussian Splatting (3DGS) enabling real-time photorealistic rendering.\n\u25c6 Despite its efficiency, 3DGS suffers from large memory requirements and costly training procedures, motivating efforts toward compression.\n\u25c6 Existing approaches, however, operate at fixed rates, limiting adaptability to varying bandwidth and device constraints.|\n",
    "2512.06774": "|2025-12-07|RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting|Longjie Zhao\u7b49|[2512.06774](http://arxiv.org/pdf/2512.06774)|\u65e0|\u25c6 3D Gaussian Splatting (3DGS) has enabled the creation of digital assets and downstream applications, underscoring the need for robust copyright protection via digital watermarking.\n\u25c6 However, existing 3DGS watermarking methods remain highly vulnerable to diffusion-based editing, which can easily erase embedded provenance.\n\u25c6 This challenge highlights the urgent need for 3DGS watermarking techniques that are intrinsically resilient to diffusion-based editing.|\n",
    "2512.06438": "|2025-12-06|AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars|Ramazan Fazylov\u7b49|[2512.06438](http://arxiv.org/pdf/2512.06438)|\u65e0|\u25c6 The generation of high-fidelity, animatable 3D human avatars remains a core challenge in computer graphics and vision, with applications in VR, telepresence, and entertainment.\n\u25c6 Existing approaches based on implicit representations like NeRFs suffer from slow rendering and dynamic inconsistencies, while 3D Gaussian Splatting (3DGS) methods are typically limited to static head generation, lacking dynamic control.\n\u25c6 We bridge this gap by introducing AGORA, a novel framework that extends 3DGS within a generative adversarial network to produce animatable avatars.|\n",
    "2512.05446": "|2025-12-05|TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression|Cheng-Yuan Ho\u7b49|[2512.05446](http://arxiv.org/pdf/2512.05446)|\u65e0|\u25c6 Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension to dynamic scenes, commonly referred to as 4DGS or dynamic 3DGS, has attracted increasing attention.\n\u25c6 However, designing more compact and efficient deformation schemes together with rate-distortion-optimized compression strategies for dynamic 3DGS representations remains an underexplored area.\n\u25c6 Prior methods either rely on space-time 4DGS with overspecified, short-lived Gaussian primitives or on canonical 3DGS with deformation that lacks explicit temporal control.|\n",
    "2512.08625": "|2025-12-09|OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics|Jisang Yoo\u7b49|[2512.08625](http://arxiv.org/pdf/2512.08625)|\u65e0|\u25c6 Simultaneous Localization and Mapping (SLAM) is a foundational component in robotics, AR/VR, and autonomous systems.\n\u25c6 With the rising focus on spatial AI in recent years, combining SLAM with semantic understanding has become increasingly important for enabling intelligent perception and interaction.\n\u25c6 Recent efforts have explored this integration, but they often rely on depth sensors or closed-set semantic models, limiting their scalability and adaptability in open-world environments.|\n",
    "2512.08498": "|2025-12-09|On-the-fly Large-scale 3D Reconstruction from Multi-Camera Rigs|Yijia Guo\u7b49|[2512.08498](http://arxiv.org/pdf/2512.08498)|\u65e0|\u25c6 Recent advances in 3D Gaussian Splatting (3DGS) have enabled efficient free-viewpoint rendering and photorealistic scene reconstruction.\n\u25c6 While on-the-fly extensions of 3DGS have shown promise for real-time reconstruction from monocular RGB streams, they often fail to achieve complete 3D coverage due to the limited field of view (FOV).\n\u25c6 Employing a multi-camera rig fundamentally addresses this limitation.|\n",
    "2512.08478": "|2025-12-09|Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform|Yuning Gong\u7b49|[2512.08478](http://arxiv.org/pdf/2512.08478)|\u65e0|\u25c6 Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models.\n\u25c6 However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models.\n\u25c6 In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering.|\n",
    "2512.08271": "|2025-12-09|Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation|Srijan Dokania\u7b49|[2512.08271](http://arxiv.org/pdf/2512.08271)|\u65e0|\u25c6 We introduce Zero-Splat TeleAssist, a zero-shot sensor-fusion pipeline that transforms commodity CCTV streams into a shared, 6-DoF world model for multilateral teleoperation.|\n",
    "2512.09923": "|2025-12-10|Splatent: Splatting Diffusion Latents for Novel View Synthesis|Or Hirschorn\u7b49|[2512.09923](http://arxiv.org/pdf/2512.09923)|\u65e0|\u25c6 Radiance field representations have recently been explored in the latent space of VAEs that are commonly used by diffusion models.\n\u25c6 This direction offers efficient rendering and seamless integration with diffusion-based pipelines.\n\u25c6 However, these methods face a fundamental limitation: The VAE latent space lacks multi-view consistency, leading to blurred textures and missing details during 3D reconstruction.|\n",
    "2512.09903": "|2025-12-10|YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos|Ryan Meegan\u7b49|[2512.09903](http://arxiv.org/pdf/2512.09903)|\u65e0|\u25c6 Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning.\n\u25c6 However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive.\n\u25c6 We address the problem of visual navigation when exploration videos of a large environment are available.|\n",
    "2512.09335": "|2025-12-11|Relightable and Dynamic Gaussian Avatar Reconstruction from Monocular Video|Seonghwa Choi\u7b49|[2512.09335](http://arxiv.org/pdf/2512.09335)|\u65e0|\u25c6 Modeling relightable and animatable human avatars from monocular video is a long-standing and challenging task.\n\u25c6 Recently, Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) methods have been employed to reconstruct the avatars.\n\u25c6 However, they often produce unsatisfactory photo-realistic results because of insufficient geometrical details related to body motion, such as clothing wrinkles.|\n",
    "2512.09270": "|2025-12-10|MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification|Sangwoon Kwak\u7b49|[2512.09270](http://arxiv.org/pdf/2512.09270)|\u65e0|\u25c6 Recent advances in 4D Gaussian Splatting (4DGS) have extended the high-speed rendering capability of 3D Gaussian Splatting (3DGS) into the temporal domain, enabling real-time rendering of dynamic scenes.\n\u25c6 However, one of the major remaining challenges lies in modeling long-range motion-contained dynamic videos, where a naive extension of existing methods leads to severe memory explosion, temporal flickering, and failure to handle appearing or disappearing occlusions over time.\n\u25c6 To address these challenges, we propose a novel 4DGS framework characterized by an Anchor Relay-based Bidirectional Blending (ARBB) mechanism, named MoRel, which enables temporally consistent and memory-efficient modeling of long-range dynamic scenes.|\n",
    "2512.10369": "|2025-12-11|Breaking the Vicious Cycle: Coherent 3D Gaussian Splatting from Sparse and Motion-Blurred Views|Zhankuo Xu\u7b49|[2512.10369](http://arxiv.org/pdf/2512.10369)|\u65e0|\u25c6 3D Gaussian Splatting (3DGS) has emerged as a state-of-the-art method for novel view synthesis.\n\u25c6 However, its performance heavily relies on dense, high-quality input imagery, an assumption that is often violated in real-world applications, where data is typically sparse and motion-blurred.\n\u25c6 These two issues create a vicious cycle: sparse views ignore the multi-view constraints necessary to resolve motion blur, while motion blur erases high-frequency details crucial for aligning the limited views.|\n",
    "2512.11800": "|2025-12-12|Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance|Jan U. M\u00fcller\u7b49|[2512.11800](http://arxiv.org/pdf/2512.11800)|\u65e0|\u25c6 The recent success of 3D Gaussian Splatting (3DGS) has reshaped novel view synthesis by enabling fast optimization and real-time rendering of high-quality radiance fields.\n\u25c6 However, it relies on simplified, order-dependent alpha blending and coarse approximations of the density integral within the rasterizer, thereby limiting its ability to render complex, overlapping semi-transparent objects.\n\u25c6 In this paper, we extend rasterization-based rendering of 3D Gaussian representations with a novel method for high-fidelity transmittance computation, entirely avoiding the need for ray tracing or per-pixel sample sorting.|\n",
    "2512.13411": "|2025-12-15|Computer vision training dataset generation for robotic environments using Gaussian splatting|Patryk Ni\u017ceniec\u7b49|[2512.13411](http://arxiv.org/pdf/2512.13411)|\u65e0|\u25c6 This paper introduces a novel pipeline for generating large-scale, highly realistic, and automatically labeled datasets for computer vision tasks in robotic environments.\n\u25c6 Our approach addresses the critical challenges of the domain gap between synthetic and real-world imagery and the time-consuming bottleneck of manual annotation.\n\u25c6 We leverage 3D Gaussian Splatting (3DGS) to create photorealistic representations of the operational environment and objects.|\n",
    "2512.14352": "|2025-12-16|HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis|Kaizhe Zhang\u7b49|[2512.14352](http://arxiv.org/pdf/2512.14352)|\u65e0|\u25c6 Dynamic novel view synthesis (NVS) is essential for creating immersive experiences.\n\u25c6 Existing approaches have advanced dynamic NVS by introducing 3D Gaussian Splatting (3DGS) with implicit deformation fields or indiscriminately assigned time-varying parameters, surpassing NeRF-based methods.\n\u25c6 However, due to excessive model complexity and parameter redundancy, they incur large model sizes and slow rendering speeds, making them inefficient for real-time applications, particularly on resource-constrained devices.|\n",
    "2512.14087": "|2025-12-16|GaussianPlant: Structure-aligned Gaussian Splatting for 3D Reconstruction of Plants|Yang Yang\u7b49|[2512.14087](http://arxiv.org/pdf/2512.14087)|\u65e0|\u25c6 We present a method for jointly recovering the appearance and internal structure of botanical plants from multi-view images based on 3D Gaussian Splatting (3DGS).\n\u25c6 While 3DGS exhibits robust reconstruction of scene appearance for novel-view synthesis, it lacks structural representations underlying those appearances (e.g., branching patterns of plants), which limits its applicability to tasks such as plant phenotyping.\n\u25c6 To achieve both high-fidelity appearance and structural reconstruction, we introduce GaussianPlant, a hierarchical 3DGS representation, which disentangles structure and appearance.|\n",
    "2512.15508": "|2025-12-17|Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting|Arthur Moreau\u7b49|[2512.15508](http://arxiv.org/pdf/2512.15508)|\u65e0|\u25c6 Feed-forward 3D Gaussian Splatting (3DGS) models enable real-time scene generation but are hindered by suboptimal pixel-aligned primitive placement, which relies on a dense, rigid grid and limits both quality and efficiency.\n\u25c6 We introduce a new feed-forward architecture that detects 3D Gaussian primitives at a sub-pixel level, replacing the pixel grid with an adaptive, \"Off The Grid\" distribution.\n\u25c6 Inspired by keypoint detection, our multi-resolution decoder learns to distribute primitives across image patches.|\n",
    "2512.15048": "|2025-12-17|MVGSR: Multi-View Consistent 3D Gaussian Super-Resolution via Epipolar Guidance|Kaizhe Zhang\u7b49|[2512.15048](http://arxiv.org/pdf/2512.15048)|\u65e0|\u25c6 Scenes reconstructed by 3D Gaussian Splatting (3DGS) trained on low-resolution (LR) images are unsuitable for high-resolution (HR) rendering.\n\u25c6 Consequently, a 3DGS super-resolution (SR) method is needed to bridge LR inputs and HR rendering.\n\u25c6 Early 3DGS SR methods rely on single-image SR networks, which lack cross-view consistency and fail to fuse complementary information across views.|\n",
    "2512.16706": "|2025-12-18|SDFoam: Signed-Distance Foam for explicit surface reconstruction|Antonella Rech\u7b49|[2512.16706](http://arxiv.org/pdf/2512.16706)|\u65e0|\u25c6 Neural radiance fields (NeRF) have driven impressive progress in view synthesis by using ray-traced volumetric rendering.\n\u25c6 Splatting-based methods such as 3D Gaussian Splatting (3DGS) provide faster rendering by rasterizing 3D primitives.\n\u25c6 RadiantFoam (RF) brought ray tracing back, achieving throughput comparable to Gaussian Splatting by organizing radiance with an explicit Voronoi Diagram (VD).|\n",
    "2512.17817": "|2025-12-22|Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding|Yue Li\u7b49|[2512.17817](http://arxiv.org/pdf/2512.17817)|\u65e0|\u25c6 While 3DGS has emerged as a high-fidelity scene representation, encoding rich, general-purpose features directly from its primitives remains under-explored.\n\u25c6 We address this gap by introducing Chorus, a multi-teacher pretraining framework that learns a holistic feed-forward 3D Gaussian Splatting (3DGS) scene encoder by distilling complementary signals from 2D foundation models.\n\u25c6 Chorus employs a shared 3D encoder and teacher-specific projectors to learn from language-aligned, generalist, and object-aware teachers, encouraging a shared embedding space that captures signals from high-level semantics to fine-grained structure.|\n",
    "2512.17349": "|2025-12-19|Flying in Clutter on Monocular RGB by Learning in 3D Radiance Fields with Domain Adaptation|Xijie Huang\u7b49|[2512.17349](http://arxiv.org/pdf/2512.17349)|\u65e0|\u25c6 Modern autonomous navigation systems predominantly rely on lidar and depth cameras.\n\u25c6 However, a fundamental question remains: Can flying robots navigate in clutter using solely monocular RGB images?\n\u25c6 Given the prohibitive costs of real-world data collection, learning policies in simulation offers a promising path.|\n",
    "2512.17796": "|2025-12-18|Animate Any Character in Any World|Yitong Wang\u7b49|[2512.17796](http://arxiv.org/pdf/2512.17796)|\u65e0|\u25c6 Recent advances in world models have greatly enhanced interactive environment simulation.\n\u25c6 Existing methods mainly fall into two categories: (1) static world generation models, which construct 3D environments without active agents, and (2) controllable-entity models, which allow a single entity to perform limited actions in an otherwise uncontrollable environment.\n\u25c6 In this work, we introduce AniX, leveraging the realism and structural grounding of static world generation while extending controllable-entity models to support user-specified characters capable of performing open-ended actions.|\n",
    "2512.19678": "|2025-12-22|WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion|Hanyang Kong\u7b49|[2512.19678](http://arxiv.org/pdf/2512.19678)|\u65e0|\u25c6 Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space.\n\u25c6 This disconnect causes current methods to struggle with occluded areas and complex camera trajectories.\n\u25c6 To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner.|\n",
    "2512.19390": "|2025-12-22|TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation|Hongwei Fan\u7b49|[2512.19390](http://arxiv.org/pdf/2512.19390)|\u65e0|\u25c6 The robotics field is evolving towards data-driven, end-to-end learning, inspired by multimodal large models.\n\u25c6 However, reliance on expensive real-world data limits progress.\n\u25c6 Simulators offer cost-effective alternatives, but the gap between simulation and reality challenges effective policy transfer.|\n",
    "2512.18692": "|2025-12-21|EcoSplat: Efficiency-controllable Feed-forward 3D Gaussian Splatting from Multi-view Images|Jongmin Park\u7b49|[2512.18692](http://arxiv.org/pdf/2512.18692)|\u65e0|\u25c6 Feed-forward 3D Gaussian Splatting (3DGS) enables efficient one-pass scene reconstruction, providing 3D representations for novel view synthesis without per-scene optimization.\n\u25c6 However, existing methods typically predict pixel-aligned primitives per-view, producing an excessive number of primitives in dense-view settings and offering no explicit control over the number of predicted Gaussians.\n\u25c6 To address this, we propose EcoSplat, the first efficiency-controllable feed-forward 3DGS framework that adaptively predicts the 3D representation for any given target primitive count at inference time.|\n",
    "2512.18640": "|2025-12-21|Geometric-Photometric Event-based 3D Gaussian Ray Tracing|Kai Kohyama\u7b49|[2512.18640](http://arxiv.org/pdf/2512.18640)|\u65e0|\u25c6 Event cameras offer a high temporal resolution over traditional frame-based cameras, which makes them suitable for motion and structure estimation.\n\u25c6 However, it has been unclear how event-based 3D Gaussian Splatting (3DGS) approaches could leverage fine-grained temporal information of sparse events.\n\u25c6 This work proposes a framework to address the trade-off between accuracy and temporal resolution in event-based 3DGS.|\n",
    "2512.20495": "|2025-12-23|Nebula: Enable City-Scale 3D Gaussian Splatting in Virtual Reality via Collaborative Rendering and Accelerated Stereo Rasterization|He Zhu\u7b49|[2512.20495](http://arxiv.org/pdf/2512.20495)|\u65e0|\u25c6 3D Gaussian splatting (3DGS) has drawn significant attention in the architectural community recently.\n\u25c6 However, current architectural designs often overlook the 3DGS scalability, making them fragile for extremely large-scale 3DGS.\n\u25c6 Meanwhile, the VR bandwidth requirement makes it impossible to deliver high-fidelity and smooth VR content from the cloud.|\n",
    "2512.20148": "|2025-12-23|Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)|Robert van de Ven\u7b49|[2512.20148](http://arxiv.org/pdf/2512.20148)|\u65e0|\u25c6 Automating tasks in orchards is challenging because of the large amount of variation in the environment and occlusions.\n\u25c6 One of the challenges is apple pose estimation, where key points, such as the calyx, are often occluded.\n\u25c6 Recently developed pose estimation methods no longer rely on these key points, but still require them for annotations, making annotating challenging and time-consuming.|\n",
    "2512.22830": "|2025-12-28|3D Scene Change Modeling With Consistent Multi-View Aggregation|Zirui Zhou\u7b49|[2512.22830](http://arxiv.org/pdf/2512.22830)|\u65e0|\u25c6 Change detection plays a vital role in scene monitoring, exploration, and continual reconstruction.\n\u25c6 Existing 3D change detection methods often exhibit spatial inconsistency in the detected changes and fail to explicitly separate pre- and post-change states.\n\u25c6 To address these limitations, we propose SCaR-3D, a novel 3D scene change detection framework that identifies object-level changes from a dense-view pre-change image sequence and sparse-view post-change images.|\n",
    "2512.24986": "|2025-12-31|PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes|Luca Collorone\u7b49|[2512.24986](http://arxiv.org/pdf/2512.24986)|\u65e0|\u25c6 Realistic visual simulations are omnipresent, yet their creation requires computing time, rendering, and expert animation knowledge.\n\u25c6 Open-vocabulary visual effects generation from text inputs emerges as a promising solution that can unlock immense creative potential.\n\u25c6 However, current pipelines lack both physical realism and effective language interfaces, requiring slow offline optimization.|\n",
    "2512.24763": "|2025-12-31|UniC-Lift: Unified 3D Instance Segmentation via Contrastive Learning|Ankit Dhiman\u7b49|[2512.24763](http://arxiv.org/pdf/2512.24763)|\u65e0|\u25c6 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF) have advanced novel-view synthesis.\n\u25c6 Recent methods extend multi-view 2D segmentation to 3D, enabling instance/semantic segmentation for better scene understanding.\n\u25c6 A key challenge is the inconsistency of 2D instance labels across views, leading to poor 3D predictions.|\n",
    "2512.24742": "|2025-12-31|Splatwizard: A Benchmark Toolkit for 3D Gaussian Splatting Compression|Xiang Liu\u7b49|[2512.24742](http://arxiv.org/pdf/2512.24742)|\u65e0|\u25c6 The recent advent of 3D Gaussian Splatting (3DGS) has marked a significant breakthrough in real-time novel view synthesis.\n\u25c6 However, the rapid proliferation of 3DGS-based algorithms has created a pressing need for standardized and comprehensive evaluation tools, especially for compression task.\n\u25c6 Existing benchmarks often lack the specific metrics necessary to holistically assess the unique characteristics of different methods, such as rendering speed, rate distortion trade-offs memory efficiency, and geometric accuracy.|\n",
    "2512.23998": "|2025-12-30|Improved 3D Gaussian Splatting of Unknown Spacecraft Structure Using Space Environment Illumination Knowledge|Tae Ha Park\u7b49|[2512.23998](http://arxiv.org/pdf/2512.23998)|\u65e0|\u25c6 This work presents a novel pipeline to recover the 3D structure of an unknown target spacecraft from a sequence of images captured during Rendezvous and Proximity Operations (RPO) in space.\n\u25c6 The target's geometry and appearance are represented as a 3D Gaussian Splatting (3DGS) model.\n\u25c6 However, learning 3DGS requires static scenes, an assumption in contrast to dynamic lighting conditions encountered in spaceborne imagery.|\n",
    "2601.02339": "|2026-01-05|Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling with Anisotropic Local Encoding|Jingming He\u7b49|[2601.02339](http://arxiv.org/pdf/2601.02339)|\u65e0|\u25c6 Recent works propose extending 3DGS with semantic feature vectors for simultaneous semantic segmentation and image rendering.\n\u25c6 However, these methods often treat the semantic and rendering branches separately, relying solely on 2D supervision while ignoring the 3D Gaussian geometry.\n\u25c6 Moreover, current adaptive strategies adapt the Gaussian set depending solely on rendering gradients, which can be insufficient in subtle or textureless regions.|\n",
    "2601.02102": "|2026-01-05|360-GeoGS: Geometrically Consistent Feed-Forward 3D Gaussian Splatting Reconstruction for 360 Images|Jiaqi Yao\u7b49|[2601.02102](http://arxiv.org/pdf/2601.02102)|\u65e0|\u25c6 3D scene reconstruction is fundamental for spatial intelligence applications such as AR, robotics, and digital twins.\n\u25c6 Traditional multi-view stereo struggles with sparse viewpoints or low-texture regions, while neural rendering approaches, though capable of producing high-quality results, require per-scene optimization and lack real-time efficiency.\n\u25c6 Explicit 3D Gaussian Splatting (3DGS) enables efficient rendering, but most feed-forward variants focus on visual quality rather than geometric consistency, limiting accurate surface reconstruction and overall reliability in spatial perception tasks.|\n",
    "2601.01660": "|2026-01-04|Animated 3DGS Avatars in Diverse Scenes with Consistent Lighting and Shadows|Aymen Mir\u7b49|[2601.01660](http://arxiv.org/pdf/2601.01660)|\u65e0|\u25c6 We present a method for consistent lighting and shadows when animated 3D Gaussian Splatting (3DGS) avatars interact with 3DGS scenes or with dynamic objects inserted into otherwise static scenes.\n\u25c6 Our key contribution is Deep Gaussian Shadow Maps (DGSM), a modern analogue of the classical shadow mapping algorithm tailored to the volumetric 3DGS representation.\n\u25c6 Building on the classic deep shadow mapping idea, we show that 3DGS admits closed form light accumulation along light rays, enabling volumetric shadow computation without meshing.|\n",
    "2601.00939": "|2026-01-04|ShadowGS: Shadow-Aware 3D Gaussian Splatting for Satellite Imagery|Feng Luo\u7b49|[2601.00939](http://arxiv.org/pdf/2601.00939)|\u65e0|\u25c6 3D Gaussian Splatting (3DGS) has emerged as a novel paradigm for 3D reconstruction from satellite imagery.\n\u25c6 However, in multi-temporal satellite images, prevalent shadows exhibit significant inconsistencies due to varying illumination conditions.\n\u25c6 To address this, we propose ShadowGS, a novel framework based on 3DGS.|\n",
    "2601.01386": "|2026-01-04|ParkGaussian: Surround-view 3D Gaussian Splatting for Autonomous Parking|Xiaobao Wei\u7b49|[2601.01386](http://arxiv.org/pdf/2601.01386)|\u65e0|\u25c6 Parking is a critical task for autonomous driving systems (ADS), with unique challenges in crowded parking slots and GPS-denied environments.\n\u25c6 However, existing works focus on 2D parking slot perception, mapping, and localization, 3D reconstruction remains underexplored, which is crucial for capturing complex spatial geometry in parking scenarios.\n\u25c6 Naively improving the visual quality of reconstructed parking scenes does not directly benefit autonomous parking, as the key entry point for parking is the slots perception module.|\n",
    "2601.00913": "|2026-01-01|Clean-GS: Semantic Mask-Guided Pruning for 3D Gaussian Splatting|Subhankar Mishra|[2601.00913](http://arxiv.org/pdf/2601.00913)|\u65e0|\u25c6 3D Gaussian Splatting produces high-quality scene reconstructions but generates hundreds of thousands of spurious Gaussians (floaters) scattered throughout the environment.\n\u25c6 These artifacts obscure objects of interest and inflate model sizes, hindering deployment in bandwidth-constrained applications.\n\u25c6 We present Clean-GS, a method for removing background clutter and floaters from 3DGS reconstructions using sparse semantic masks.|\n",
    "2601.03200": "|2026-01-06|A High-Fidelity Digital Twin for Robotic Manipulation Based on 3D Gaussian Splatting|Ziyang Sun\u7b49|[2601.03200](http://arxiv.org/pdf/2601.03200)|\u65e0|\u25c6 Developing high-fidelity, interactive digital twins is crucial for enabling closed-loop motion planning and reliable real-world robot execution, which are essential to advancing sim-to-real transfer.\n\u25c6 However, existing approaches often suffer from slow reconstruction, limited visual fidelity, and difficulties in converting photorealistic models into planning-ready collision geometry.\n\u25c6 We present a practical framework that constructs high-quality digital twins within minutes from sparse RGB inputs.|\n",
    "2601.03357": "|2026-01-06|RelightAnyone: A Generalized Relightable 3D Gaussian Head Model|Yingyan Xu\u7b49|[2601.03357](http://arxiv.org/pdf/2601.03357)|\u65e0|\u25c6 3D Gaussian Splatting (3DGS) has become a standard approach to reconstruct and render photorealistic 3D head avatars.\n\u25c6 A major challenge is to relight the avatars to match any scene illumination.\n\u25c6 For high quality relighting, existing methods require subjects to be captured under complex time-multiplexed illumination, such as one-light-at-a-time (OLAT).|\n",
    "2601.03319": "|2026-01-06|CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature|Eldad Matmon\u7b49|[2601.03319](http://arxiv.org/pdf/2601.03319)|\u65e0|\u25c6 A photorealistic and controllable 3D caricaturization framework for faces is introduced.\n\u25c6 We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders.\n\u25c6 To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars.|\n",
    "2601.04754": "|2026-01-08|ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting|Yen-Jen Chiou\u7b49|[2601.04754](http://arxiv.org/pdf/2601.04754)|\u65e0|\u25c6 We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS).\n\u25c6 The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning.\n\u25c6 Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering.|\n",
    "2601.05738": "|2026-01-09|FeatureSLAM: Feature-enriched 3D gaussian splatting SLAM in real time|Christopher Thirgood\u7b49|[2601.05738](http://arxiv.org/pdf/2601.05738)|\u65e0|\u25c6 We present a real-time tracking SLAM system that unifies efficient camera tracking with photorealistic feature-enriched mapping using 3D Gaussian Splatting (3DGS).\n\u25c6 Our main contribution is integrating dense feature rasterization into the novel-view synthesis, aligned with a visual foundation model.\n\u25c6 This yields strong semantics, going beyond basic RGB-D input, aiding both tracking and mapping accuracy.|\n",
    "2601.05394": "|2026-01-08|Sketch&Patch++: Efficient Structure-Aware 3D Gaussian Representation|Yuang Shi\u7b49|[2601.05394](http://arxiv.org/pdf/2601.05394)|\u65e0|\u25c6 We observe that Gaussians exhibit distinct roles and characteristics analogous to traditional artistic techniques -- like how artists first sketch outlines before filling in broader areas with color, some Gaussians capture high-frequency features such as edges and contours, while others represent broader, smoother regions analogous to brush strokes that add volume and depth.\n\u25c6 Based on this observation, we propose a hybrid representation that categorizes Gaussians into (i) Sketch Gaussians, which represent high-frequency, boundary-defining features, and (ii) Patch Gaussians, which cover low-frequency, smooth regions.\n\u25c6 This semantic separation naturally enables layered progressive streaming, where the compact Sketch Gaussians establish the structural skeleton before Patch Gaussians incrementally refine volumetric detail.|\n",
    "2601.07540": "|2026-01-13|ViewMorpher3D: A 3D-aware Diffusion Framework for Multi-Camera Novel View Synthesis in Autonomous Driving|Farhad G. Zanjani\u7b49|[2601.07540](http://arxiv.org/pdf/2601.07540)|\u65e0|\u25c6 Autonomous driving systems rely heavily on multi-view images to ensure accurate perception and robust decision-making.\n\u25c6 To effectively develop and evaluate perception stacks and planning algorithms, realistic closed-loop simulators are indispensable.\n\u25c6 While 3D reconstruction techniques such as Gaussian Splatting offer promising avenues for simulator construction, the rendered novel views often exhibit artifacts, particularly in extrapolated perspectives or when available observations are sparse.|\n",
    "2601.07518": "|2026-01-12|Mon3tr: Monocular 3D Telepresence with Pre-built Gaussian Avatars as Amortization|Fangyu Lin\u7b49|[2601.07518](http://arxiv.org/pdf/2601.07518)|\u65e0|\u25c6 Immersive telepresence aims to transform human interaction in AR/VR applications by enabling lifelike full-body holographic representations for enhanced remote collaboration.\n\u25c6 However, existing systems rely on hardware-intensive multi-camera setups and demand high bandwidth for volumetric streaming, limiting their real-time performance on mobile devices.\n\u25c6 To overcome these challenges, we propose Mon3tr, a novel Monocular 3D telepresence framework that integrates 3D Gaussian splatting (3DGS) based parametric human modeling into telepresence for the first time.|\n",
    "2601.07484": "|2026-01-12|R3-RECON: Radiance-Field-Free Active Reconstruction via Renderability|Xiaofeng Jin\u7b49|[2601.07484](http://arxiv.org/pdf/2601.07484)|\u65e0|\u25c6 In active reconstruction, an embodied agent must decide where to look next to efficiently acquire views that support high-quality novel-view rendering.\n\u25c6 Recent work on active view planning for neural rendering largely derives next-best-view (NBV) criteria by backpropagating through radiance fields or estimating information entropy over 3D Gaussian primitives.\n\u25c6 While effective, these strategies tightly couple view selection to heavy, representation-specific mechanisms and fail to account for the computational and resource constraints required for lightweight online deployment.|\n",
    "2601.06831": "|2026-01-11|SARA: Scene-Aware Reconstruction Accelerator|Jee Won Lee\u7b49|[2601.06831](http://arxiv.org/pdf/2601.06831)|\u65e0|\u25c6 We present SARA (Scene-Aware Reconstruction Accelerator), a geometry-driven pair selection module for Structure-from-Motion (SfM).\n\u25c6 Unlike conventional pipelines that select pairs based on visual similarity alone, SARA introduces geometry-first pair selection by scoring reconstruction informativeness - the product of overlap and parallax - before expensive matching.\n\u25c6 A lightweight pre-matching stage uses mutual nearest neighbors and RANSAC to estimate these cues, then constructs an Information-Weighted Spanning Tree (IWST) augmented with targeted edges for loop closure, long-baseline anchors, and weak-view reinforcement.|\n",
    "2601.06479": "|2026-01-10|SRFlow: A Dataset and Regularization Model for High-Resolution Facial Optical Flow via Splatting Rasterization|JiaLin Zhang\u7b49|[2601.06479](http://arxiv.org/pdf/2601.06479)|\u65e0|\u25c6 Facial optical flow supports a wide range of tasks in facial motion analysis.\n\u25c6 However, the lack of high-resolution facial optical flow datasets has hindered progress in this area.\n\u25c6 In this paper, we introduce Splatting Rasterization Flow (SRFlow), a high-resolution facial optical flow dataset, and Splatting Rasterization Guided FlowNet (SRFlowNet), a facial optical flow model with tailored regularization losses.|\n",
    "2601.06285": "|2026-01-09|NAS-GS: Noise-Aware Sonar Gaussian Splatting|Shida Xu\u7b49|[2601.06285](http://arxiv.org/pdf/2601.06285)|\u65e0|\u25c6 Underwater sonar imaging plays a crucial role in various applications, including autonomous navigation in murky water, marine archaeology, and environmental monitoring.\n\u25c6 However, the unique characteristics of sonar images, such as complex noise patterns and the lack of elevation information, pose significant challenges for 3D reconstruction and novel view synthesis.\n\u25c6 In this paper, we present NAS-GS, a novel Noise-Aware Sonar Gaussian Splatting framework specifically designed to address these challenges.|\n",
    "2601.05853": "|2026-01-09|LayerGS: Decomposition and Inpainting of Layered 3D Human Avatars via 2D Gaussian Splatting|Yinghan Xu\u7b49|[2601.05853](http://arxiv.org/pdf/2601.05853)|\u65e0|\u25c6 We propose a novel framework for decomposing arbitrarily posed humans into animatable multi-layered 3D human avatars, separating the body and garments.\n\u25c6 Conventional single-layer reconstruction methods lock clothing to one identity, while prior multi-layer approaches struggle with occluded regions.\n\u25c6 We overcome both limitations by encoding each layer as a set of 2D Gaussians for accurate geometry and photorealistic rendering, and inpainting hidden regions with a pretrained 2D diffusion model via score-distillation sampling (SDS).|\n",
    "2601.05584": "|2026-01-09|GS-DMSR: Dynamic Sensitive Multi-scale Manifold Enhancement for Accelerated High-Quality 3D Gaussian Splatting|Nengbo Lu\u7b49|[2601.05584](http://arxiv.org/pdf/2601.05584)|\u65e0|\u25c6 In the field of 3D dynamic scene reconstruction, how to balance model convergence rate and rendering quality has long been a critical challenge that urgently needs to be addressed, particularly in high-precision modeling of scenes with complex dynamic motions.\n\u25c6 To tackle this issue, this study proposes the GS-DMSR method.\n\u25c6 By quantitatively analyzing the dynamic evolution process of Gaussian attributes, this mechanism achieves adaptive gradient focusing, enabling it to dynamically identify significant differences in the motion states of Gaussian models.|\n",
    "2601.05511": "|2026-01-09|GaussianSwap: Animatable Video Face Swapping with 3D Gaussian Splatting|Xuan Cheng\u7b49|[2601.05511](http://arxiv.org/pdf/2601.05511)|\u65e0|\u25c6 We introduce GaussianSwap, a novel video face swapping framework that constructs a 3D Gaussian Splatting based face avatar from a target video while transferring identity from a source image to the avatar.\n\u25c6 Conventional video swapping frameworks are limited to generating facial representations in pixel-based formats.\n\u25c6 The resulting swapped faces exist merely as a set of unstructured pixels without any capacity for animation or interactive manipulation.|\n",
    "2601.07963": "|2026-01-12|3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing|Jiahua Dong\u7b49|[2601.07963](http://arxiv.org/pdf/2601.07963)|\u65e0|\u25c6 The transformative potential of 3D content creation has been progressively unlocked through advancements in generative models.\n\u25c6 Recently, intuitive drag editing with geometric changes has attracted significant attention in 2D editing yet remains challenging for 3D scenes.\n\u25c6 In this paper, we introduce 3DGS-Drag -- a point-based 3D editing framework that provides efficient, intuitive drag manipulation of real 3D scenes.|\n",
    "2601.09417": "|2026-01-14|Variable Basis Mapping for Real-Time Volumetric Visualization|Qibiao Li\u7b49|[2601.09417](http://arxiv.org/pdf/2601.09417)|\u65e0|\u25c6 Real-time visualization of large-scale volumetric data remains challenging, as direct volume rendering and voxel-based methods suffer from prohibitively high computational cost.\n\u25c6 We propose Variable Basis Mapping (VBM), a framework that transforms volumetric fields into 3D Gaussian Splatting (3DGS) representations through wavelet-domain analysis.\n\u25c6 First, we precompute a compact Wavelet-to-Gaussian Transition Bank that provides optimal Gaussian surrogates for canonical wavelet atoms across multiple scales.|\n",
    "2601.09291": "|2026-01-14|TIDI-GS: Floater Suppression in 3D Gaussian Splatting for Enhanced Indoor Scene Fidelity|Sooyeun Yang\u7b49|[2601.09291](http://arxiv.org/pdf/2601.09291)|\u65e0|\u25c6 3D Gaussian Splatting (3DGS) is a technique to create high-quality, real-time 3D scenes from images.\n\u25c6 This method often produces visual artifacts known as floaters--nearly transparent, disconnected elements that drift in space away from the actual surface.\n\u25c6 This geometric inaccuracy undermines the reliability of these models for practical applications, which is critical.|\n",
    "2601.09265": "|2026-01-14|GaussianFluent: Gaussian Simulation for Dynamic Scenes with Mixed Materials|Bei Huang\u7b49|[2601.09265](http://arxiv.org/pdf/2601.09265)|\u65e0|\u25c6 3D Gaussian Splatting (3DGS) has emerged as a prominent 3D representation for high-fidelity and real-time rendering.\n\u25c6 Prior work has coupled physics simulation with Gaussians, but predominantly targets soft, deformable materials, leaving brittle fracture largely unresolved.\n\u25c6 This stems from two key obstacles: the lack of volumetric interiors with coherent textures in GS representation, and the absence of fracture-aware simulation methods for Gaussians.|\n",
    "2601.09243": "|2026-01-14|A$^2$TG: Adaptive Anisotropic Textured Gaussians for Efficient 3D Scene Representation|Sheng-Chi Hsu\u7b49|[2601.09243](http://arxiv.org/pdf/2601.09243)|\u65e0|\u25c6 Gaussian Splatting has emerged as a powerful representation for high-quality, real-time 3D scene rendering.\n\u25c6 While recent works extend Gaussians with learnable textures to enrich visual appearance, existing approaches allocate a fixed square texture per primitive, leading to inefficient memory usage and limited adaptability to scene variability.\n\u25c6 In this paper, we introduce adaptive anisotropic textured Gaussians (A$^2$TG), a novel representation that generalizes textured Gaussians by equipping each primitive with an anisotropic texture.|\n",
    "2601.10606": "|2026-01-15|RSATalker: Realistic Socially-Aware Talking Head Generation for Multi-Turn Conversation|Peng Chen\u7b49|[2601.10606](http://arxiv.org/pdf/2601.10606)|\u65e0|\u25c6 Talking head generation is increasingly important in virtual reality (VR), especially for social scenarios involving multi-turn conversation.\n\u25c6 Existing approaches face notable limitations: mesh-based 3D methods can model dual-person dialogue but lack realistic textures, while large-model-based 2D methods produce natural appearances but incur prohibitive computational costs.\n\u25c6 Recently, 3D Gaussian Splatting (3DGS) based methods achieve efficient and realistic rendering but remain speaker-only and ignore social relationships.|\n",
    "2601.10075": "|2026-01-15|Thinking Like Van Gogh: Structure-Aware Style Transfer via Flow-Guided 3D Gaussian Splatting|Zhendong Wang\u7b49|[2601.10075](http://arxiv.org/pdf/2601.10075)|\u65e0|\u25c6 In 1888, Vincent van Gogh wrote, \"I am seeking exaggeration in the essential.\" This principle, amplifying structural form while suppressing photographic detail, lies at the core of Post-Impressionist art.\n\u25c6 However, most existing 3D style transfer methods invert this philosophy, treating geometry as a rigid substrate for surface-level texture projection.\n\u25c6 To authentically reproduce Post-Impressionist stylization, geometric abstraction must be embraced as the primary vehicle of expression.|\n",
    "2601.14208": "|2026-01-20|Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian Splatting|Nitin Kulkarni\u7b49|[2601.14208](http://arxiv.org/pdf/2601.14208)|\u65e0|\u25c6 Inspecting the undercarriage of used vehicles is a labor-intensive task that requires inspectors to crouch or crawl underneath each vehicle to thoroughly examine it.\n\u25c6 Additionally, online buyers rarely see undercarriage photos.\n\u25c6 We present an end-to-end pipeline that utilizes a three-camera rig to capture videos of the undercarriage as the vehicle drives over it, and produces an interactive 3D model of the undercarriage.|\n",
    "2601.14161": "|2026-01-20|One-Shot Refiner: Boosting Feed-forward Novel View Synthesis via One-Step Diffusion|Yitong Dong\u7b49|[2601.14161](http://arxiv.org/pdf/2601.14161)|\u65e0|\u25c6 We present a novel framework for high-fidelity novel view synthesis (NVS) from sparse images, addressing key limitations in recent feed-forward 3D Gaussian Splatting (3DGS) methods built on Vision Transformer (ViT) backbones.\n\u25c6 While ViT-based pipelines offer strong geometric priors, they are often constrained by low-resolution inputs due to computational costs.\n\u25c6 Moreover, existing generative enhancement methods tend to be 3D-agnostic, resulting in inconsistent structures across views, especially in unseen regions.|\n",
    "2601.13706": "|2026-01-20|ParkingTwin: Training-Free Streaming 3D Reconstruction for Parking-Lot Digital Twins|Xinhao Liu\u7b49|[2601.13706](http://arxiv.org/pdf/2601.13706)|\u65e0|\u25c6 High-fidelity parking-lot digital twins provide essential priors for path planning, collision checking, and perception validation in Automated Valet Parking (AVP).\n\u25c6 Yet robot-oriented reconstruction faces a trilemma: sparse forward-facing views cause weak parallax and ill-posed geometry; dynamic occlusions and extreme lighting hinder stable texture fusion; and neural rendering typically needs expensive offline optimization, violating edge-side streaming constraints.\n\u25c6 We propose ParkingTwin, a training-free, lightweight system for online streaming 3D reconstruction.|\n",
    "2601.13132": "|2026-01-19|GaussExplorer: 3D Gaussian Splatting for Embodied Exploration and Reasoning|Kim Yu-Ji\u7b49|[2601.13132](http://arxiv.org/pdf/2601.13132)|\u65e0|\u25c6 We present GaussExplorer, a framework for embodied exploration and reasoning built on 3D Gaussian Splatting (3DGS).\n\u25c6 While prior approaches to language-embedded 3DGS have made meaningful progress in aligning simple text queries with Gaussian embeddings, they are generally optimized for relatively simple queries and struggle to interpret more complex, compositional language queries.\n\u25c6 Alternative studies based on object-centric RGB-D structured memories provide spatial grounding but are constrained by pre-fixed viewpoints.|\n",
    "2601.12823": "|2026-01-19|TreeDGS: Aerial Gaussian Splatting for Distant DBH Measurement|Belal Shaheen\u7b49|[2601.12823](http://arxiv.org/pdf/2601.12823)|\u65e0|\u25c6 Aerial remote sensing enables efficient large-area surveying, but accurate direct object-level measurement remains difficult in complex natural scenes.\n\u25c6 Recent advancements in 3D vision, particularly learned radiance-field representations such as NeRF and 3D Gaussian Splatting, have begun to raise the ceiling on reconstruction fidelity and densifiable geometry from posed imagery.\n\u25c6 Nevertheless, direct aerial measurement of important natural attributes such as tree diameter at breast height (DBH) remains challenging.|\n",
    "2601.12814": "|2026-01-19|CSGaussian: Progressive Rate-Distortion Compression and Segmentation for 3D Gaussian Splatting|Yu-Jen Tseng\u7b49|[2601.12814](http://arxiv.org/pdf/2601.12814)|\u65e0|\u25c6 We present the first unified framework for rate-distortion-optimized compression and segmentation of 3D Gaussian Splatting (3DGS).\n\u25c6 While 3DGS has proven effective for both real-time rendering and semantic scene understanding, prior works have largely treated these tasks independently, leaving their joint consideration unexplored.\n\u25c6 Inspired by recent advances in rate-distortion-optimized 3DGS compression, this work integrates semantic learning into the compression pipeline to support decoder-side applications--such as scene editing and manipulation--that extend beyond traditional scene reconstruction and view synthesis.|\n",
    "2601.12736": "|2026-01-19|KaoLRM: Repurposing Pre-trained Large Reconstruction Models for Parametric 3D Face Reconstruction|Qingtian Zhu\u7b49|[2601.12736](http://arxiv.org/pdf/2601.12736)|\u65e0|\u25c6 We propose KaoLRM to re-target the learned prior of the Large Reconstruction Model (LRM) for parametric 3D face reconstruction from single-view images.\n\u25c6 Parametric 3D Morphable Models (3DMMs) have been widely used for facial reconstruction due to their compact and interpretable parameterization, yet existing 3DMM regressors often exhibit poor consistency across varying viewpoints.\n\u25c6 To address this, we harness the pre-trained 3D prior of LRM and incorporate FLAME-based 2D Gaussian Splatting into LRM's rendering pipeline.|\n",
    "2601.12683": "|2026-01-19|GaussianTrimmer: Online Trimming Boundaries for 3DGS Segmentation|Liwei Liao\u7b49|[2601.12683](http://arxiv.org/pdf/2601.12683)|\u65e0|\u25c6 With the widespread application of 3D Gaussians in 3D scene representation, 3D scene segmentation methods based on 3D Gaussians have also gradually emerged.\n\u25c6 However, existing 3D Gaussian segmentation methods basically segment on the basis of Gaussian primitives.\n\u25c6 Due to the large variation range of the scale of 3D Gaussians, large-sized Gaussians that often span the foreground and background lead to jagged boundaries of segmented objects.|\n",
    "2601.12122": "|2026-01-17|Active Semantic Mapping of Horticultural Environments Using Gaussian Splatting|Jose Cuaran\u7b49|[2601.12122](http://arxiv.org/pdf/2601.12122)|\u65e0|\u25c6 Semantic reconstruction of agricultural scenes plays a vital role in tasks such as phenotyping and yield estimation.\n\u25c6 However, traditional approaches that rely on manual scanning or fixed camera setups remain a major bottleneck in this process.\n\u25c6 In this work, we propose an active 3D reconstruction framework for horticultural environments using a mobile manipulator.|\n",
    "2601.12020": "|2026-01-17|DIAMOND-SSS: Diffusion-Augmented Multi-View Optimization for Data-efficient SubSurface Scattering|Guillermo Figueroa-Araneda\u7b49|[2601.12020](http://arxiv.org/pdf/2601.12020)|\u65e0|\u25c6 Subsurface scattering (SSS) gives translucent materials -- such as wax, jade, marble, and skin -- their characteristic soft shadows, color bleeding, and diffuse glow.\n\u25c6 Modeling these effects in neural rendering remains challenging due to complex light transport and the need for densely captured multi-view, multi-light datasets (often more than 100 views and 112 OLATs).\n\u25c6 We present DIAMOND-SSS, a data-efficient framework for high-fidelity translucent reconstruction from extremely sparse supervision -- even as few as ten images.|\n",
    "2601.15283": "|2026-01-21|LuxRemix: Lighting Decomposition and Remixing for Indoor Scenes|Ruofan Liang\u7b49|[2601.15283](http://arxiv.org/pdf/2601.15283)|\u65e0|\u25c6 We present a novel approach for interactive light editing in indoor scenes from a single multi-view scene capture.\n\u25c6 Our method leverages a generative image-based light decomposition model that factorizes complex indoor scene illumination into its constituent light sources.\n\u25c6 This factorization enables independent manipulation of individual light sources, specifically allowing control over their state (on/off), chromaticity, and intensity.|\n",
    "2601.15221": "|2026-01-21|ScenDi: 3D-to-2D Scene Diffusion Cascades for Urban Generation|Hanlei Guo\u7b49|[2601.15221](http://arxiv.org/pdf/2601.15221)|\u65e0|\u25c6 Recent advancements in 3D object generation using diffusion models have achieved remarkable success, but generating realistic 3D urban scenes remains challenging.\n\u25c6 Existing methods relying solely on 3D diffusion models tend to suffer a degradation in appearance details, while those utilizing only 2D diffusion models typically compromise camera controllability.\n\u25c6 To overcome this limitation, we propose ScenDi, a method for urban scene generation that integrates both 3D and 2D diffusion models.|\n",
    "2601.14821": "|2026-01-21|POTR: Post-Training 3DGS Compression|Bert Ramlot\u7b49|[2601.14821](http://arxiv.org/pdf/2601.14821)|\u65e0|\u25c6 3D Gaussian Splatting (3DGS) has recently emerged as a promising contender to Neural Radiance Fields (NeRF) in 3D scene reconstruction and real-time novel view synthesis.\n\u25c6 3DGS outperforms NeRF in training and inference speed but has substantially higher storage requirements.\n\u25c6 To remedy this downside, we propose POTR, a post-training 3DGS codec built on two novel techniques.|\n",
    "2601.14510": "|2026-01-22|Structured Image-based Coding for Efficient Gaussian Splatting Compression|Pedro Martin\u7b49|[2601.14510](http://arxiv.org/pdf/2601.14510)|\u65e0|\u25c6 Gaussian Splatting (GS) has recently emerged as a state-of-the-art representation for radiance fields, combining real-time rendering with high visual fidelity.\n\u25c6 However, GS models require storing millions of parameters, leading to large file sizes that impair their use in practical multimedia systems.\n\u25c6 To address this limitation, this paper introduces GS Image-based Compression (GSICO), a novel GS codec that efficiently compresses pre-trained GS models while preserving perceptual fidelity.|\n",
    "2601.15951": "|2026-01-22|EVolSplat4D: Efficient Volume-based Gaussian Splatting for 4D Urban Scene Synthesis|Sheng Miao\u7b49|[2601.15951](http://arxiv.org/pdf/2601.15951)|\u65e0|\u25c6 Novel view synthesis (NVS) of static and dynamic urban scenes is essential for autonomous driving simulation, yet existing methods often struggle to balance reconstruction time with quality.\n\u25c6 While state-of-the-art neural radiance fields and 3D Gaussian Splatting approaches achieve photorealism, they often rely on time-consuming per-scene optimization.\n\u25c6 Conversely, emerging feed-forward methods frequently adopt per-pixel Gaussian representations, which lead to 3D inconsistencies when aggregating multi-view predictions in complex, dynamic environments.|\n",
    "2601.15897": "|2026-01-22|ThermoSplat: Cross-Modal 3D Gaussian Splatting with Feature Modulation and Geometry Decoupling|Zhaoqi Su\u7b49|[2601.15897](http://arxiv.org/pdf/2601.15897)|\u65e0|\u25c6 Multi-modal scene reconstruction integrating RGB and thermal infrared data is essential for robust environmental perception across diverse lighting and weather conditions.\n\u25c6 However, extending 3D Gaussian Splatting (3DGS) to multi-spectral scenarios remains challenging.\n\u25c6 Current approaches often struggle to fully leverage the complementary information of multi-modal data, typically relying on mechanisms that either tend to neglect cross-modal correlations or leverage shared representations that fail to adaptively handle the complex structural correlations and physical discrepancies between spectrums.|\n",
    "2601.15772": "|2026-01-22|LL-GaussianImage: Efficient Image Representation for Zero-shot Low-Light Enhancement with 2D Gaussian Splatting|Yuhan Chen\u7b49|[2601.15772](http://arxiv.org/pdf/2601.15772)|\u65e0|\u25c6 2D Gaussian Splatting (2DGS) is an emerging explicit scene representation method with significant potential for image compression due to high fidelity and high compression ratios.\n\u25c6 However, existing low-light enhancement algorithms operate predominantly within the pixel domain.\n\u25c6 Processing 2DGS-compressed images necessitates a cumbersome decompression-enhancement-recompression pipeline, which compromises efficiency and introduces secondary degradation.|\n",
    "2601.15766": "|2026-01-22|LL-GaussianMap: Zero-shot Low-Light Image Enhancement via 2D Gaussian Splatting Guided Gain Maps|Yuhan Chen\u7b49|[2601.15766](http://arxiv.org/pdf/2601.15766)|\u65e0|\u25c6 Significant progress has been made in low-light image enhancement with respect to visual quality.\n\u25c6 However, most existing methods primarily operate in the pixel domain or rely on implicit feature representations.\n\u25c6 As a result, the intrinsic geometric structural priors of images are often neglected.|\n",
    "2601.15431": "|2026-01-21|SplatBus: A Gaussian Splatting Viewer Framework via GPU Interprocess Communication|Yinghan Xu\u7b49|[2601.15431](http://arxiv.org/pdf/2601.15431)|\u65e0|\u25c6 Radiance field-based rendering methods have attracted significant interest from the computer vision and computer graphics communities.\n\u25c6 They enable high-fidelity rendering with complex real-world lighting effects, but at the cost of high rendering time.\n\u25c6 3D Gaussian Splatting solves this issue with a rasterisation-based approach for real-time rendering, enabling applications such as autonomous driving, robotics, virtual reality, and extended reality.|\n",
    "2601.16736": "|2026-01-26|A Step to Decouple Optimization in 3DGS|Renjie Ding\u7b49|[2601.16736](http://arxiv.org/pdf/2601.16736)|\u65e0|\u25c6 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time novel view synthesis.\n\u25c6 As an explicit representation optimized through gradient propagation among primitives, optimization widely accepted in deep neural networks (DNNs) is actually adopted in 3DGS, such as synchronous weight updating and Adam with the adaptive gradient.\n\u25c6 However, considering the physical significance and specific design in 3DGS, there are two overlooked details in the optimization of 3DGS: (i) update step coupling, which induces optimizer state rescaling and costly attribute updates outside the viewpoints, and (ii) gradient coupling in the moment, which may lead to under- or over-effective regularization.|\n",
    "2601.16672": "|2026-01-23|ReWeaver: Towards Simulation-Ready and Topology-Accurate Garment Reconstruction|Ming Li\u7b49|[2601.16672](http://arxiv.org/pdf/2601.16672)|\u65e0|\u25c6 High-quality 3D garment reconstruction plays a crucial role in mitigating the sim-to-real gap in applications such as digital avatars, virtual try-on and robotic manipulation.\n\u25c6 However, existing garment reconstruction methods typically rely on unstructured representations, such as 3D Gaussian Splats, struggling to provide accurate reconstructions of garment topology and sewing structures.\n\u25c6 As a result, the reconstructed outputs are often unsuitable for high-fidelity physical simulation.|\n",
    "2601.18633": "|2026-01-26|Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting|Tong Shi\u7b49|[2601.18633](http://arxiv.org/pdf/2601.18633)|\u65e0|\u25c6 Talking Head Generation aims at synthesizing natural-looking talking videos from speech and a single portrait image.\n\u25c6 Previous 3D talking head generation methods have relied on domain-specific heuristics such as warping-based facial motion representation priors to animate talking motions, yet still produce inaccurate 3D avatar reconstructions, thus undermining the realism of generated animations.\n\u25c6 We introduce Splat-Portrait, a Gaussian-splatting-based method that addresses the challenges of 3D head reconstruction and lip motion synthesis.|\n",
    "2601.18629": "|2026-01-26|ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection|Yiming Wang\u7b49|[2601.18629](http://arxiv.org/pdf/2601.18629)|\u65e0|\u25c6 Real-to-Sim-to-Real technique is gaining increasing interest for robotic manipulation, as it can generate scalable data in simulation while having narrower sim-to-real gap.\n\u25c6 However, previous methods mainly focused on environment-level visual real-to-sim transfer, ignoring the transfer of interactions, which could be challenging and inefficient to obtain purely in simulation especially for contact-rich tasks.\n\u25c6 We propose ExoGS, a robot-free 4D Real-to-Sim-to-Real framework that captures both static environments and dynamic interactions in the real world and transfers them seamlessly to a simulated environment.|\n",
    "2601.18475": "|2026-01-26|LoD-Structured 3D Gaussian Splatting for Streaming Video Reconstruction|Xinhui Liu\u7b49|[2601.18475](http://arxiv.org/pdf/2601.18475)|\u65e0|\u25c6 Free-Viewpoint Video (FVV) reconstruction enables photorealistic and interactive 3D scene visualization; however, real-time streaming is often bottlenecked by sparse-view inputs, prohibitive training costs, and bandwidth constraints.\n\u25c6 While recent 3D Gaussian Splatting (3DGS) has advanced FVV due to its superior rendering speed, Streaming Free-Viewpoint Video (SFVV) introduces additional demands for rapid optimization, high-fidelity reconstruction under sparse constraints, and minimal storage footprints.\n\u25c6 To bridge this gap, we propose StreamLoD-GS, an LoD-based Gaussian Splatting framework designed specifically for SFVV.|\n",
    "2601.17835": "|2026-01-25|Geometry-Grounded Gaussian Splatting|Baowen Zhang\u7b49|[2601.17835](http://arxiv.org/pdf/2601.17835)|\u65e0|\u25c6 Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis.\n\u25c6 However, shape extraction from Gaussian primitives remains an open problem.\n\u25c6 Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters.|\n",
    "2601.17720": "|2026-01-25|Advancing Structured Priors for Sparse-Voxel Surface Reconstruction|Ting-Hsun Chi\u7b49|[2601.17720](http://arxiv.org/pdf/2601.17720)|\u65e0|\u25c6 Reconstructing accurate surfaces with radiance fields has progressed rapidly, yet two promising explicit representations, 3D Gaussian Splatting and sparse-voxel rasterization, exhibit complementary strengths and weaknesses.\n\u25c6 3D Gaussian Splatting converges quickly and carries useful geometric priors, but surface fidelity is limited by its point-like parameterization.\n\u25c6 Sparse-voxel rasterization provides continuous opacity fields and crisp geometry, but its typical uniform dense-grid initialization slows convergence and underutilizes scene structure.|\n",
    "2601.17354": "|2026-01-24|PocketGS: On-Device Training of 3D Gaussian Splatting for High Perceptual Modeling|Wenzhi Guo\u7b49|[2601.17354](http://arxiv.org/pdf/2601.17354)|\u65e0|\u25c6 Efficient and high-fidelity 3D scene modeling is a long-standing pursuit in computer graphics.\n\u25c6 While recent 3D Gaussian Splatting (3DGS) methods achieve impressive real-time modeling performance, they rely on resource-unconstrained training assumptions that fail on mobile devices, which are limited by minute-scale training budgets and hardware-available peak-memory.\n\u25c6 We present PocketGS, a mobile scene modeling paradigm that enables on-device 3DGS training under these tightly coupled constraints while preserving high perceptual fidelity.|\n",
    "2601.17185": "|2026-01-23|LGDWT-GS: Local and Global Discrete Wavelet-Regularized 3D Gaussian Splatting for Sparse-View Scene Reconstruction|Shima Salehi\u7b49|[2601.17185](http://arxiv.org/pdf/2601.17185)|\u65e0|\u25c6 We propose a new method for few-shot 3D reconstruction that integrates global and local frequency regularization to stabilize geometry and preserve fine details under sparse-view conditions, addressing a key limitation of existing 3D Gaussian Splatting (3DGS) models.\n\u25c6 We also introduce a new multispectral greenhouse dataset containing four spectral bands captured from diverse plant species under controlled conditions.\n\u25c6 Alongside the dataset, we release an open-source benchmarking package that defines standardized few-shot reconstruction protocols for evaluating 3DGS-based methods.|\n",
    "2601.19843": "|2026-01-27|Graphical X Splatting (GraphiXS): A Graphical Model for 4D Gaussian Splatting under Uncertainty|Doga Yilmaz\u7b49|[2601.19843](http://arxiv.org/pdf/2601.19843)|\u65e0|\u25c6 We propose a new framework to systematically incorporate data uncertainty in Gaussian Splatting.\n\u25c6 Being the new paradigm of neural rendering, Gaussian Splatting has been investigated in many applications, with the main effort in extending its representation, improving its optimization process, and accelerating its speed.\n\u25c6 However, one orthogonal, much needed, but under-explored area is data uncertainty.|\n",
    "2601.19753": "|2026-01-27|WaterClear-GS: Optical-Aware Gaussian Splatting for Underwater Reconstruction and Restoration|Xinrui Zhang\u7b49|[2601.19753](http://arxiv.org/pdf/2601.19753)|\u65e0|\u25c6 Underwater 3D reconstruction and appearance restoration are hindered by the complex optical properties of water, such as wavelength-dependent attenuation and scattering.\n\u25c6 Existing Neural Radiance Fields (NeRF)-based methods struggle with slow rendering speeds and suboptimal color restoration, while 3D Gaussian Splatting (3DGS) inherently lacks the capability to model complex volumetric scattering effects.\n\u25c6 To address these issues, we introduce WaterClear-GS, the first pure 3DGS-based framework that explicitly integrates underwater optical properties of local attenuation and scattering into Gaussian primitives, eliminating the need for an auxiliary medium network.|\n",
    "2601.19717": "|2026-01-27|DiffStyle3D: Consistent 3D Gaussian Stylization via Attention Optimization|Yitong Yang\u7b49|[2601.19717](http://arxiv.org/pdf/2601.19717)|\u65e0|\u25c6 3D style transfer enables the creation of visually expressive 3D content, enriching the visual appearance of 3D scenes and objects.\n\u25c6 However, existing VGG- and CLIP-based methods struggle to model multi-view consistency within the model itself, while diffusion-based approaches can capture such consistency but rely on denoising directions, leading to unstable training.\n\u25c6 To address these limitations, we propose DiffStyle3D, a novel diffusion-based paradigm for 3DGS style transfer that directly optimizes in the latent space.|\n",
    "2601.19489": "|2026-01-28|Fast Converging 3D Gaussian Splatting for 1-Minute Reconstruction|Ziyu Zhang\u7b49|[2601.19489](http://arxiv.org/pdf/2601.19489)|\u65e0|\u25c6 We present a fast 3DGS reconstruction pipeline designed to converge within one minute, developed for the SIGGRAPH Asia 3DGS Fast Reconstruction Challenge.\n\u25c6 The challenge consists of an initial round using SLAM-generated camera poses (with noisy trajectories) and a final round using COLMAP poses (highly accurate).\n\u25c6 To robustly handle these heterogeneous settings, we develop a two-stage solution.|\n",
    "2601.19310": "|2026-01-27|ClipGS-VR: Immersive and Interactive Cinematic Visualization of Volumetric Medical Data in Mobile Virtual Reality|Yuqi Tong\u7b49|[2601.19310](http://arxiv.org/pdf/2601.19310)|\u65e0|\u25c6 High-fidelity cinematic medical visualization on mobile virtual reality (VR) remains challenging.\n\u25c6 Although ClipGS enables cross-sectional exploration via 3D Gaussian Splatting, it lacks arbitrary-angle slicing on consumer-grade VR headsets.\n\u25c6 To achieve real-time interactive performance, we introduce ClipGS-VR and restructure ClipGS's neural inference into a consolidated dataset, integrating high-fidelity layers from multiple pre-computed slicing states into a unified rendering structure.|\n",
    "2601.19247": "|2026-01-27|TIGaussian: Disentangle Gaussians for Spatial-Awared Text-Image-3D Alignment|Jiarun Liu\u7b49|[2601.19247](http://arxiv.org/pdf/2601.19247)|\u65e0|\u25c6 While visual-language models have profoundly linked features between texts and images, the incorporation of 3D modality data, such as point clouds and 3D Gaussians, further enables pretraining for 3D-related tasks, e.g., cross-modal retrieval, zero-shot classification, and scene recognition.\n\u25c6 As challenges remain in extracting 3D modal features and bridging the gap between different modalities, we propose TIGaussian, a framework that harnesses 3D Gaussian Splatting (3DGS) characteristics to strengthen cross-modality alignment through multi-branch 3DGS tokenizer and modality-specific 3D feature alignment strategies.\n\u25c6 Specifically, our multi-branch 3DGS tokenizer decouples the intrinsic properties of 3DGS structures into compact latent representations, enabling more generalizable feature extraction.|\n",
    "2601.19233": "|2026-01-27|UniMGS: Unifying Mesh and 3D Gaussian Splatting with Single-Pass Rasterization and Proxy-Based Deformation|Zeyu Xiao\u7b49|[2601.19233](http://arxiv.org/pdf/2601.19233)|\u65e0|\u25c6 Joint rendering and deformation of mesh and 3D Gaussian Splatting (3DGS) have significant value as both representa tions offer complementary advantages for graphics applica tions.\n\u25c6 However, due to differences in representation and ren dering pipelines, existing studies render meshes and 3DGS separately, making it difficult to accurately handle occlusions and transparency.\n\u25c6 Moreover, the deformed 3DGS still suffers from visual artifacts due to the sensitivity to the topology quality of the proxy mesh.|\n",
    "2601.19216": "|2026-01-27|Bridging Visual and Wireless Sensing: A Unified Radiation Field for 3D Radio Map Construction|Chaozheng Wen\u7b49|[2601.19216](http://arxiv.org/pdf/2601.19216)|\u65e0|\u25c6 The emerging applications of next-generation wireless networks (e.g., immersive 3D communication, low-altitude networks, and integrated sensing and communication) necessitate high-fidelity environmental intelligence.\n\u25c6 3D radio maps have emerged as a critical tool for this purpose, enabling spectrum-aware planning and environment-aware sensing by bridging the gap between physical environments and electromagnetic signal propagation.\n\u25c6 However, constructing accurate 3D radio maps requires fine-grained 3D geometric information and a profound understanding of electromagnetic wave propagation.|\n",
    "2601.20857": "|2026-01-28|FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models|Hongyu Zhou\u7b49|[2601.20857](http://arxiv.org/pdf/2601.20857)|\u65e0|\u25c6 Neural Radiance Fields and 3D Gaussian Splatting have advanced novel view synthesis, yet still rely on dense inputs and often degrade at extrapolated views.\n\u25c6 Recent approaches leverage generative models, such as diffusion models, to provide additional supervision, but face a trade-off between generalization and fidelity: fine-tuning diffusion models for artifact removal improves fidelity but risks overfitting, while fine-tuning-free methods preserve generalization but often yield lower fidelity.\n\u25c6 We introduce FreeFix, a fine-tuning-free approach that pushes the boundary of this trade-off by enhancing extrapolated rendering with pretrained image diffusion models.|\n",
    "2601.20429": "|2026-01-28|GRTX: Efficient Ray Tracing for 3D Gaussian-Based Rendering|Junseo Lee\u7b49|[2601.20429](http://arxiv.org/pdf/2601.20429)|\u65e0|\u25c6 3D Gaussian Splatting has gained widespread adoption across diverse applications due to its exceptional rendering performance and visual quality.\n\u25c6 While most existing methods rely on rasterization to render Gaussians, recent research has started investigating ray tracing approaches to overcome the fundamental limitations inherent in rasterization.\n\u25c6 However, current Gaussian ray tracing methods suffer from inefficiencies such as bloated acceleration structures and redundant node traversals, which greatly degrade ray tracing performance.|\n",
    "2601.20331": "|2026-01-28|GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction|Mai Su\u7b49|[2601.20331](http://arxiv.org/pdf/2601.20331)|\u65e0|\u25c6 3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging.\n\u25c6 Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors.\n\u25c6 However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision.|\n",
    "2601.22046": "|2026-01-30|PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction|Changjian Jiang\u7b49|[2601.22046](http://arxiv.org/pdf/2601.22046)|\u65e0|\u25c6 Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both.\n\u25c6 We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner.\n\u25c6 This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy.|\n",
    "2601.22026": "|2026-01-29|Hybrid Foveated Path Tracing with Peripheral Gaussians for Immersive Anatomy|Constantin Kleinbeck\u7b49|[2601.22026](http://arxiv.org/pdf/2601.22026)|\u65e0|\u25c6 Volumetric medical imaging offers great potential for understanding complex pathologies.\n\u25c6 Yet, traditional 2D slices provide little support for interpreting spatial relationships, forcing users to mentally reconstruct anatomy into three dimensions.\n\u25c6 Direct volumetric path tracing and VR rendering can improve perception but are computationally expensive, while precomputed representations, like Gaussian Splatting, require planning ahead.|\n",
    "2601.21269": "|2026-01-29|Lightweight High-Fidelity Low-Bitrate Talking Face Compression for 3D Video Conference|Jianglong Li\u7b49|[2601.21269](http://arxiv.org/pdf/2601.21269)|\u65e0|\u25c6 The demand for immersive and interactive communication has driven advancements in 3D video conferencing, yet achieving high-fidelity 3D talking face representation at low bitrates remains a challenge.\n\u25c6 Traditional 2D video compression techniques fail to preserve fine-grained geometric and appearance details, while implicit neural rendering methods like NeRF suffer from prohibitive computational costs.\n\u25c6 To address these challenges, we propose a lightweight, high-fidelity, low-bitrate 3D talking face compression framework that integrates FLAME-based parametric modeling with 3DGS neural rendering.|\n",
    "2601.23065": "|2026-01-30|EAG-PT: Emission-Aware Gaussians and Path Tracing for Indoor Scene Reconstruction and Editing|Xijie Yang\u7b49|[2601.23065](http://arxiv.org/pdf/2601.23065)|\u65e0|\u25c6 Recent reconstruction methods based on radiance field such as NeRF and 3DGS reproduce indoor scenes with high visual fidelity, but break down under scene editing due to baked illumination and the lack of explicit light transport.\n\u25c6 In contrast, physically based inverse rendering relies on mesh representations and path tracing, which enforce correct light transport but place strong requirements on geometric fidelity, becoming a practical bottleneck for real indoor scenes.\n\u25c6 In this work, we propose Emission-Aware Gaussians and Path Tracing (EAG-PT), aiming for physically based light transport with a unified 2D Gaussian representation.|\n",
    "2601.22988": "|2026-01-30|Learning Geometrically-Grounded 3D Visual Representations for View-Generalizable Robotic Manipulation|Di Zhang\u7b49|[2601.22988](http://arxiv.org/pdf/2601.22988)|\u65e0|\u25c6 Real-world robotic manipulation demands visuomotor policies capable of robust spatial scene understanding and strong generalization across diverse camera viewpoints.\n\u25c6 While recent advances in 3D-aware visual representations have shown promise, they still suffer from several key limitations, including reliance on multi-view observations during inference which is impractical in single-view restricted scenarios, incomplete scene modeling that fails to capture holistic and fine-grained geometric structures essential for precise manipulation, and lack of effective policy training strategies to retain and exploit the acquired 3D knowledge.\n\u25c6 To address these challenges, we present MethodName, a unified representation-policy learning framework for view-generalizable robotic manipulation.|\n",
    "2601.22808": "|2026-01-30|Diachronic Stereo Matching for Multi-Date Satellite Imagery|El\u00edas Masquil\u7b49|[2601.22808](http://arxiv.org/pdf/2601.22808)|\u65e0|\u25c6 Recent advances in image-based satellite 3D reconstruction have progressed along two complementary directions.\n\u25c6 On one hand, multi-date approaches using NeRF or Gaussian-splatting jointly model appearance and geometry across many acquisitions, achieving accurate reconstructions on opportunistic imagery with numerous observations.\n\u25c6 On the other hand, classical stereoscopic reconstruction pipelines deliver robust and scalable results for simultaneous or quasi-simultaneous image pairs.|\n"
  },
  "Depth Estimation": {
    "2512.01889": "|2025-12-01|KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM|Zaid Nasser\u7b49|[2512.01889](http://arxiv.org/pdf/2512.01889)|\u65e0|\u25c6 We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments.\n\u25c6 Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training.\n\u25c6 KM-ViPE tightly couples DINO visual features with geometric constraints through a high-level features based adaptive robust kernel that handles both moving objects and movable static objects (e.g., moving furniture in ego-centric views).|\n",
    "2512.01366": "|2025-12-01|BlinkBud: Detecting Hazards from Behind via Sampled Monocular 3D Detection on a Single Earbud|Yunzhe Li\u7b49|[2512.01366](http://arxiv.org/pdf/2512.01366)|\u65e0|\u25c6 Failing to be aware of speeding vehicles approaching from behind poses a huge threat to the road safety of pedestrians and cyclists.\n\u25c6 In this paper, we propose BlinkBud, which utilizes a single earbud and a paired phone to online detect hazardous objects approaching from behind of a user.\n\u25c6 The core idea is to accurately track visually identified objects utilizing a small number of sampled camera images taken from the earbud.|\n",
    "2512.01317": "|2025-12-01|Data-Driven Learnability Transition of Measurement-Induced Entanglement|Dongheng Qian\u7b49|[2512.01317](http://arxiv.org/pdf/2512.01317)|\u65e0|\u25c6 Measurement-induced entanglement (MIE) captures how local measurements generate long-range quantum correlations and drive dynamical phase transitions in many-body systems.\n\u25c6 Yet estimating MIE experimentally remains challenging: direct evaluation requires extensive post-selection over measurement outcomes, raising the question of whether MIE is accessible with only polynomial resources.\n\u25c6 We address this challenge by reframing MIE detection as a data-driven learning problem that assumes no prior knowledge of state preparation.|\n",
    "2512.03000": "|2025-12-03|DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling|Kairun Wen\u7b49|[2512.03000](http://arxiv.org/pdf/2512.03000)|\u65e0|\u25c6 Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities.\n\u25c6 However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet.\n\u25c6 To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video.|\n",
    "2512.02972": "|2025-12-02|BEVDilation: LiDAR-Centric Multi-Modal Fusion for 3D Object Detection|Guowen Zhang\u7b49|[2512.02972](http://arxiv.org/pdf/2512.02972)|\u65e0|\u25c6 Integrating LiDAR and camera information in the bird's eye view (BEV) representation has demonstrated its effectiveness in 3D object detection.\n\u25c6 However, because of the fundamental disparity in geometric accuracy between these sensors, indiscriminate fusion in previous methods often leads to degraded performance.\n\u25c6 In this paper, we propose BEVDilation, a novel LiDAR-centric framework that prioritizes LiDAR information in the fusion.|\n",
    "2512.02727": "|2025-12-02|DF-Mamba: Deformable State Space Modeling for 3D Hand Pose Estimation in Interactions|Yifan Zhou\u7b49|[2512.02727](http://arxiv.org/pdf/2512.02727)|\u65e0|\u25c6 Modeling daily hand interactions often struggles with severe occlusions, such as when two hands overlap, which highlights the need for robust feature learning in 3D hand pose estimation (HPE).\n\u25c6 To handle such occluded hand images, it is vital to effectively learn the relationship between local image features (e.g., for occluded joints) and global context (e.g., cues from inter-joints, inter-hands, or the scene).\n\u25c6 However, most current 3D HPE methods still rely on ResNet for feature extraction, and such CNN's inductive bias may not be optimal for 3D HPE due to its limited capability to model the global context.|\n",
    "2512.02263": "|2025-12-01|DepthScape: Authoring 2.5D Designs via Depth Estimation, Semantic Understanding, and Geometry Extraction|Xia Su\u7b49|[2512.02263](http://arxiv.org/pdf/2512.02263)|\u65e0|\u25c6 2.5D effects, such as occlusion and perspective foreshortening, enhance visual dynamics and realism by incorporating 3D depth cues into 2D designs.\n\u25c6 However, creating such effects remains challenging and labor-intensive due to the complexity of depth perception.\n\u25c6 We introduce DepthScape, a human-AI collaborative system that facilitates 2.5D effect creation by directly placing design elements into 3D reconstructions.|\n",
    "2512.04085": "|2025-12-03|Unique Lives, Shared World: Learning from Single-Life Videos|Tengda Han\u7b49|[2512.04085](http://arxiv.org/pdf/2512.04085)|\u65e0|\u25c6 We introduce the \"single-life\" learning paradigm, where we train a distinct vision model exclusively on egocentric videos captured by one individual.\n\u25c6 We leverage the multiple viewpoints naturally captured within a single life to learn a visual encoder in a self-supervised manner.\n\u25c6 Our experiments demonstrate three key findings.|\n",
    "2512.04069": "|2025-12-03|SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL|Siyi Chen\u7b49|[2512.04069](http://arxiv.org/pdf/2512.04069)|\u65e0|\u25c6 Vision Language Models (VLMs) demonstrate strong qualitative visual understanding, but struggle with metrically precise spatial reasoning required for embodied applications.\n\u25c6 The agentic paradigm promises that VLMs can use a wide variety of tools that could augment these capabilities, such as depth estimators, segmentation models, and pose estimators.\n\u25c6 Yet it remains an open challenge how to realize this vision without solely relying on handcrafted prompting strategies or enforcing fixed, predefined tool pipelines that limit VLMs' ability to discover optimal tool-use patterns.|\n",
    "2512.03971": "|2025-12-03|Approximate Optimal Active Learning of Decision Trees|Zunchen Huang\u7b49|[2512.03971](http://arxiv.org/pdf/2512.03971)|\u65e0|\u25c6 We consider the problem of actively learning an unknown binary decision tree using only membership queries, a setting in which the learner must reason about a large hypothesis space while maintaining formal guarantees.\n\u25c6 Rather than enumerating candidate trees or relying on heuristic impurity or entropy measures, we encode the entire space of bounded-depth decision trees symbolically in SAT formulas.\n\u25c6 We propose a symbolic method for active learning of decision trees, in which approximate model counting is used to estimate the reduction of the hypothesis space caused by each potential query, enabling near-optimal query selection without full model enumeration.|\n",
    "2512.03958": "|2025-12-03|MDE-AgriVLN: Agricultural Vision-and-Language Navigation with Monocular Depth Estimation|Xiaobei Zhao\u7b49|[2512.03958](http://arxiv.org/pdf/2512.03958)|\u65e0|\u25c6 Agricultural robots are serving as powerful assistants across a wide range of agricultural tasks, nevertheless, still heavily relying on manual operations or railway systems for movement.\n\u25c6 The AgriVLN method and the A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural domain, enabling a robot to navigate to a target position following a natural language instruction.\n\u25c6 Unlike human binocular vision, most agricultural robots are only given a single camera for monocular vision, which results in limited spatial perception.|\n",
    "2512.03632": "|2025-12-03|Three-dimensional modelling of drag anchor penetration using the material point method|Robert E. Bird\u7b49|[2512.03632](http://arxiv.org/pdf/2512.03632)|\u65e0|\u25c6 Drag embedment anchors are a key threat to buried subsea linear infrastructure, such as power/data cables and pipelines.\n\u25c6 For cables, selecting a burial depth is a compromise between protecting the cable from anchor strike and the increased cost of deeper installation.\n\u25c6 This presents an efficient large deformation, elasto-plastic Material Point Method-based soil-structure interaction predictive tool for the estimation of anchor penetration based on Cone Penetration Test (CPT) site investigation data.|\n",
    "2512.03559": "|2025-12-03|Postseismicity of slow-slip doublets discerned on the outermost of the Nankai Trough subduction megathrust|Dye SK Sato\u7b49|[2512.03559](http://arxiv.org/pdf/2512.03559)|\u65e0|\u25c6 Despite dissimilar slip rates, slow earthquakes are faulting as ordinary earthquakes are.\n\u25c6 It is therefore physically natural that slow earthquakes also cause postseismic motions similarly to ordinary earthquakes, even though coseismic and postseismic slips remain undifferentiated for slow earthquakes.\n\u25c6 We pursue the slow-earthquake postseismicity based on the analysis of a fault slip beneath the Bungo Channel, the westernmost region of the Nankai Trough subduction zone in southwestern Japan.|\n",
    "2512.03427": "|2025-12-03|Generalization Evaluation of Deep Stereo Matching Methods for UAV-Based Forestry Applications|Yida Lin\u7b49|[2512.03427](http://arxiv.org/pdf/2512.03427)|\u65e0|\u25c6 Autonomous UAV forestry operations require robust depth estimation methods with strong cross-domain generalization.\n\u25c6 However, existing evaluations focus on urban and indoor scenarios, leaving a critical gap for specialized vegetation-dense environments.\n\u25c6 We present the first systematic zero-shot evaluation of eight state-of-the-art stereo methods--RAFT-Stereo, IGEV, IGEV++, BridgeDepth, StereoAnywhere, DEFOM (plus baseline methods ACVNet, PSMNet, TCstereo)--spanning iterative refinement, foundation model, and zero-shot adaptation paradigms.|\n",
    "2512.03332": "|2025-12-03|A three-dimensional model for the reversal in the local large-scale interstellar magnetic field|Rebecca A. Booth\u7b49|[2512.03332](http://arxiv.org/pdf/2512.03332)|\u65e0|\u25c6 We probe the three-dimensional geometry of the large-scale Galactic magnetic field within 1 kpc of the Sun using the Dominion Radio Astrophysical Observatory (DRAO) Global Magneto-Ionic Medium Survey (GMIMS) of the Northern Sky (DRAGONS).\n\u25c6 DRAGONS is a new full polarization survey of the Northern sky from 350 to 1030 MHz covering declinations -20\u00b0 < $\u03b4$ < 90\u00b0 and a component of GMIMS.\n\u25c6 The first moment of the Faraday depth spectra produced from DRAGONS above 500 MHz reveals large-angular-scale Faraday depth structures with signs that alternate only once in the Southern Galactic hemisphere and twice in the Northern hemisphere, patterns shared by other Faraday rotation datasets.|\n",
    "2512.04615": "|2025-12-04|Ground state energy and phase transitions of Long-range XXZ using VQE|Mrinal Dev\u7b49|[2512.04615](http://arxiv.org/pdf/2512.04615)|\u65e0|\u25c6 The variational quantum eigen solver (VQE), has been widely used to find the ground state energy of different Hamiltonians with no analytical solutions and are classically difficult to compute.\n\u25c6 In our work, we have used VQE to identify the phase transition boundary for an infinite order phase transition.\n\u25c6 We use long-range XXZ (LRXXZ) chain for our study.|\n",
    "2512.04587": "|2025-12-04|Supramolecular approach-based intermolecular interaction energy calculations using quantum phase estimation algorithm|Yuhei Tachi\u7b49|[2512.04587](http://arxiv.org/pdf/2512.04587)|\u65e0|\u25c6 Accurate computation of non-covalent, intermolecular interaction energies is important to understand various chemical phenomena, and quantum computers are anticipated to accelerate it.\n\u25c6 Although the state-of-the-art quantum computers are still noisy and intermediate-scale ones, development of theoretical frameworks those are expected to work on a fault-tolerant quantum computer is an urgent issue.\n\u25c6 In this work, we explore resource-efficient implementation of the quantum phase estimation-based complete active space configuration interaction (QPE-CASCI) calculations, with the aid of the second-order M\u00f8ller--Plesset perturbation theory (MP2)-based active space selection with Boys localized orbitals.|\n",
    "2512.04563": "|2025-12-04|COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence|Zefeng Zhang\u7b49|[2512.04563](http://arxiv.org/pdf/2512.04563)|\u65e0|\u25c6 Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning.\n\u25c6 Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation.\n\u25c6 In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence.|\n",
    "2512.04386": "|2025-12-04|MASE: Interpretable NLP Models via Model-Agnostic Saliency Estimation|Zhou Yang\u7b49|[2512.04386](http://arxiv.org/pdf/2512.04386)|\u65e0|\u25c6 Deep neural networks (DNNs) have made significant strides in Natural Language Processing (NLP), yet their interpretability remains elusive, particularly when evaluating their intricate decision-making processes.\n\u25c6 Traditional methods often rely on post-hoc interpretations, such as saliency maps or feature visualization, which might not be directly applicable to the discrete nature of word data in NLP.\n\u25c6 Addressing this, we introduce the Model-agnostic Saliency Estimation (MASE) framework.|\n",
    "2512.04358": "|2025-12-04|MAFNet:Multi-frequency Adaptive Fusion Network for Real-time Stereo Matching|Ao Xu\u7b49|[2512.04358](http://arxiv.org/pdf/2512.04358)|\u65e0|\u25c6 Existing stereo matching networks typically rely on either cost-volume construction based on 3D convolutions or deformation methods based on iterative optimization.\n\u25c6 The former incurs significant computational overhead during cost aggregation, whereas the latter often lacks the ability to model non-local contextual information.\n\u25c6 These methods exhibit poor compatibility on resource-constrained mobile devices, limiting their deployment in real-time applications.|\n",
    "2512.04303": "|2025-12-03|Gamma-from-Mono: Road-Relative, Metric, Self-Supervised Monocular Geometry for Vehicular Applications|Gasser Elazab\u7b49|[2512.04303](http://arxiv.org/pdf/2512.04303)|\u65e0|\u25c6 Accurate perception of the vehicle's 3D surroundings, including fine-scale road geometry, such as bumps, slopes, and surface irregularities, is essential for safe and comfortable vehicle control.\n\u25c6 However, conventional monocular depth estimation often oversmooths these features, losing critical information for motion planning and stability.\n\u25c6 To address this, we introduce Gamma-from-Mono (GfM), a lightweight monocular geometry estimation method that resolves the projective ambiguity in single-camera reconstruction by decoupling global and local structure.|\n",
    "2512.07596": "|2025-12-10|More than Segmentation: Benchmarking SAM 3 for Segmentation, 3D Perception, and Reconstruction in Robotic Surgery|Wenzhen Dong\u7b49|[2512.07596](http://arxiv.org/pdf/2512.07596)|\u65e0|\u25c6 The recent Segment Anything Model (SAM) 3 has introduced significant advancements over its predecessor, SAM 2, particularly with the integration of language-based segmentation and enhanced 3D perception capabilities.\n\u25c6 SAM 3 supports zero-shot segmentation across a wide range of prompts, including point, bounding box, and language-based prompts, allowing for more flexible and intuitive interactions with the model.\n\u25c6 In this empirical evaluation, we assess the performance of SAM 3 in robot-assisted surgery, benchmarking its zero-shot segmentation with point and bounding box prompts and exploring its effectiveness in dynamic video tracking, alongside its newly introduced language prompt segmentation.|\n",
    "2512.06793": "|2025-12-07|Generalized Geometry Encoding Volume for Real-time Stereo Matching|Jiaxin Liu\u7b49|[2512.06793](http://arxiv.org/pdf/2512.06793)|\u65e0|\u25c6 Real-time stereo matching methods primarily focus on enhancing in-domain performance but often overlook the critical importance of generalization in real-world applications.\n\u25c6 In contrast, recent stereo foundation models leverage monocular foundation models (MFMs) to improve generalization, but typically suffer from substantial inference latency.\n\u25c6 To address this trade-off, we propose Generalized Geometry Encoding Volume (GGEV), a novel real-time stereo matching network that achieves strong generalization.|\n",
    "2512.06663": "|2025-12-07|CoT4Det: A Chain-of-Thought Framework for Perception-Oriented Vision-Language Tasks|Yu Qi\u7b49|[2512.06663](http://arxiv.org/pdf/2512.06663)|\u65e0|\u25c6 Large Vision-Language Models (LVLMs) have demonstrated remarkable success in a broad range of vision-language tasks, such as general visual question answering and optical character recognition (OCR).\n\u25c6 However, their performance on perception-centric tasks -- such as object detection, semantic segmentation, and depth estimation -- remains significantly inferior to that of task-specific expert models.\n\u25c6 For example, Qwen2.5-VL-7B-Instruct achieves only 19% mAP on COCO2017 val, particularly struggling with dense scenes and small object recall.|\n",
    "2512.06368": "|2025-12-09|HuPrior3R: Incorporating Human Priors for Better 3D Dynamic Reconstruction from Monocular Videos|Weitao Xiong\u7b49|[2512.06368](http://arxiv.org/pdf/2512.06368)|\u65e0|\u25c6 Monocular dynamic video reconstruction faces significant challenges in dynamic human scenes due to geometric inconsistencies and resolution degradation issues.\n\u25c6 Existing methods lack 3D human structural understanding, producing geometrically inconsistent results with distorted limb proportions and unnatural human-object fusion, while memory-constrained downsampling causes human boundary drift toward background geometry.\n\u25c6 To address these limitations, we propose to incorporate hybrid geometric priors that combine SMPL human body models with monocular depth estimation.|\n",
    "2512.05529": "|2025-12-05|See in Depth: Training-Free Surgical Scene Segmentation with Monocular Depth Priors|Kunyi Yang\u7b49|[2512.05529](http://arxiv.org/pdf/2512.05529)|\u65e0|\u25c6 Pixel-wise segmentation of laparoscopic scenes is essential for computer-assisted surgery but difficult to scale due to the high cost of dense annotations.\n\u25c6 We propose depth-guided surgical scene segmentation (DepSeg), a training-free framework that utilizes monocular depth as a geometric prior together with pretrained vision foundation models.\n\u25c6 DepSeg first estimates a relative depth map with a pretrained monocular depth estimation network and proposes depth-guided point prompts, which SAM2 converts into class-agnostic masks.|\n",
    "2512.05412": "|2025-12-05|YOLO and SGBM Integration for Autonomous Tree Branch Detection and Depth Estimation in Radiata Pine Pruning Applications|Yida Lin\u7b49|[2512.05412](http://arxiv.org/pdf/2512.05412)|\u65e0|\u25c6 Manual pruning of radiata pine trees poses significant safety risks due to extreme working heights and challenging terrain.\n\u25c6 This paper presents a computer vision framework that integrates YOLO object detection with Semi-Global Block Matching (SGBM) stereo vision for autonomous drone-based pruning operations.\n\u25c6 Our system achieves precise branch detection and depth estimation using only stereo camera input, eliminating the need for expensive LiDAR sensors.|\n",
    "2512.05410": "|2025-12-05|Genetic Algorithms For Parameter Optimization for Disparity Map Generation of Radiata Pine Branch Images|Yida Lin\u7b49|[2512.05410](http://arxiv.org/pdf/2512.05410)|\u65e0|\u25c6 Traditional stereo matching algorithms like Semi-Global Block Matching (SGBM) with Weighted Least Squares (WLS) filtering offer speed advantages over neural networks for UAV applications, generating disparity maps in approximately 0.5 seconds per frame.\n\u25c6 However, these algorithms require meticulous parameter tuning.\n\u25c6 We propose a Genetic Algorithm (GA) based parameter optimization framework that systematically searches for optimal parameter configurations for SGBM and WLS, enabling UAVs to measure distances to tree branches with enhanced precision while maintaining processing efficiency.|\n",
    "2512.08700": "|2025-12-09|Scale-invariant and View-relational Representation Learning for Full Surround Monocular Depth|Kyumin Hwang\u7b49|[2512.08700](http://arxiv.org/pdf/2512.08700)|\u65e0|\u25c6 Recent foundation models demonstrate strong generalization capabilities in monocular depth estimation.\n\u25c6 However, directly applying these models to Full Surround Monocular Depth Estimation (FSMDE) presents two major challenges: (1) high computational cost, which limits real-time performance, and (2) difficulty in estimating metric-scale depth, as these models are typically trained to predict only relative depth.\n\u25c6 To address these limitations, we propose a novel knowledge distillation strategy that transfers robust depth knowledge from a foundation model to a lightweight FSMDE network.|\n",
    "2512.08163": "|2025-12-09|Accuracy Does Not Guarantee Human-Likeness in Monocular Depth Estimators|Yuki Kubota\u7b49|[2512.08163](http://arxiv.org/pdf/2512.08163)|\u65e0|\u25c6 Monocular depth estimation is a fundamental capability for real-world applications such as autonomous driving and robotics.\n\u25c6 Although deep neural networks (DNNs) have achieved superhuman accuracy on physical-based benchmarks, a key challenge remains: aligning model representations with human perception, a promising strategy for enhancing model robustness and interpretability.\n\u25c6 Research in object recognition has revealed a complex trade-off between model accuracy and human-like behavior, raising a question whether a similar divergence exist in depth estimation, particularly for natural outdoor scenes where benchmarks rely on sensor-based ground truth rather than human perceptual estimates.|\n",
    "2512.10956": "|2025-12-11|Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision|Wentao Zhou\u7b49|[2512.10956](http://arxiv.org/pdf/2512.10956)|\u65e0|\u25c6 The success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs).\n\u25c6 NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely.\n\u25c6 While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain.|\n",
    "2512.10725": "|2025-12-11|Video Depth Propagation|Luigi Piccinelli\u7b49|[2512.10725](http://arxiv.org/pdf/2512.10725)|\u65e0|\u25c6 Depth estimation in videos is essential for visual perception in real-world applications.\n\u25c6 However, existing methods either rely on simple frame-by-frame monocular models, leading to temporal inconsistencies and inaccuracies, or use computationally demanding temporal modeling, unsuitable for real-time applications.\n\u25c6 These limitations significantly restrict general applicability and performance in practical settings.|\n",
    "2512.10719": "|2025-12-11|SpaceDrive: Infusing Spatial Awareness into VLM-based Autonomous Driving|Peizheng Li\u7b49|[2512.10719](http://arxiv.org/pdf/2512.10719)|\u65e0|\u25c6 End-to-end autonomous driving methods built on vision language models (VLMs) have undergone rapid development driven by their universal visual understanding and strong reasoning capabilities obtained from the large-scale pretraining.\n\u25c6 However, we find that current VLMs struggle to understand fine-grained 3D spatial relationships which is a fundamental requirement for systems interacting with the physical world.\n\u25c6 To address this issue, we propose SpaceDrive, a spatial-aware VLM-based driving framework that treats spatial information as explicit positional encodings (PEs) instead of textual digit tokens, enabling joint reasoning over semantic and spatial representations.|\n",
    "2512.10498": "|2025-12-11|Robust Shape from Focus via Multiscale Directional Dilated Laplacian and Recurrent Network|Khurram Ashfaq\u7b49|[2512.10498](http://arxiv.org/pdf/2512.10498)|\u65e0|\u25c6 Shape-from-Focus (SFF) is a passive depth estimation technique that infers scene depth by analyzing focus variations in a focal stack.\n\u25c6 Most recent deep learning-based SFF methods typically operate in two stages: first, they extract focus volumes (a per pixel representation of focus likelihood across the focal stack) using heavy feature encoders; then, they estimate depth via a simple one-step aggregation technique that often introduces artifacts and amplifies noise in the depth map.\n\u25c6 To address these issues, we propose a hybrid framework.|\n",
    "2512.11773": "|2025-12-17|ProbeMDE: Uncertainty-Guided Active Proprioception for Monocular Depth Estimation in Surgical Robotics|Britton Jordan\u7b49|[2512.11773](http://arxiv.org/pdf/2512.11773)|\u65e0|\u25c6 Monocular depth estimation (MDE) provides a useful tool for robotic perception, but its predictions are often uncertain and inaccurate in challenging environments such as surgical scenes where textureless surfaces, specular reflections, and occlusions are common.\n\u25c6 To address this, we propose ProbeMDE, a cost-aware active sensing framework that combines RGB images with sparse proprioceptive measurements for MDE.\n\u25c6 Our approach utilizes an ensemble of MDE models to predict dense depth maps conditioned on both RGB images and on a sparse set of known depth measurements obtained via proprioception, where the robot has touched the environment in a known configuration.|\n",
    "2512.11130": "|2025-12-11|Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching|Bowen Wen\u7b49|[2512.11130](http://arxiv.org/pdf/2512.11130)|\u65e0|\u25c6 Stereo foundation models achieve strong zero-shot generalization but remain computationally prohibitive for real-time applications.\n\u25c6 Efficient stereo architectures, on the other hand, sacrifice robustness for speed and require costly per-domain fine-tuning.\n\u25c6 To bridge this gap, we present Fast-FoundationStereo, a family of architectures that achieve, for the first time, strong zero-shot generalization at real-time frame rate.|\n",
    "2512.13147": "|2025-12-15|StarryGazer: Leveraging Monocular Depth Estimation Models for Domain-Agnostic Single Depth Image Completion|Sangmin Hong\u7b49|[2512.13147](http://arxiv.org/pdf/2512.13147)|\u65e0|\u25c6 The problem of depth completion involves predicting a dense depth image from a single sparse depth map and an RGB image.\n\u25c6 Unsupervised depth completion methods have been proposed for various datasets where ground truth depth data is unavailable and supervised methods cannot be applied.\n\u25c6 However, these models require auxiliary data to estimate depth values, which is far from real scenarios.|\n",
    "2512.12425": "|2025-12-13|BokehDepth: Enhancing Monocular Depth Estimation through Bokeh Generation|Hangwei Zhang\u7b49|[2512.12425](http://arxiv.org/pdf/2512.12425)|\u65e0|\u25c6 Bokeh and monocular depth estimation are tightly coupled through the same lens imaging geometry, yet current methods exploit this connection in incomplete ways.\n\u25c6 High-quality bokeh rendering pipelines typically depend on noisy depth maps, which amplify estimation errors into visible artifacts, while modern monocular metric depth models still struggle on weakly textured, distant and geometrically ambiguous regions where defocus cues are most informative.\n\u25c6 We introduce BokehDepth, a two-stage framework that decouples bokeh synthesis from depth prediction and treats defocus as an auxiliary supervision-free geometric cue.|\n",
    "2512.14536": "|2025-12-16|DASP: Self-supervised Nighttime Monocular Depth Estimation with Domain Adaptation of Spatiotemporal Priors|Yiheng Huang\u7b49|[2512.14536](http://arxiv.org/pdf/2512.14536)|\u65e0|\u25c6 Self-supervised monocular depth estimation has achieved notable success under daytime conditions.\n\u25c6 However, its performance deteriorates markedly at night due to low visibility and varying illumination, e.g., insufficient light causes textureless areas, and moving objects bring blurry regions.\n\u25c6 To this end, we propose a self-supervised framework named DASP that leverages spatiotemporal priors for nighttime depth estimation.|\n",
    "2512.14236": "|2025-12-16|Elastic3D: Controllable Stereo Video Conversion with Guided Latent Decoding|Nando Metzger\u7b49|[2512.14236](http://arxiv.org/pdf/2512.14236)|\u65e0|\u25c6 The growing demand for immersive 3D content calls for automated monocular-to-stereo video conversion.\n\u25c6 We present Elastic3D, a controllable, direct end-to-end method for upgrading a conventional video to a binocular one.\n\u25c6 Our approach, based on (conditional) latent diffusion, avoids artifacts due to explicit depth estimation and warping.|\n",
    "2512.14028": "|2025-12-16|Robust Single-shot Structured Light 3D Imaging via Neural Feature Decoding|Jiaheng Li\u7b49|[2512.14028](http://arxiv.org/pdf/2512.14028)|\u65e0|\u25c6 We consider the problem of active 3D imaging using single-shot structured light systems, which are widely employed in commercial 3D sensing devices such as Apple Face ID and Intel RealSense.\n\u25c6 Traditional structured light methods typically decode depth correspondences through pixel-domain matching algorithms, resulting in limited robustness under challenging scenarios like occlusions, fine-structured details, and non-Lambertian surfaces.\n\u25c6 Inspired by recent advances in neural feature matching, we propose a learning-based structured light decoding framework that performs robust correspondence matching within feature space rather than the fragile pixel domain.|\n",
    "2512.14020": "|2025-12-16|Deep Learning Perspective of Scene Understanding in Autonomous Robots|Afia Maham\u7b49|[2512.14020](http://arxiv.org/pdf/2512.14020)|\u65e0|\u25c6 This paper provides a review of deep learning applications in scene understanding in autonomous robots, including innovations in object detection, semantic and instance segmentation, depth estimation, 3D reconstruction, and visual SLAM.\n\u25c6 It emphasizes how these techniques address limitations of traditional geometric models, improve depth perception in real time despite occlusions and textureless surfaces, and enhance semantic reasoning to understand the environment better.\n\u25c6 When these perception modules are integrated into dynamic and unstructured environments, they become more effective in decisionmaking, navigation and interaction.|\n",
    "2512.15715": "|2025-12-17|In Pursuit of Pixel Supervision for Visual Pre-training|Lihe Yang\u7b49|[2512.15715](http://arxiv.org/pdf/2512.15715)|\u65e0|\u25c6 At the most basic level, pixels are the source of the visual information through which we perceive the world.\n\u25c6 Pixels contain information at all levels, ranging from low-level attributes to high-level concepts.\n\u25c6 Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs.|\n",
    "2512.16913": "|2025-12-18|Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation|Xin Lin\u7b49|[2512.16913](http://arxiv.org/pdf/2512.16913)|\u65e0|\u25c6 In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances.\n\u25c6 We explore a data-in-the-loop paradigm from the view of both data construction and framework design.\n\u25c6 We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web.|\n",
    "2512.16561": "|2025-12-18|N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models|Yuxin Wang\u7b49|[2512.16561](http://arxiv.org/pdf/2512.16561)|\u65e0|\u25c6 While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes.\n\u25c6 In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding.\n\u25c6 Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions.|\n",
    "2512.17908": "|2025-12-19|Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting|Ananta R. Bhattarai\u7b49|[2512.17908](http://arxiv.org/pdf/2512.17908)|\u65e0|\u25c6 Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution.\n\u25c6 We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models.\n\u25c6 Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input.|\n",
    "2512.17784": "|2025-12-19|Long-Range depth estimation using learning based Hybrid Distortion Model for CCTV cameras|Ami Pandat\u7b49|[2512.17784](http://arxiv.org/pdf/2512.17784)|\u65e0|\u25c6 Accurate camera models are essential for photogrammetry applications such as 3D mapping and object localization, particularly for long distances.\n\u25c6 Various stereo-camera based 3D localization methods are available but are limited to few hundreds of meters' range.\n\u25c6 This is majorly due to the limitation of the distortion models assumed for the non-linearities present in the camera lens.|\n",
    "2512.17724": "|2025-12-19|SAVeD: A First-Person Social Media Video Dataset for ADAS-equipped vehicle Near-Miss and Crash Event Analyses|Shaoyan Zhai\u7b49|[2512.17724](http://arxiv.org/pdf/2512.17724)|\u65e0|\u25c6 The advancement of safety-critical research in driving behavior in ADAS-equipped vehicles require real-world datasets that not only include diverse traffic scenarios but also capture high-risk edge cases such as near-miss events and system failures.\n\u25c6 However, existing datasets are largely limited to either simulated environments or human-driven vehicle data, lacking authentic ADAS (Advanced Driver Assistance System) vehicle behavior under risk conditions.\n\u25c6 To address this gap, this paper introduces SAVeD, a large-scale video dataset curated from publicly available social media content, explicitly focused on ADAS vehicle-related crashes, near-miss incidents, and disengagements.|\n",
    "2512.17040": "|2025-12-18|Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation|Min-Jung Kim\u7b49|[2512.17040](http://arxiv.org/pdf/2512.17040)|\u65e0|\u25c6 Recent progress in video diffusion models has spurred growing interest in camera-controlled novel-view video generation for dynamic scenes, aiming to provide creators with cinematic camera control capabilities in post-production.\n\u25c6 A key challenge in camera-controlled video generation is ensuring fidelity to the specified camera pose, while maintaining view consistency and reasoning about occluded geometry from limited observations.\n\u25c6 To address this, existing methods either train trajectory-conditioned video generation model on trajectory-video pair dataset, or estimate depth from the input video to reproject it along a target trajectory and generate the unprojected regions.|\n",
    "2512.19083": "|2025-12-24|CoDrone: Autonomous Drone Navigation Assisted by Edge and Cloud Foundation Models|Pengyu Chen\u7b49|[2512.19083](http://arxiv.org/pdf/2512.19083)|\u65e0|\u25c6 Autonomous navigation for Unmanned Aerial Vehicles faces key challenges from limited onboard computational resources, which restrict deployed deep neural networks to shallow architectures incapable of handling complex environments.\n\u25c6 Offloading tasks to remote edge servers introduces high latency, creating an inherent trade-off in system design.\n\u25c6 To address these limitations, we propose CoDrone - the first cloud-edge-end collaborative computing framework integrating foundation models into autonomous UAV cruising scenarios - effectively leveraging foundation models to enhance performance of resource-constrained unmanned aerial vehicle platforms.|\n",
    "2512.19020": "|2025-12-22|CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization|Zelin Zhao\u7b49|[2512.19020](http://arxiv.org/pdf/2512.19020)|\u65e0|\u25c6 Achieving precise camera control in video generation remains challenging, as existing methods often rely on camera pose annotations that are difficult to scale to large and dynamic datasets and are frequently inconsistent with depth estimation, leading to train-test discrepancies.\n\u25c6 We introduce CETCAM, a camera-controllable video generation framework that eliminates the need for camera annotations through a consistent and extensible tokenization scheme.\n\u25c6 CETCAM leverages recent advances in geometry foundation models, such as VGGT, to estimate depth and camera parameters and converts them into unified, geometry-aware tokens.|\n",
    "2512.18684": "|2025-12-21|A Study of Finetuning Video Transformers for Multi-view Geometry Tasks|Huimin Wu\u7b49|[2512.18684](http://arxiv.org/pdf/2512.18684)|\u65e0|\u25c6 This paper presents an investigation of vision transformer learning for multi-view geometry tasks, such as optical flow estimation, by fine-tuning video foundation models.\n\u25c6 Unlike previous methods that involve custom architectural designs and task-specific pretraining, our research finds that general-purpose models pretrained on videos can be readily transferred to multi-view problems with minimal adaptation.\n\u25c6 The core insight is that general-purpose attention between patches learns temporal and spatial information for geometric reasoning.|\n",
    "2512.18159": "|2025-12-20|EndoStreamDepth: Temporally Consistent Monocular Depth Estimation for Endoscopic Video Streams|Hao Li\u7b49|[2512.18159](http://arxiv.org/pdf/2512.18159)|\u65e0|\u25c6 This work presents EndoStreamDepth, a monocular depth estimation framework for endoscopic video streams.\n\u25c6 It provides accurate depth maps with sharp anatomical boundaries for each frame, temporally consistent predictions across frames, and real-time throughput.\n\u25c6 Unlike prior work that uses batched inputs, EndoStreamDepth processes individual frames with a temporal module to propagate inter-frame information.|\n",
    "2512.21983": "|2025-12-26|Bab_Sak Robotic Intubation System (BRIS): A Learning-Enabled Control Framework for Safe Fiberoptic Endotracheal Intubation|Saksham Gupta\u7b49|[2512.21983](http://arxiv.org/pdf/2512.21983)|\u65e0|\u25c6 Endotracheal intubation is a critical yet technically demanding procedure, with failure or improper tube placement leading to severe complications.\n\u25c6 Existing robotic and teleoperated intubation systems primarily focus on airway navigation and do not provide integrated control of endotracheal tube advancement or objective verification of tube depth relative to the carina.\n\u25c6 This paper presents the Robotic Intubation System (BRIS), a compact, human-in-the-loop platform designed to assist fiberoptic-guided intubation while enabling real-time, objective depth awareness.|\n",
    "2512.21970": "|2025-12-26|StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision|Shengliang Deng\u7b49|[2512.21970](http://arxiv.org/pdf/2512.21970)|\u65e0|\u25c6 Stereo cameras closely mimic human binocular vision, providing rich spatial cues critical for precise robotic manipulation.\n\u25c6 Despite their advantage, the adoption of stereo vision in vision-language-action models (VLAs) remains underexplored.\n\u25c6 In this work, we present StereoVLA, a VLA model that leverages rich geometric cues from stereo vision.|\n",
    "2512.23024": "|2025-12-28|With Great Context Comes Great Prediction Power: Classifying Objects via Geo-Semantic Scene Graphs|Ciprian Constantinescu\u7b49|[2512.23024](http://arxiv.org/pdf/2512.23024)|\u65e0|\u25c6 Humans effortlessly identify objects by leveraging a rich understanding of the surrounding scene, including spatial relationships, material properties, and the co-occurrence of other objects.\n\u25c6 In contrast, most computational object recognition systems operate on isolated image regions, devoid of meaning in isolation, thus ignoring this vital contextual information.\n\u25c6 This paper argues for the critical role of context and introduces a novel framework for contextual object classification.|\n",
    "2512.22819": "|2025-12-28|Depth Anything in $360^\\circ$: Towards Scale Invariance in the Wild|Hualie Jiang\u7b49|[2512.22819](http://arxiv.org/pdf/2512.22819)|\u65e0|\u25c6 Panoramic depth estimation provides a comprehensive solution for capturing complete $360^\\circ$ environmental structural information, offering significant benefits for robotics and AR/VR applications.\n\u25c6 However, while extensively studied in indoor settings, its zero-shot generalization to open-world domains lags far behind perspective images, which benefit from abundant training data.\n\u25c6 This disparity makes transferring capabilities from the perspective domain an attractive solution.|\n",
    "2512.22653": "|2025-12-27|Visual Autoregressive Modelling for Monocular Depth Estimation|Amir El-Ghoussani\u7b49|[2512.22653](http://arxiv.org/pdf/2512.22653)|\u65e0|\u25c6 We propose a monocular depth estimation method based on visual autoregressive (VAR) priors, offering an alternative to diffusion-based approaches.\n\u25c6 Our method adapts a large-scale text-to-image VAR model and introduces a scale-wise conditional upsampling mechanism with classifier-free guidance.\n\u25c6 Our approach performs inference in ten fixed autoregressive stages, requiring only 74K synthetic samples for fine-tuning, and achieves competitive results.|\n",
    "2512.22392": "|2025-12-26|iOSPointMapper: RealTime Pedestrian and Accessibility Mapping with Mobile AI|Himanshu Naidu\u7b49|[2512.22392](http://arxiv.org/pdf/2512.22392)|\u65e0|\u25c6 Accurate, up-to-date sidewalk data is essential for building accessible and inclusive pedestrian infrastructure, yet current approaches to data collection are often costly, fragmented, and difficult to scale.\n\u25c6 We introduce iOSPointMapper, a mobile application that enables real-time, privacy-conscious sidewalk mapping on the ground, using recent-generation iPhones and iPads.\n\u25c6 The system leverages on-device semantic segmentation, LiDAR-based depth estimation, and fused GPS/IMU data to detect and localize sidewalk-relevant features such as traffic signs, traffic lights and poles.|\n",
    "2512.24792": "|2025-12-31|Projection-based Adversarial Attack using Physics-in-the-Loop Optimization for Monocular Depth Estimation|Takeru Kusakabe\u7b49|[2512.24792](http://arxiv.org/pdf/2512.24792)|\u65e0|\u25c6 Deep neural networks (DNNs) remain vulnerable to adversarial attacks that cause misclassification when specific perturbations are added to input images.\n\u25c6 This vulnerability also threatens the reliability of DNN-based monocular depth estimation (MDE) models, making robustness enhancement a critical need in practical applications.\n\u25c6 To validate the vulnerability of DNN-based MDE models, this study proposes a projection-based adversarial attack method that projects perturbation light onto a target object.|\n",
    "2512.24111": "|2025-12-30|Guided Diffusion-based Generation of Adversarial Objects for Real-World Monocular Depth Estimation Attacks|Yongtao Chen\u7b49|[2512.24111](http://arxiv.org/pdf/2512.24111)|\u65e0|\u25c6 Monocular Depth Estimation (MDE) serves as a core perception module in autonomous driving systems, but it remains highly susceptible to adversarial attacks.\n\u25c6 Errors in depth estimation may propagate through downstream decision making and influence overall traffic safety.\n\u25c6 Existing physical attacks primarily rely on texture-based patches, which impose strict placement constraints and exhibit limited realism, thereby reducing their effectiveness in complex driving environments.|\n",
    "2512.23786": "|2025-12-29|Leveraging Synthetic Priors for Monocular Depth Estimation in Specular Surgical Environments|Ankan Aich\u7b49|[2512.23786](http://arxiv.org/pdf/2512.23786)|\u65e0|\u25c6 Accurate Monocular Depth Estimation (MDE) is critical for robotic surgery but remains fragile in specular, fluid-filled endoscopic environments.\n\u25c6 Existing self-supervised methods, typically relying on foundation models trained with noisy real-world pseudo-labels, often suffer from boundary collapse on thin surgical tools and transparent surfaces.\n\u25c6 In this work, we address this by leveraging the high-fidelity synthetic priors of the Depth Anything V2 architecture, which inherently captures precise geometric details of thin structures.|\n",
    "2601.00796": "|2026-01-02|AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction|Jiewen Chan\u7b49|[2601.00796](http://arxiv.org/pdf/2601.00796)|\u65e0|\u25c6 Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion.\n\u25c6 Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability.\n\u25c6 Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation.|\n",
    "2601.02020": "|2026-01-05|Adapting Depth Anything to Adverse Imaging Conditions with Events|Shihan Peng\u7b49|[2601.02020](http://arxiv.org/pdf/2601.02020)|\u65e0|\u25c6 Robust depth estimation under dynamic and adverse lighting conditions is essential for robotic systems.\n\u25c6 Currently, depth foundation models, such as Depth Anything, achieve great success in ideal scenes but remain challenging under adverse imaging conditions such as extreme illumination and motion blur.\n\u25c6 These degradations corrupt the visual signals of frame cameras, weakening the discriminative features of frame-based depths across the spatial and temporal dimensions.|\n",
    "2601.01822": "|2026-01-05|DisCo-FLoc: Using Dual-Level Visual-Geometric Contrasts to Disambiguate Depth-Aware Visual Floorplan Localization|Shiyong Meng\u7b49|[2601.01822](http://arxiv.org/pdf/2601.01822)|\u65e0|\u25c6 Since floorplan data is readily available, long-term persistent, and robust to changes in visual appearance, visual Floorplan Localization (FLoc) has garnered significant attention.\n\u25c6 Existing methods either ingeniously match geometric priors or utilize sparse semantics to reduce FLoc uncertainty.\n\u25c6 However, they still suffer from ambiguous FLoc caused by repetitive structures within minimalist floorplans.|\n",
    "2601.01457": "|2026-01-04|Language as Prior, Vision as Calibration: Metric Scale Recovery for Monocular Depth Estimation|Mingxing Zhan\u7b49|[2601.01457](http://arxiv.org/pdf/2601.01457)|\u65e0|\u25c6 Relative-depth foundation models transfer well, yet monocular metric depth remains ill-posed due to unidentifiable global scale and heightened domain-shift sensitivity.\n\u25c6 Under a frozen-backbone calibration setting, we recover metric depth via an image-specific affine transform in inverse depth and train only lightweight calibration heads while keeping the relative-depth backbone and the CLIP text encoder fixed.\n\u25c6 Since captions provide coarse but noisy scale cues that vary with phrasing and missing objects, we use language to predict an uncertainty-aware envelope that bounds feasible calibration parameters in an unconstrained space, rather than committing to a text-only point estimate.|\n",
    "2601.03252": "|2026-01-06|InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields|Hao Yu\u7b49|[2601.03252](http://arxiv.org/pdf/2601.03252)|\u65e0|\u25c6 Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids.\n\u25c6 Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery.\n\u25c6 This paper introduces InfiniDepth, which represents depth as neural implicit fields.|\n",
    "2601.02798": "|2026-01-06|Reinforcement Learning for Follow-the-Leader Robotic Endoscopic Navigation via Synthetic Data|Sicong Gao\u7b49|[2601.02798](http://arxiv.org/pdf/2601.02798)|\u65e0|\u25c6 Autonomous navigation is crucial for both medical and industrial endoscopic robots, enabling safe and efficient exploration of narrow tubular environments without continuous human intervention, where avoiding contact with the inner walls has been a longstanding challenge for prior approaches.\n\u25c6 We present a follow-the-leader endoscopic robot based on a flexible continuum structure designed to minimize contact between the endoscope body and intestinal walls, thereby reducing patient discomfort.\n\u25c6 To achieve this objective, we propose a vision-based deep reinforcement learning framework guided by monocular depth estimation.|\n",
    "2601.02793": "|2026-01-06|StableDPT: Temporal Stable Monocular Video Depth Estimation|Ivan Sobko\u7b49|[2601.02793](http://arxiv.org/pdf/2601.02793)|\u65e0|\u25c6 Applying single image Monocular Depth Estimation (MDE) models to video sequences introduces significant temporal instability and flickering artifacts.\n\u25c6 We propose a novel approach that adapts any state-of-the-art image-based (depth) estimation model for video processing by integrating a new temporal module - trainable on a single GPU in a few days.\n\u25c6 Our architecture StableDPT builds upon an off-the-shelf Vision Transformer (ViT) encoder and enhances the Dense Prediction Transformer (DPT) head.|\n",
    "2601.02760": "|2026-01-06|AnyDepth: Depth Estimation Made Easy|Zeyu Ren\u7b49|[2601.02760](http://arxiv.org/pdf/2601.02760)|\u65e0|\u25c6 Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images.\n\u25c6 Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability.\n\u25c6 In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation.|\n",
    "2601.03869": "|2026-01-15|Bayesian Monocular Depth Refinement via Neural Radiance Fields|Arun Muthukkumar|[2601.03869](http://arxiv.org/pdf/2601.03869)|\u65e0|\u25c6 Monocular depth estimation has applications in many fields, such as autonomous navigation and extended reality, making it an essential computer vision task.\n\u25c6 However, current methods often produce smooth depth maps that lack the fine geometric detail needed for accurate scene understanding.\n\u25c6 We propose MDENeRF, an iterative framework that refines monocular depth estimates using depth information from Neural Radiance Fields (NeRFs).|\n",
    "2601.03824": "|2026-01-07|IDESplat: Iterative Depth Probability Estimation for Generalizable 3D Gaussian Splatting|Wei Long\u7b49|[2601.03824](http://arxiv.org/pdf/2601.03824)|\u65e0|\u25c6 Generalizable 3D Gaussian Splatting aims to directly predict Gaussian parameters using a feed-forward network for scene reconstruction.\n\u25c6 Among these parameters, Gaussian means are particularly difficult to predict, so depth is usually estimated first and then unprojected to obtain the Gaussian sphere centers.\n\u25c6 Existing methods typically rely solely on a single warp to estimate depth probability, which hinders their ability to fully leverage cross-view geometric cues, resulting in unstable and coarse depth maps.|\n",
    "2601.03362": "|2026-01-06|Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views|Xiang Zhang\u7b49|[2601.03362](http://arxiv.org/pdf/2601.03362)|\u65e0|\u25c6 Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues.\n\u25c6 This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks.\n\u25c6 Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions.|\n",
    "2601.03309": "|2026-01-06|VLM4VLA: Revisiting Vision-Language-Models in Vision-Language-Action Models|Jianke Zhang\u7b49|[2601.03309](http://arxiv.org/pdf/2601.03309)|\u65e0|\u25c6 Vision-Language-Action (VLA) models, which integrate pretrained large Vision-Language Models (VLM) into their policy backbone, are gaining significant attention for their promising generalization capabilities.\n\u25c6 This paper revisits a fundamental yet seldom systematically studied question: how VLM choice and competence translate to downstream VLA policies performance?\n\u25c6 We introduce VLM4VLA, a minimal adaptation pipeline that converts general-purpose VLMs into VLA policies using only a small set of new learnable parameters for fair and efficient comparison.|\n",
    "2601.05246": "|2026-01-08|Pixel-Perfect Visual Geometry Estimation|Gangwei Xu\u7b49|[2601.05246](http://arxiv.org/pdf/2601.05246)|\u65e0|\u25c6 Recovering clean and accurate geometry from images is essential for robotics and augmented reality.\n\u25c6 However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details.\n\u25c6 In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space.|\n",
    "2601.05839": "|2026-01-20|GeoSurDepth: Harnessing Foundation Model for Spatial Geometry Consistency-Oriented Self-Supervised Surround-View Depth Estimation|Weimin Liu\u7b49|[2601.05839](http://arxiv.org/pdf/2601.05839)|\u65e0|\u25c6 Accurate surround-view depth estimation provides a competitive alternative to laser-based sensors and is essential for 3D scene understanding in autonomous driving.\n\u25c6 While prior studies have proposed various approaches that primarily focus on enforcing cross-view constraints at the photometric level, few explicitly exploit the rich geometric structure inherent in both monocular and surround-view setting.\n\u25c6 In this work, we propose GeoSurDepth, a framework that leverages geometry consistency as the primary cue for surround-view depth estimation.|\n",
    "2601.06909": "|2026-01-18|UDPNet: Unleashing Depth-based Priors for Robust Image Dehazing|Zengyuan Zuo\u7b49|[2601.06909](http://arxiv.org/pdf/2601.06909)|\u65e0|\u25c6 Image dehazing has witnessed significant advancements with the development of deep learning models.\n\u25c6 However, a few methods predominantly focus on single-modal RGB features, neglecting the inherent correlation between scene depth and haze distribution.\n\u25c6 Even those that jointly optimize depth estimation and image dehazing often suffer from suboptimal performance due to inadequate utilization of accurate depth information.|\n",
    "2601.08175": "|2026-01-13|CogniMap3D: Cognitive 3D Mapping and Rapid Retrieval|Feiran Wang\u7b49|[2601.08175](http://arxiv.org/pdf/2601.08175)|\u65e0|\u25c6 We present CogniMap3D, a bioinspired framework for dynamic 3D scene understanding and reconstruction that emulates human cognitive processes.\n\u25c6 Our approach maintains a persistent memory bank of static scenes, enabling efficient spatial knowledge storage and rapid retrieval.\n\u25c6 CogniMap3D integrates three core capabilities: a multi-stage motion cue framework for identifying dynamic objects, a cognitive mapping system for storing, recalling, and updating static scenes across multiple visits, and a factor graph optimization strategy for refining camera poses.|\n",
    "2601.09823": "|2026-01-16|NanoSD: Edge Efficient Foundation Model for Real Time Image Restoration|Subhajit Sanyal\u7b49|[2601.09823](http://arxiv.org/pdf/2601.09823)|\u65e0|\u25c6 Latent diffusion models such as Stable Diffusion 1.5 offer strong generative priors that are highly valuable for image restoration, yet their full pipelines remain too computationally heavy for deployment on edge devices.\n\u25c6 Existing lightweight variants predominantly compress the denoising U-Net or reduce the diffusion trajectory, which disrupts the underlying latent manifold and limits generalization beyond a single task.\n\u25c6 We introduce NanoSD, a family of Pareto-optimal diffusion foundation models distilled from Stable Diffusion 1.5 through network surgery, feature-wise generative distillation, and structured architectural scaling jointly applied to the U-Net and the VAE encoder-decoder.|\n",
    "2601.10814": "|2026-01-20|SurfSLAM: Sim-to-Real Underwater Stereo Reconstruction For Real-Time SLAM|Onur Bagoren\u7b49|[2601.10814](http://arxiv.org/pdf/2601.10814)|\u65e0|\u25c6 Localization and mapping are core perceptual capabilities for underwater robots.\n\u25c6 Stereo cameras provide a low-cost means of directly estimating metric depth to support these tasks.\n\u25c6 However, despite recent advances in stereo depth estimation on land, computing depth from image pairs in underwater scenes remains challenging.|\n",
    "2601.12423": "|2026-01-18|HOT-POT: Optimal Transport for Sparse Stereo Matching|Antonin Clerc\u7b49|[2601.12423](http://arxiv.org/pdf/2601.12423)|\u65e0|\u25c6 Stereo vision between images faces a range of challenges, including occlusions, motion, and camera distortions, across applications in autonomous driving, robotics, and face analysis.\n\u25c6 Due to parameter sensitivity, further complications arise for stereo matching with sparse features, such as facial landmarks.\n\u25c6 To overcome this ill-posedness and enable unsupervised sparse matching, we consider line constraints of the camera geometry from an optimal transport (OT) viewpoint.|\n",
    "2601.11772": "|2026-01-16|studentSplat: Your Student Model Learns Single-view 3D Gaussian Splatting|Yimu Pan\u7b49|[2601.11772](http://arxiv.org/pdf/2601.11772)|\u65e0|\u25c6 Recent advance in feed-forward 3D Gaussian splatting has enable remarkable multi-view 3D scene reconstruction or single-view 3D object reconstruction but single-view 3D scene reconstruction remain under-explored due to inherited ambiguity in single-view.\n\u25c6 We present \\textbf{studentSplat}, a single-view 3D Gaussian splatting method for scene reconstruction.\n\u25c6 To overcome the scale ambiguity and extrapolation problems inherent in novel-view supervision from a single input, we introduce two techniques: 1) a teacher-student architecture where a multi-view teacher model provides geometric supervision to the single-view student during training, addressing scale ambiguity and encourage geometric validity; and 2) an extrapolation network that completes missing scene context, enabling high-quality extrapolation.|\n",
    "2601.11729": "|2026-01-16|SpaRRTa: A Synthetic Benchmark for Evaluating Spatial Intelligence in Visual Foundation Models|Turhan Can Kargin\u7b49|[2601.11729](http://arxiv.org/pdf/2601.11729)|\u65e0|\u25c6 Visual Foundation Models (VFMs), such as DINO and CLIP, excel in semantic understanding of images but exhibit limited spatial reasoning capabilities, which limits their applicability to embodied systems.\n\u25c6 As a result, recent work incorporates some 3D tasks (such as depth estimation) into VFM training.\n\u25c6 However, VFM performance remains inconsistent across other spatial tasks, raising the question of whether these models truly have spatial awareness or overfit to specific 3D objectives.|\n",
    "2601.15275": "|2026-01-21|RayRoPE: Projective Ray Positional Encoding for Multi-view Attention|Yu Wu\u7b49|[2601.15275](http://arxiv.org/pdf/2601.15275)|\u65e0|\u25c6 We study positional encodings for multi-view transformers that process tokens from a set of posed input images, and seek a mechanism that encodes patches uniquely, allows SE(3)-invariant attention with multi-frequency similarity, and can be adaptive to the geometry of the underlying scene.\n\u25c6 We find that prior (absolute or relative) encoding schemes for multi-view attention do not meet the above desiderata, and present RayRoPE to address this gap.\n\u25c6 RayRoPE represents patch positions based on associated rays but leverages a predicted point along the ray instead of the direction for a geometry-aware encoding.|\n",
    "2601.16532": "|2026-01-26|AnchoredDream: Zero-Shot 360\u00b0 Indoor Scene Generation from a Single View via Geometric Grounding|Runmao Yao\u7b49|[2601.16532](http://arxiv.org/pdf/2601.16532)|\u65e0|\u25c6 Single-view indoor scene generation plays a crucial role in a range of real-world applications.\n\u25c6 However, generating a complete 360\u00b0 scene from a single image remains a highly ill-posed and challenging problem.\n\u25c6 Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation.|\n",
    "2601.17657": "|2026-01-25|SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation|Taewan Cho\u7b49|[2601.17657](http://arxiv.org/pdf/2601.17657)|\u65e0|\u25c6 Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure.\n\u25c6 Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient.\n\u25c6 This paper introduces a fundamentally different approach using a dual-pathway decoder.|\n",
    "2601.17550": "|2026-01-24|AsterNav: Autonomous Aerial Robot Navigation In Darkness Using Passive Computation|Deepak Singh\u7b49|[2601.17550](http://arxiv.org/pdf/2601.17550)|\u65e0|\u25c6 Autonomous aerial navigation in absolute darkness is crucial for post-disaster search and rescue operations, which often occur from disaster-zone power outages.\n\u25c6 Yet, due to resource constraints, tiny aerial robots, perfectly suited for these operations, are unable to navigate in the darkness to find survivors safely.\n\u25c6 In this paper, we present an autonomous aerial robot for navigation in the dark by combining an Infra-Red (IR) monocular camera with a large-aperture coded lens and structured light without external infrastructure like GPS or motion-capture.|\n",
    "2601.17271": "|2026-01-24|Cross360: 360\u00b0 Monocular Depth Estimation via Cross Projections Across Scales|Kun Huang\u7b49|[2601.17271](http://arxiv.org/pdf/2601.17271)|\u65e0|\u25c6 360\u00b0 depth estimation is a challenging research problem due to the difficulty of finding a representation that both preserves global continuity and avoids distortion in spherical images.\n\u25c6 Existing methods attempt to leverage complementary information from multiple projections, but struggle with balancing global and local consistency.\n\u25c6 Their local patch features have limited global perception, and the combined global representation does not address discrepancies in feature extraction at the boundaries between patches.|\n",
    "2601.17046": "|2026-01-20|Atomic Depth Estimation From Noisy Electron Microscopy Data Via Deep Learning|Matan Leibovich\u7b49|[2601.17046](http://arxiv.org/pdf/2601.17046)|\u65e0|\u25c6 We present a novel approach for extracting 3D atomic-level information from transmission electron microscopy (TEM) images affected by significant noise.\n\u25c6 The approach is based on formulating depth estimation as a semantic segmentation problem.\n\u25c6 We address the resulting segmentation problem by training a deep convolutional neural network to generate pixel-wise depth segmentation maps using simulated data corrupted by synthetic noise.|\n",
    "2601.19489": "|2026-01-28|Fast Converging 3D Gaussian Splatting for 1-Minute Reconstruction|Ziyu Zhang\u7b49|[2601.19489](http://arxiv.org/pdf/2601.19489)|\u65e0|\u25c6 We present a fast 3DGS reconstruction pipeline designed to converge within one minute, developed for the SIGGRAPH Asia 3DGS Fast Reconstruction Challenge.\n\u25c6 The challenge consists of an initial round using SLAM-generated camera poses (with noisy trajectories) and a final round using COLMAP poses (highly accurate).\n\u25c6 To robustly handle these heterogeneous settings, we develop a two-stage solution.|\n",
    "2601.19461": "|2026-01-27|Towards Gold-Standard Depth Estimation for Tree Branches in UAV Forestry: Benchmarking Deep Stereo Matching Methods|Yida Lin\u7b49|[2601.19461](http://arxiv.org/pdf/2601.19461)|\u65e0|\u25c6 Autonomous UAV forestry operations require robust depth estimation with strong cross-domain generalization, yet existing evaluations focus on urban and indoor scenarios, leaving a critical gap for vegetation-dense environments.\n\u25c6 We present the first systematic zero-shot evaluation of eight stereo methods spanning iterative refinement, foundation model, diffusion-based, and 3D CNN paradigms.\n\u25c6 All methods use officially released pretrained weights (trained on Scene Flow) and are evaluated on four standard benchmarks (ETH3D, KITTI 2012/2015, Middlebury) plus a novel 5,313-pair Canterbury Tree Branches dataset ($1920 \\times 1080$).|\n",
    "2601.19385": "|2026-01-27|MIRAGE: Enabling Real-Time Automotive Mediated Reality|Pascal Jansen\u7b49|[2601.19385](http://arxiv.org/pdf/2601.19385)|\u65e0|\u25c6 Traffic is inherently dangerous, with around 1.19 million fatalities annually.\n\u25c6 Automotive Mediated Reality (AMR) can enhance driving safety by overlaying critical information (e.g., outlines, icons, text) on key objects to improve awareness, altering objects' appearance to simplify traffic situations, and diminishing their appearance to minimize distractions.\n\u25c6 However, real-world AMR evaluation remains limited due to technical challenges.|\n",
    "2601.19314": "|2026-01-27|Instance-Guided Radar Depth Estimation for 3D Object Detection|Chen-Chou Lo\u7b49|[2601.19314](http://arxiv.org/pdf/2601.19314)|\u65e0|\u25c6 Accurate depth estimation is fundamental to 3D perception in autonomous driving, supporting tasks such as detection, tracking, and motion planning.\n\u25c6 However, monocular camera-based 3D detection suffers from depth ambiguity and reduced robustness under challenging conditions.\n\u25c6 Radar provides complementary advantages such as resilience to poor lighting and adverse weather, but its sparsity and low resolution limit its direct use in detection frameworks.|\n",
    "2601.18929": "|2026-01-26|On the Role of Depth in Surgical Vision Foundation Models: An Empirical Study of RGB-D Pre-training|John J. Han\u7b49|[2601.18929](http://arxiv.org/pdf/2601.18929)|\u65e0|\u25c6 Vision foundation models (VFMs) have emerged as powerful tools for surgical scene understanding.\n\u25c6 However, current approaches predominantly rely on unimodal RGB pre-training, overlooking the complex 3D geometry inherent to surgical environments.\n\u25c6 Although several architectures support multimodal or geometry-aware inputs in general computer vision, the benefits of incorporating depth information in surgical settings remain underexplored.|\n",
    "2601.20331": "|2026-01-28|GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction|Mai Su\u7b49|[2601.20331](http://arxiv.org/pdf/2601.20331)|\u65e0|\u25c6 3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging.\n\u25c6 Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors.\n\u25c6 However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision.|\n",
    "2601.20303": "|2026-01-28|Physically Guided Visual Mass Estimation from a Single RGB Image|Sungjae Lee\u7b49|[2601.20303](http://arxiv.org/pdf/2601.20303)|\u65e0|\u25c6 Estimating object mass from visual input is challenging because mass depends jointly on geometric volume and material-dependent density, neither of which is directly observable from RGB appearance.\n\u25c6 Consequently, mass prediction from pixels is ill-posed and therefore benefits from physically meaningful representations to constrain the space of plausible solutions.\n\u25c6 We propose a physically structured framework for single-image mass estimation that addresses this ambiguity by aligning visual cues with the physical factors governing mass.|\n",
    "2601.22054": "|2026-01-29|MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources|Baorui Ma\u7b49|[2601.22054](http://arxiv.org/pdf/2601.22054)|\u65e0|\u25c6 Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data.\n\u25c6 We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures.\n\u25c6 Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases.|\n",
    "2601.21935": "|2026-01-29|Belief Propagation Converges to Gaussian Distributions in Sparsely-Connected Factor Graphs|Tom Yates\u7b49|[2601.21935](http://arxiv.org/pdf/2601.21935)|\u65e0|\u25c6 Belief Propagation (BP) is a powerful algorithm for distributed inference in probabilistic graphical models, however it quickly becomes infeasible for practical compute and memory budgets.\n\u25c6 Many efficient, non-parametric forms of BP have been developed, but the most popular is Gaussian Belief Propagation (GBP), a variant that assumes all distributions are locally Gaussian.\n\u25c6 GBP is widely used due to its efficiency and empirically strong performance in applications like computer vision or sensor networks - even when modelling non-Gaussian problems.|\n",
    "2601.22917": "|2026-01-30|Deep in the Jungle: Towards Automating Chimpanzee Population Estimation|Tom Raynes\u7b49|[2601.22917](http://arxiv.org/pdf/2601.22917)|\u65e0|\u25c6 The estimation of abundance and density in unmarked populations of great apes relies on statistical frameworks that require animal-to-camera distance measurements.\n\u25c6 In practice, acquiring these distances depends on labour-intensive manual interpretation of animal observations across large camera trap video corpora.\n\u25c6 This study introduces and evaluates an only sparsely explored alternative: the integration of computer vision-based monocular depth estimation (MDE) pipelines directly into ecological camera trap workflows for great ape conservation.|\n",
    "2601.22808": "|2026-01-30|Diachronic Stereo Matching for Multi-Date Satellite Imagery|El\u00edas Masquil\u7b49|[2601.22808](http://arxiv.org/pdf/2601.22808)|\u65e0|\u25c6 Recent advances in image-based satellite 3D reconstruction have progressed along two complementary directions.\n\u25c6 On one hand, multi-date approaches using NeRF or Gaussian-splatting jointly model appearance and geometry across many acquisitions, achieving accurate reconstructions on opportunistic imagery with numerous observations.\n\u25c6 On the other hand, classical stereoscopic reconstruction pipelines deliver robust and scalable results for simultaneous or quasi-simultaneous image pairs.|\n",
    "2601.22445": "|2026-01-30|High-Definition 5MP Stereo Vision Sensing for Robotics|Leaf Jiang\u7b49|[2601.22445](http://arxiv.org/pdf/2601.22445)|\u65e0|\u25c6 High-resolution (5MP+) stereo vision systems are essential for advancing robotic capabilities, enabling operation over longer ranges and generating significantly denser and accurate 3D point clouds.\n\u25c6 However, realizing the full potential of high-angular-resolution sensors requires a commensurately higher level of calibration accuracy and faster processing -- requirements often unmet by conventional methods.\n\u25c6 This study addresses that critical gap by processing 5MP camera imagery using a novel, advanced frame-to-frame calibration and stereo matching methodology designed to achieve both high accuracy and speed.|\n"
  }
}